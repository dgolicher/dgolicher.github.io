[
["introduction-to-gis.html", "Quantitative and spatial analysis Chapter 1 Introduction to GIS 1.1 What is a GIS? 1.2 Desktop GIS 1.3 Some key GIS concept 1.4 Making your own maps 1.5 Raster terrain data 1.6 Vector data from the WFS service 1.7 Saving layers locally for processing and offline use", " Quantitative and spatial analysis Duncan Golicher 2019-12-12 Chapter 1 Introduction to GIS 1.1 What is a GIS? The initials GIS stand for Geographical Information System. For many years a handful of companies produced and marketed GIS software. Many GIS courses were designed around these products. This led to the term being closely associated with desktop GIS such as Esri’s Arc GIS. The rise in use of this particular software occurred prior to the internet. Today, Geographical Information is an integral part of the internet. It is collected by us all every time we use a phone or computer (IP addresses are geo referenced). Almost all environmental data also has some form of spatial component. So, a modern Geographical Information System must link to the internet and must take in data from many different sources. There are a wide range of different tools for achieving this. Modern spatial data processing links these tools together. So a GIS is not a single piece of software, it is best thought of of an interlinked stack of software tools that are all working together. Some of the elements are visible to users and so controllable through a Graphical User Interface (GUI). Other elements work behind the scenes to provide the data streams and spatial algorithms. Many of the more traditional skills taught on GIS courses that used single pieces of software are still relevant, but they now link into this broader based concept of spatial data analysis. We can consider Google Maps, SatNavs, sports tracking phone apps, weather maps etc as all examples of modern GIS’s. The statistical software R, that we will use later in the course, is also a GIS. We all use GIS’s every day, often without realising it. 1.1.0.1 Exercise Open this weather mapping app in a browser. https://www.windy.com Which elements of a GIS does it use? What sort of data is being processed? How does spatial data (maps) link to temporal data (time series)? How important is good visualisation in allowing users to understand large quantities of data? 1.2 Desktop GIS Desktop software with a GUI remain important analytical and cartographic tools for all environmental scientists and managers. In 2018 there are two popular options. Esri’s ArcGIS still has the largest commercial market share. However many users have now switched to the free and Open Source QGIS. This provides most of the same basic functionality as ArcGIS and a similar look and feel. One advantage of QGIS is clearly the cost (it is free). QGIS also links very cleanly with a wide range of additional open source software for spatial data processing. If you learn to use QGIS it is easy that can implement the more advanced operations that are also available in ArcGIS. The interfaces are quite similar, although there are a few differences in layout. All spatial data sets can be saved in generic formats that can be opened in both pieces of software. GIS skills are not really software specific. If you learn to run an analysis in one piece of software you can easily learn to run it in another. The key element required to become comfortable working with GIS is a good understanding of the nature of spatial data sets and the range of operations that can be performed on them. 1.3 Some key GIS concept 1.3.1 Coordinate reference systems and projections. Geographical information is of course spatially explicit and so can be mapped onto the earth surface. This means that we work with coordinates. The following are two common types of coordinate systems used in a geographic information system (GIS): A global or spherical coordinate system such as latitude-longitude. These are often referred to as geographic coordinate systems. Projected coordinate system such as universal transverse Mercator (UTM), the British National Grid, Albers Equal Area, Google mercator etc. There are a large number of other map projection models which all provide mechanisms to project maps of the earth’s spherical surface onto a two-dimensional Cartesian coordinate plane. A GIS can transform (project) spatial data from one CRS to another more or less seamlessly. However it is very important to know which CRS you are using and which CRS is used by the native data format. The issue can become complicated when working with data provided by researchers in more than one country, as each nation may use a different gridded system for projection. The datums used for the projections have also changed over time. To help enforce consistency the European Petroleum Survey Group (EPSG) catalogued all the known CRSs in use and provided them with a common code number for reference. The EPSG code is the simplest and most consistent way to refer to a CRS. In the UK there are three very commonly used EPSG codes. EPSG 4326: This is the code for unprojected (i.e. Latitude and Longitude) geographical coordinates. It is universal and global in extent. This is the best CRS for raw data as all the others can be easily derived from it, no matter where you are. EPSG 27700: The British National grid. The default national system that respects distances and areas. This is the CRS for site mapping in the UK. EPSG 3857: Web mercator. The projection used by Google maps and many other online sources. So to summarise. All GIS can reproject data from latitude and longitude form into a projection for calculating distances and areas it is generally preferable to keep data that may be shared internationally in EPSG 4326. However when working with researchers in the UK EPSG 27700 is very commonly used as the standard format. EPSG 3857 is convenient for global visualisaion but the calculations made directly within this CRS are inaccurate. 1.3.2 Raster vs vector data There are two ways that geographical data can be represented. Most GIS projects include a mixture of them Raster layers are made up of pixels or cells. A satellite image or an aerial photo are examples of rasters. In these cases the coloured visual representation is made of three bands, red, blue and green which are blended together to form an image. Another typical example of a raster layer is a digital elevation model. In this case the cell values correspond to height above sea level, or some other reference point. The pixel size defines the resolution of a raster layer. The smaller the pixel the finer the resolution and level of detail. 1.4 Making your own maps On screen digitising used to be the commonest use of GIS. Digitising involves drawing your own map, adding information, capturing and storing the results. This is the most obvious way to form an original map. Other methods involve geoprocessing or image classification. Digitising has become a slightly less important skill than it once was, due to the availability of online spatial data and web maps that already have the information needed. However there are still many uses for digitisation. In this case we will be using QGIS to produce a phase one map for Hengistbury head. 1.4.1 The QGIS interface The interface to QGIS can be customised by the user to show different elements in different regions of the screen. You can add or subtract toolbars, change the panels which are visible using the menus at the top and move things around in many different ways. The screenshot below shows a basic default layout. The concept of a “project” is an important element in all desktop GIS. A project file contains information regarding the CRS being used and the files that are loaded. Projects are used to restore the GIS to the state it was in when you were last using it. It is important to be aware that when you save a project you do not actually save the files themselves. 1.4.2 QGIS GUI QGIS interface 1.4.3 Adding base maps A typical first step in any GIS project is to add some base maps. In QGIS 3.2 this is achieved by selecting from the XYZ tile item in the browser tab. This should be preloaded with a set of useful options. If your PC does not have basemaps preloaded, or if you are using your laptop you can load all the options by pasting this script into the python console (under the plugins menu). https://raw.githubusercontent.com/klakar/QGIS_resources/master/collections/Geosupportsystem/python/qgis_basemaps.py Open the python console. copy the whole script into it, press run and wait for a few minutes for the installation to complete. This only needs to be done once. 1.4.4 Loading a base map The base maps are typically streamed in the EPSG:3857 coordinate reference system, i.e the Google or Web Mercator. However data files in the UK are typically held in EPSG:277000 i.e. British National Grid. This could cause problems when working with the two data sources together. Fortunately QGIS 3.2 has very smooth “on the fly” reprojection that allows layers in two different CRSs to be lined up. To change the project CRS into EPSG:27700 for processsing and consitency click on the project properties icon. Find the British National grid CRS by searching for 277000. Apply this CRS to the project. 1.4.4.1 Exercise Experiment with the base maps. Load several maps. Try reordering them in the layers panel and switching them on and off. Right clicking the name of the layer allows you to remove them. 1.4.5 Saving projects and files You can now start saving your project. Open a new folder somewhere on your profile and save a new project file with an appropriate name. It should be obvious how to do this from the files menu. You should always take into account that the project file itself is simply a text file with details of the loaded layers, the CRS being used, the area that is zoomed to, etc. So, you can’t just share a project file which has information about layers that are loaded and expect someone else to be able to open it unless you also share the files themselves. 1.4.6 Adding a WCS raster layer from the local Geoserver A Geoserver is one of a set of tools that allows geographical information to be shared over a network or on the internet. Opening a geoserver connection allows access to geographical information held on a server. If institutions are holding confidential or valuable information then access will only be allowed on site and will require a log in. We will use some simple, non confidential layers in the current project. There are several different types of services available on a geoserver. WMS: Web map service. Allows fast overlays of pictures of the data that can be included in web maps. This is useful for visualistion, but cannot be used for analytical purposes. This is similar to the base maps you have already loaded as you can see the layer but you can’t manipulate it. WFS: Web feature service. This allows vector layers to be loaded locally for desktop processing. The layers are usually then saved locally and used for further processing. WCS: Web coverage service: Similar to WFS, but for raster layers. As we are going to process data locally we will use the WCS and WFS services. Look for the WFS tab in the browser and click on add a service. The URLs you will need are the following http://r.bournemouth.ac.uk:8083/bu/wfs? Or on campus http://172.25.50.33:8083/bu/wfs? You can give the service any name that you want. There is no need for authentication. Likewise open a WCS service in the same way using the same URL but with the final letters changed to wcs (i.e http://172.25.50.33:8083/bu/wcs?) 1.5 Raster terrain data The WCS service links to Lidar derived raster layers held on the server. There are two layers for Hengistbury Head and for Arne. The digital surface model (DSM) represents the elevation of the top of the vegetation layer. The digital terrain model has values for the elevaltion of the ground. Both the layers are at 2m resolution. We will look at how these layers can be used in a later class. For the moment load the two layers into your Hengistbury project by clicking on them. You will then see the names added to the layers panel. The best way to get used to working with loaded layers in GIS is to experiment yourself. You will find that right clicking the layer brings up a menu that allows you to zoom to the layer extent, amongst other options. You could also try styling the layer to change its colours from the standard grey scale. We will look at these options in more detail later. For the moment, after experimenting zoom to the area that will form the focus of your study. 1.6 Vector data from the WFS service We will add two additional layers from the WFS. The first shows the boundaries of sites of special scientific interest as provided by Natural England. The second contains lines from the open street map layers. Although you can visualise open street map data as base layers, you can’t use the base leyers for any form of geoprocessing or analysis. DO NOT ZOOM TO THE FULL EXTENT OF EITHER OF THESE LAYERS The layers held on the Geoserver are quite large. The natural england SSSI layer contains data for the whole of England Wales. You do not want to download all these data at this time. 1.7 Saving layers locally for processing and offline use In some situations it can be useful to work with data held on the server directly. However this may lead to modifications to shared data that other users will see. When a large data set has been partially loaded from the server using either WFS or WCS, accidentally zooming out of the study region will lead to the server downloading large amounts of data and rendering the results as a map. You may also wish to extract data from a network that is only available on campus in order to work with it at home. Working with local data is therefore the usual proctice for desktop GIS. So, after zooming to the area of interest make copies of all the four layers in your project folder. Give the layers informative names and make sure that you only save data that falls inside the study area. Once you have saved all four layers and loaded them into the canvas you should remove the WCS and WFS versions, as you no longer need them and they may slow down rendering and processing by reloading data from the network. Make sure that you click to only save the map view extent. Repeat this for all four layers. Notice that by default you save vector layers as shapefiles. A shapefile is something of a misnomer as the shapefile format actually uses multiple files to describe the data. The files with a shp extension hold the geometry, those with dbf extensions hold the attributes and the .prj file contains the CRS (projection) information. If you pass a shapefile to another user you would usually zip all these elements up together. Shapefiles are a legacy format, as there are many better ways of holding vector data, but due to the dominance of Esri they have remain the default. When you save raster data the default format is a geotiff. These files can be opened in any GIS. When you have finished saving the four layers and removed the WCS and WFS connected layers your project may look rather like the screenshot below. Note that I have used very simple names for the files and they are all saved to a single folder along with the project file. The project can now be recreated offline. 1.7.0.1 Exercise Now that you have saved all the layers locally you may want to try changing their styles and experimenting with some other options in GIS. There is no harm in trial and error. The underlying data won’t be affected. If you find colour schemes that you like they will be saved when you save the project. Remember that saving a project DOES NOT save data. It simply saves the state of QGIS when the appropriate data are loaded. "],
["on-screen-digitising.html", "Chapter 2 On screen digitising 2.1 Digitising tool bar 2.2 Drawing a polygon 2.3 Saving the polygon 2.4 Snapping 2.5 Avoiding intersections 2.6 Splitting on paths 2.7 Summary", " Chapter 2 On screen digitising On screen digitising produces new vector layers. There are three basic geometries that can be captured. Points Lines Polygons The geometries are associated with sets of attributes that are held as a table. So, if a point represents some observation on the ground, say the position of a soil core that is analysed in the laboratory the attributes may be the amount of organic matter, pH, sediment type etc. Lines and polygons also have attributes. Vector layers can also consist of multiple geometries with a single attribute table. To understand this think in terms of a country, such as the United Kingdom, that consists of multiple islands. In order to hold country level data that correspond to every polygon we use a table with multiple geometries. Roads with many sections consist of multiple lines. Collections or observations of a species may consist of multiple points. We will look at how to work with this concept later in this practical. We can add attributes to the geometries either while digitising on screen or later. If a table of data has a unique identifyier that corresponds to the captured geometries (say a code for the soil sample) the attributes can be merged into the map after the lab work has been completed. 2.1 Digitising tool bar The basic tool bar for digtising should already show up on the default layout of QGIS. If you right click on a blank space on the top bar you can add or subtract tool bars easily. You may want to add the advanced digitising tool (useful if you need to cut holes in polygons). You should certainly add the snapping toolbar when working with polygons as this is usually essential. To start digitising you need to add a new blank layer to the map. Go to layer on the top menu, create layer then create new shapefile layer. Shapefiles are generic but do remember that a shapefile consists of multiple files. Make sure all the work is saved in the same folder as the project. If you do this you can easily move the work onto another PC or laptop. The default geometry type is point, so make sure that you change this to polygon before saving the new layer. To begin with we’ll just digitise a throw away practice layer in order to get used to all the concepts involved. For the actual phase one mapping assignment you will start again and use these skills to more carefully delimitate the boundaries of the habitat types using both points collected on the ground and visual cues from base map imagery. Change the geometry type to polygon and type in a file name to save as. Check carefully again at this moment that the file is being saved in the same directory as your project. The vector layer will be made up of the geometry, that you will draw on screen, and a table of attributes that you can add after drawing each polygon. Many other attributes can be added through deriving them within the GIS. For example, the area of the polygon, its perimeter and a range of other elements can be derived directly from the geometry. Other elements, such as average slope, elevation above sea level etc can be derived by combining the polygon with terrain analysis. We will see how this can be achieved later. So, when you are digitising you only need to think about adding attributes that you have observed yourself. In the case of a phase one mapping exercise these attributes may be the code for the vegetation type and some observations regarding the condition of the area. These are held in the form of text. So you might add two attributes in this trial run. The first would be called something such pas vegcode, or phase1code. The second may be condition. Don’t worry too much about the details for this trial run. You can be more careful and more specific for the assignment task. 2.2 Drawing a polygon Once you have set up your shapefile with a couple of attributes and saved it you can start digitising. Choose an area of Hengistbury with some paths (we’ll see why later) and draw a polygon. You can use a satelite image as a guide, but for the moment don’t worry too much about actually drawing around real features (wait until you have the assignment data for that). Select the new layer in the layers panel. Click on the pencil tool at the top to start editing. You should see the draw polygon icon become active. If you do not, it is probably because you have used the default point option when creating the shapefile. If this happens go back and start the process again. 2.2.1 Valid and invalid geometries You have to be careful when digitising to avoid invalid geometries. The commonest mistake is to form a knot. This occurs when you draw back over a line forming a kite tail type polygon. If you think about it, this is invalid as a single polygon as it actually consists of two polygons that just touch at a single vertex. Another problem may arise if you click on exactly the same point twice. So be careful. If you do produce an invalid geometry QGIS will warn you. Just right click, cancel and start on the polygon again. 2.3 Saving the polygon Once you have drawn the first polygon right click to end. If you can’t see the polygon it may be because your layer is hidden under other active layers. Move the layer up to the top of the layers panel. QGIS will have filled the polygon with some default colour. I usually change the style when digitising to a see through hashed style of some kind. You can do this by right clicking the layer, finding properties and adjusting the symbology (don’t click on styles at this point, we will see how useful styles are later). You can enter some attribute data when saving the polygon. This can always be editted later if you want. The polygon is now visible and will be shaded in with some default (usually ugly!) colour. If it is not visible, move the layer to the top in the layers panel. Right clicking the layer allows you to open the properties menu. You can now try altering the shading. We will see later how different attributes can be given different types of shading in order to form a chloropleth map. 2.4 Snapping Snapping is an extremely important concept to understand when producing vegetation maps. If you start drawing another polygon without touching activating snapping you will find that you can draw it anywhere on the screen exactly in the same way as the first. This is good for some purposes. However it means that you can draw over the first polygon easily. This could lead to some areas being given two different sets of attributes, which is clearly a problem. You can also leave gaps between the polygons, which may be OK, but may be undesirable if the aim is to make sure that the entire area is classified. Snapping is designed to solve this. However it can be a bit fiddly to use, so it needs some practice. Find the magnet tool at the top of the screen and turn snapping on. Notice that there are several options alongside the magnet including snalling distance and whether to snap to vertices, segments or both. You can change the snapping distance and also decide whether to snap to vertices, segments or both using the snapping tool. Snapping to vertices can often be the safest method, but it can depend on the shape of the poygon. If you set the snapping distance too large you will find points being annoyingly drawn back to the previous polygon, when that is not what you want. So you should try switching snapping on and off and adjusting the other elements until you are comfortable that you understand the concept. Don’t worry at this stage about drawing around actual features as that may be distracting. With snapping on, if you click near the first polygon the vertex will be drawn to meet it. You can then draw along the line and meet the polygon again Experiment until you gate the hang of snapping. Make a map with several polygons with no gaps between them. 2.5 Avoiding intersections One trick that can save a lot of time and effort is to turn on the avoid intersections feature. This is available in the advanced settings of the snapping toolbox. Avoiding intersections with layers that are distinct from the one that you are actually creating is a truly advanced feature, as it can be very awkward to use correctly. However avoiding intesections with your own layer is quite a simple trick. Find the tool. Choose to avoid intesections on the current layer. If you have now set this up you can safely draw right over the layer you have already created. The areas that intersect with the previous polygons will be removed leaving you with a perfcet join. This is often much easier than using snapping and is particularly useful if you wish to draw polygons with holes in them, for example habitat around a lake. in this case draw the lake first then draw a larger polygon on top of it after setting up the avoid intersection feature. The new polygon will form a ring around the lake. 2.6 Splitting on paths Nature reserves often have paths and other features running through them. You could digitise these yourself as lines. In our case some lines have been provided from the open street map layer. This operation will introduce a few useful geoprocessing operations. The first is buffering. We will make a new temporary layer around the paths with a width that corresponds to the area that we wish to extract from our prototype vegetation classification. Find buffering from the vector geoprocessing menu at the top of the window. Choose the lines layer that you saved from the Open Street map lines earlier (If you haven-t done this, look for it in the WFS connection and save a copy locally). Set the distance around the paths that you wish to buffer to and run the buffer, saving the results to a temporary file. You can see the temporary results. You could save these into your project if you think you may use the buffer again. Temprorary files vanish when the program is closed, which is actually a good thing (feature, not a bug), as it prevents too many intermediate steps piling up in your project. However think about whether you have a use for the results later or not. For example, if you wanted to measure the proportion of the area that was within 2 m of a path then this layer might prove useful. Now find the difference option, again from the geoprocessing menu. Finding the differences between two geometries or the interesections between them is a very common GIS operation. As these operations form a new layer the original attributes that are copied from the original layers may not always apply to the results. For example, if you had calculated the area of the polygons before differencing it will now be less. The old attributes will be copied over to start with, but you may change these later. Again, save the results as a temporary layer. Look at the results You may want to move the layer order and switch the visibility of layers. If you look at the attribute table you will see that you still have the same number of rows as you had in the original layer. However if you look at the map you can see that there are now more polygons than before. This is an example of a multipolygon geometry. The original habitat polygons have been split up by the action into several pieces, but they are held together. If you wish to turn this layer into a table with one polygon per row then you need to run the multipart to single part operation on the layer. Find the option to split the geopmetry. Now investigate the results. 2.7 Summary In this class you have looked at the basics of on screen digitising. Accurate digitising is something of an art form and takes time to master. There are some more advanced features, such as snapping to different layers, cutting and filling holes and drawing curves that we have not looked at. There are many online tutorials available that show some of these methods. You will not need particularly advanced techniques to produce the phase one map for the assignment. As there are many different options available in GIS the best way to learn is through practical experience and solving problems as they arise. Save the work as you go on, and always remember that saving a project does not save changes to individual files. Get into the habitat if backing up any major changes to your work as you progress. Save files with a new name if you are worried that changes might not turn out as you expect, so you can easily roll back to the file you started with. Think about how geoprocessing operations such as buffering, differencing and finding intersections can speed up the work and produce new layers that may be useful. "],
["styling-and-cartography.html", "Chapter 3 Styling and cartography 3.1 Introduction 3.2 Using a prebuilt style", " Chapter 3 Styling and cartography 3.1 Introduction Although the use of “slipppy”, zoomable, web maps to embed within webpages has now become a key part of GIS, traditional maps that can be printed out and included in paper based reports and posters are still useful. In this class we will look at how QGIS can be used in this context to form a phase one map that could be given to visitors to Hengistbury head. We will start with a digitised map consisting of polygons. For the purposes of illustration I have simply drawn a few polygons on screen to illustrate the concepts. Your own map will look very different from the one shown in the handouts, but the instructions will work in the same way with any set of polygons. For the assignment you will rpoduce your own phase one map and canuse these steps to shade it in with the conventional colour scheme. 3.2 Using a prebuilt style This is my example, blank map with no styling. 3.2.1 Finding the styling dialogue Right clicking on the layer name in the layers panel will bring up a menu which includes the properties tab (don’t click on styles at this point). Find symbology on the properties menu. At the bottom of the window there is a button labelled style. Click on this 3.2.2 Loading the style sheet colour scheme for the polygons. This will prompt you to load a style. You should have a phase one style sheet in your project folder. You can download one (there are quite a few available that have been prepared by different people working on phase one mapping) from here. http://r.bournemouth.ac.uk:82/PhaseOneMapping/QGIS-UK-habitat-syles-for-phase1/ I used this one http://r.bournemouth.ac.uk:82/PhaseOneMapping/QGIS-UK-habitat-syles-for-phase1/P1%20Habitat%20Survey%20Toolkit%20QGIS%20QML%20Polygon%20Style%20File.qml You should also download the handbook for phase one mapping for reference. http://r.bournemouth.ac.uk:82/PhaseOneMapping/pub10_handbookforphase1habitatsurvey.pdf Make sure you place the style file in the project folder tha you are using. Now open the style and apply it to the layer. For this operation to work you will need to have ensured that one of the attributes for the polygons represents the code used for the phase one maps. This is very important as the letters in the attribute text should correspond exactly with those used in the phase one mapping guide. You should check this carefully as this will be needed in the assignment. Choose the column that corresponds to the code. You should see that the polygons have now all changed thier fill colour. 3.2.3 Filtering the visible styles The style sheet has entries for all the possible phase one habitats. This is rather awkward, as you won’t have used many of these options in your own map. So you can show only the ones that you have actually used by applying the filter (inverted funnel) at the topof the layers panel. 3.2.4 Copying and pasting styles To illustrate this concept a little further I then ran the buffering and differencing operations shown in the last handout in order to split the polygons by the paths. This layer does not yet have a style, as I have just derived it. However by right clicking on the original layer brings up a menu with “styles” included. If you click on this for the original layer you can copy the style. You can then open the menu for the new layer and paste the style into it. The new layer will then be shaded just as the original was. 3.2.5 Adding derived attributes At this point it might be worth including an explanation of how some additional attributes can be added to a digitised layer. It is often useful to know the area of a polygon and the perimeter. These can be added to the layer using the filed calculator. Right click on a layer nd find the attributes table. When the attributes table is open for editing you will be able to add new attributes to the layer. You may add some of these by typing values or text into the table. However others can be calculated from the geometries. Area and perimeter are obvious examples. The filed calculator buttton is on the top right (third from the end) Add a field called “area”. The operator to calculate the area is $area. So if you fill in the window as shown below and press OK you will find the areas have been added. The same can be done for perimeter. There are many other options for editing and extending the attributes table that you may want to experiment with. 3.2.6 Cartography: Designing a prinatable map QGIS has a layout manager that can be used to design a map that can be printed on paper. Designing a printable map is something of an art form. There are many possible options and it is up to you to choose those that most suit the application the map will be used for. In general, printed maps should aim for simplicity and clarity. To obtain very professonal results some map makers begin composing the map in GIS and then export the results to an application such as photoshop to touch up the final product. The key features that you will usually want to show on a printed map will be the scale and a legend that explains the meaning of any shading or symbols on the map. Find the map icon on the left hand side of the composer window and draw a rectangle to include the map as visible in the main window. You can add a legend to the sheet. There are options to add graticules with coordinates and scale bars. The best way to learn to use the layout manager is by experimenting. "],
["terrain-analysis.html", "Chapter 4 Terrain analysis 4.1 What is terrain analysis 4.2 Lidar data 4.3 Working with Lidar data in QGIS 4.4 Colouring the DTM 4.5 Calculating slope and aspect 4.6 Calculating canopy height. 4.7 Investigating canopy height in 3D 4.8 Calculating raster stats for polygons.", " Chapter 4 Terrain analysis 4.1 What is terrain analysis -Analysis of topographic features (geomorphology)&gt; For example Slope Aspect Hill shading Flow Insolation Uses digital elevation models (DEM) Raster layer (i.e. pixels) Each pixel contains an elevation value DEMs vary in extent and resolution There are many uses for terrain analysis. Hydrologists can use it to delimit watersheds and analyse flows. The layers derived from terrain analysis can be queried to produce information at points, along lines or within polygons. We can also carry out raster algebra operations on the layers. How are DEMs produced? Traditionally made through interpolation between contours from a traditional map Now mainly through remote sensing involving some form of radar based sensor SRTM (Shuttle Radar Topography mission): Global extent 90m resolution Lidar (Light detection and ranging): Local extent, varying resolution 4.2 Lidar data Raw lidar data consists of a point cloud of returns with some information regarding intensity First return: Top of vegetation or buildings Last return. Ground Lidar point clouds can be processed using a range of different software. QGIS can be linked to LASTools. We will not process raw data in this class. Source for Lidar data http://environment.data.gov.uk/ds/survey/index.jsp#/survey?grid=SY98 To see some point clouds open http://plas.io/ There are some point clouds of Arne on the server. Download them locally then open with plas.io. 4.2.0.1 Action: Note that this is not an obligatory exerise. It is to illustrate the concept of a poiint cloud interactively Click on this link http://r.bournemouth.ac.uk:82/Quantitative_and_Spatial_Analysis/Week_2/point_clouds/ in Firefox to open up the folder. Select one of the files. Download the file to your PC by write clicking and saving the target (this doesn’t work in INternet Explorer). Open up the plas.io window by clicking on the link to it http://plas.io/ Now load the file using the browse button at the top. The result will be dark at first and you won’t see anything. Scroll down to find the intensity scaling and adjust this to a very narrow band on the left hand side until the cloud is visible. You will be shown this in the class. Experiment with other options to get a feel for the nature of the point cloud Zoom in and out and rotate the image with the mouse http://r.bournemouth.ac.uk:82/Quantitative_and_Spatial_Analysis/Week_2/point_clouds/ 4.2.0.2 Plos.io 4.3 Working with Lidar data in QGIS The Lidar point clouds for both Hengistbury head and Arne have been already processed to produce the digital terrain model (ground) and digital surface model (top of vegetation and buildings). You have loaded the layers for Hengistbury head already and you should have saved them locally. 4.3.1 Analytical hill shading Hill shading is a very commonly used technique, particularly in the field of Archaeology. It helps identify visible features as defined by their topography. As Lidar can penetrate forest canopies it can be used to identify Mayan ruins “lost in the jungle”. Running the hill shading algorithm is very easy. 4.3.1.1 Action: Make a hill shaded layer from the DTM Click on the raster menu at the top bar. Select analysis. Select hillshade Choose the input layer. Use the DTM you have loaded (You can try the DSM later to see the difference) Keep the default options. Click run to produce a temporary file output IMPORTANT NOTE. All temporary files are just that. Temporary. This avoids building up lots of files that you may not eventually use. But you must save them before closing the program if you are going to keep them 4.3.1.2 This is the result. You can now experiment with the technique by changing the parameters if you wish. Major changes away from the default settings will produce very ugly results, but small tweaks can help to show up features lit from different angles. 4.4 Colouring the DTM The DTM and DSM layers are still showing in the project with a simple grey scale. We can colour them in with palettes in order to produce a clearer map. 4.4.0.1 Action: Colour the DTM using a single band “pseudocolour” Right click the layer in the layers panel to open the menu. Choose properties. Choose Symbology Select the render type to be “single band pseudocolor” Choose a colour ramp Apply the result to the map Move (or ensure that it is already) the result up in the layers panel so it is visible at the top. Move the hill shaded layer to be just below the shaded DTM Adjust the transparency of the top layer (right click the layer, find transparency in the properties window, move the slider) so that the map is transparent over the hill shaded layer. 4.4.0.2 Steps 1 to 3 4.4.0.3 Inverting the color ramp. (Click color ramp and choose invert) 4.4.0.4 Colouring the layer 4.4.0.5 Adjusting the transparency 4.4.0.6 This is the result. 4.4.1 Adding contour lines We can make a vector layer of contour lines directly from the DTM. If you use the DSM then contours may be drawn around the edges of trees and forests, which could be useful for analysis in some cases, but may look odd. 4.4.1.1 Action: Make contour lines at 2m intervals Click on the raster menu on the top Scroll to “extraction” Choose contours Select the contour spacing Run the algorithm. You may wish to adjust they styling to alter the colour and width of the lines by right clicking to see the Properties menu, selecting Symbology and altering the options there. 4.4.1.2 Selecting contours from the menu 4.4.1.3 The result 4.4.1.4 Zooming in 4.5 Calculating slope and aspect 4.5.1 Slope Calculating the slope of hillsides is a basic operation in terrain analysis. More complex algorithms can use the slope layer in a range of applications, such as hydrological modelling (flows and watershed delineation) and insolation calculations. It is easy to produce a slope layer. 4.5.1.1 Action: Produce a layer showing slope in degrees Choose “analyses” from the raster menu at the top Select the slope option from the menu Select the DTM layer that will be used to calculate slope Run the analysis with default options to produce a new temporary layer. 4.5.1.2 Selecting the algorithm 4.5.1.3 Selecting the layer 4.5.1.4 The result You may want to shade this layer using the styling options to select a pseudocolor ramp, as you did for the DTM. 4.5.2 Aspect Calculating the aspect of each hillside leads to a layer that can be used as input to other analyses. It may be particularly useful for calculating insolation. South facing slopes will tend to be warmer and this may influence the distribution of organisms such as insects and reptiles. 4.5.2.1 Action: Produce a layer showing aspect in degrees Choose “analyses” from the raster menu at the top Select the aspect option from the menu Select the DTM layer that will be used to calculate slope Run the analysis with default options to produce a new temporary layer. 4.5.2.2 Selecting the layer Again you may want to change the colour scheme on the layer. 4.5.2.3 The result 4.6 Calculating canopy height. The difference between the DTM and DSM layers (first Lidar return and last Lidar return) is a measure of the canopy height in forests and even lower vegetation such as shrubland and heathland. You can produce a simple canopy height model through subtraction. To do this we use the raster calculator option. 4.6.0.1 Action: Produce a canopy height model Choose “raster calculator” from the raster menu at the top Type a name for the output layer into the box You could call it CHM for canopy height model Form the raster calculator expression by clicking on the first layer name i.e. DSM@1 ( the @ sign represents the first band, but there is only one band per layer) then the minus sign then the second layer. It should look like this “dsm@1” - “dtm@1” Run the calculator Shade the layer using a pseudocolor option 4.6.0.2 Running the calculation 4.6.0.3 The result 4.7 Investigating canopy height in 3D You can use the qgis23js plug in to produce a three dimensional depiction of the canopy height by draping the coloured layer over the DSM. 4.8 Calculating raster stats for polygons. It can be very useful to extract values from the terrain analyses and add them to vector layers. For example, you may want to find the slope, aspect, elevation etc. of some quadrats that you have placed in a study area. We will look at this again when carrying out field work at Arne. In the case of Hengistbury you may want to find the mean elevation, mean canopy height etc. of the area within each of the phase one polygons. You can do this using an option provided in the Saga tool kit. 4.8.0.1 Action: Add some derived statistics to the phase one polygons You will need some digitised polygons or polygons derived from another source. Activate the toolbox found under the processing menu at the top. Scroll down to the SAGA options. Find the option for “raster statistics for polygons” under the vector &lt;-&gt; raster menu heading. Select the grid(s) (Saga term for raster) that you will use by clicking the button on the right. The processing can be slow for large grids so just choose the CHM Choose the vector (polygon) layer you will use as an overlay. Select the statistics you want to calculate. Just choose the ones you actually need, as the process can be slow. Say, for example, minimum, mean and maximum canopy height. Press run and wait for the analysis to finish. This can take a while and could possibly hang if the layers are very large, as this is quite a complex operation. Open the resulting layers attribute table to see the results. "],
["introduction-to-the-rstudio-server.html", "Chapter 5 Introduction to the RStudio Server 5.1 Introduction 5.2 Getting started with the RStudio server 5.3 RStudio server concepts 5.4 Finding your way around the interface 5.5 Data abstraction and data visualisation 5.6 Working with markdown documents. 5.7 Conclusion", " Chapter 5 Introduction to the RStudio Server 5.1 Introduction Rstudio is a complete environment for working with the R language. Once you have got used to it you will find that it makes working with R far more productive than using the R console version. However the concepts involved in using RStudio will be completely new to most. The interface can seem very complex. RStudio provides an interface for working with R code, rather than an interface for running analyses directly. The code for conducting analyses will be provided for you in this course. You do not need to learn to write any Rcode yourself in order to produce graphics and run basic statistical analyses. You will also be able to reproduce more sophisticated quantitative and spatial analysis in RStudio by making minor adaptations to pre-built scripts. 5.2 Getting started with the RStudio server The RStudio server version runs directly through any web browser. There is no need to install any software on your laptop, PC or tablet Access to the server is through the following URL. This works both on and off campus. http://r.bournemouth.ac.uk:8789/ 5.2.1 Action: Log into the RStudio server Click on the URL in a browser. Use Firefox or Chrome. You will see a log in page. Log in using a username and the default password msc123 The usernames for this course have been set up to be your first name as shown in brightspace and surname initial, all in lower case. veronicab elizabethe garethh minniej curtisl esmel ricardol robm ttomasn georgiap andrewr molliet anuro msc1 msc2 guest phd1 If your name is not in this list, or you want to change your user name let me know. Do not log in using someone else’s name!. If there is any abuse of this I will show users how to change to a secure password, but we will assume trust by default. 5.3 RStudio server concepts Using a server can seem strange at first. The RStudio server is an integrated platform for doing the following … Saving and sharing data files Running analyses Compiling reports Connecting to data stores Sharing analyses with others. Amongst many other potential uses. It is a very powerful tool that is freely available for all those with a log in to use it. The more advanced features can be used without any programming skills through sharing scripts. However you do need to become familiar with some new concepts in order to use these. The RStudio server is ideal for collaborative work. You have been provided with a username and a password, as this also provides you with your own permanent space on the server for saving your own work and building up a portfolio of useful analyses. Only one person can be logged in at any one time under your username. However I can always log into your user space at any time in order to help correct your errors and to give you advice. 5.4 Finding your way around the interface Once you are logged in you will see three sections of the interface by default. This will change to four sections when you begin using scripts in the interface. Look carefully at the interface and learn to recognise the sections. The RConsole. This is showing up on the left hand side when you first log in. The console can be used for running R code interactively. There is a tab showing up labelled “terminal” as well. You won’t use this, as it is for more advanced programming. The environment, history and connections pane is at the top right of the screen. The environment tab is the one that is most used. This tab will show the data that is in the active workspace in R. The concept will only become clear after beginning to use R. The files, plots, packages, help and viewer tab at the bottom right. The files tab is the most important to understand at this stage. There will be no files in your home directory yet, nor will there be any folders. A key concept to understand when using the server is that your home directory on the server is like a directory (folder) on your PC. So ,it is rather like the university H drive. However it is all “encapsulated” on the server which is also running R. So it is distinct from your H drive and not directly linked. In order to move data files and scripts into your home directory you must upload them. You will see buttons labelled New Folder, Upload, Delete, Rename and More. If you click on the More button you will also find an option to Export your files. The upload and the export buttons are frequently used to move files onto the server and to directly move files off the server. It is very important to be aware of this concept. Files saved on the server will always be available for use later. In contrast active analyses that take place in the server memory, as opposed to the server’s hard disk space, will be temporary and will be lost between sessions. 5.4.1 Using projects in RStudio Just as we have seen previously in QGIS the use of projects is good practice. QGIS can be used without a project file, but if you do that you may find that you have to go through many steps to reload files and get back to where you left off. RStudio has a similar concept. We can use a project for each week’s work in R. Let’s start one called “intro”. Just as in QGIS each project should be associated with a single folder and all the work placed within this folder. The folder can be added when the project is first started. 5.4.2 Action: Form a new project and add a new folder Click on the file menu at the top left of the interface. Go to New Project You will see a window with three options to create a project. Choose the first option labelled New Directory The next window will show a range of advanced options. Ignore them and just select New Project You will now see a window with a prompt for the Directory name (and some other options). Type “intro” as the directory name. Click create project Look at the files pane in the bottom right corner. You will now see that after Home there is the word intro. You can also see a file called intro.Rproj in the folder. Click on home. You can see a folder called intro in your home directory. So .. you have created a new project and placed the project file within the folder. This is just like starting a project in QGIS. Click the folder again to open it. 5.4.3 Using the console The R console in the bottom left of the screen allows you to run R code interactively. In this course you will not be using this very much, as we will use pre-built R code that avoids the need to code your own. However in order to use the code you do need to gain some intuitive idea of what is happening when you do. 5.4.4 Action: Make a data object (vector) in R Carefully type the following code into the console x&lt;-rnorm(100). Make sure it is all lower case and typed exactly as shown. Press enter Look in the global environment window in the top right of the screen. You will see that a data object called “x” has been added. 5.5 Data abstraction and data visualisation The action you have taken so far involves a concept that will be completely new to most of you. We can refer to it as data abstraction. Most of you will be familiar with data in the form of visible numbers, text, or even maps. However programmers (and mathematicians) think of data in the abstract. We have a data object (in this case a numerical vector) called x. It consists of 100 numbers. These numbers have been simulated by R from a normal distribution. However data analysts tend not to look directly at the numbers themselves. They are much more interested in patterns in the numbers and relationships between variables. 5.5.1 Action: Visualise the data using a simple histogram Type hist(x) carefully into the R console in the bottom left of the screen and press enter. Notice that the plots tab becomes visible in the bottom right. It shows a histogram of x. So what has happened here? The first line of code (x&lt;-rnorm(100) generated a vector of 100 numbers drawn from a normal distribution with a mean of zero and a standard deviation of 1. This vector of numbers was placed in the computer memory. All the numbers existed, but we did not look at them. We could see the vector was active in memory as it appeared in the Environment pane. We then investigated the properties of the data object by plotting a simple histogram. These concepts will become clearer over time when using R. Notice that at this stage no data is actually saved as a file on the hard disk. It is present in the computer’s memory. 5.5.2 Action: Close the project Click on the project icon (marked intro) on the top right-hand side of the top menu bar. Go to “close project” You will be asked if you want to save the workspace. Click on save You will now return to a blank workspace. Click on the files tab at the bottom right and click on the intro folder Notice that the folder now contains an .Rhistory file, an .RData file and a file called intro.RProj. This was a tiny R project, but we all have to start somewhere. Programmers often begin learning how to make the language they are learning send a “Hello World” message to the console. So this was the equivalent of “Hello R World” Let’s go back to the project. 5.5.3 Action: Restore the project Click on project on the top right of the menu bar. You should see the word “Intro” in the window. If you had more projects then they would all be listed here. Note you could also use the open project option to go back to your project, but of course, at the moment you only have one project available. Click intro Go to the plots tab and notice that there is no histogram showing now. Click on the history tab in the top right panel. Notice that the code you wrote is shown there. Click on the console panel on the right to make it active. Press the up arrow on your keyboard once. Notice that the last command that you entered (hist(x)) is now shown in the console. Press enter. Notice that the histogram now shows in the plots menu. So, what’s going on?. When you answered “yes” to save the workspace both the data and the history of the R commands were saved in the folder. So the whole “analysis” can be recreated at any time. This will all become clearer over time. 5.6 Working with markdown documents. This course will concentrate on the use of markdown documents as a way of running R code. The advantages of using markdown are many. Embedded code can be either revealed to other users to show how the results were obtained or hidden to simply produce a report with embedded figures and statistics. Annotation of the results of an analysis can be embedded around the results to explain the key results. Very limited knowledge of the R language and syntax is necessary to adapt markdown documents in order to analyse your own data. With a little more knowledge and experience of R complex methods can be applied by altering markdown found on-line. 5.6.1 Action: Produce a default markdown document. Go to file on the top menu bar Choose “New file” Choose “R Markdown” You will now see a window in which you can type in a name for the title of your analysis. By default the name is “untitled”. Change that to something like “R demo”, or anything else you feel like. You will now see an untitled markdown document added to the top pane in RStudio. It is untitled, even though you’ve given it a title, as it is not saved as a document. Press the “knit” button on the top right pane. Now you are prompted to give the file itself a name. Call it whatever you want, maybe “R demo” again. After knitting the document you will be prompted to open a window to see it. Look at the document and understand what it consists of. The steps above produce a default “demo” markdown document. Every time you start a new markdown Rstudio will start off with this one. When you use markdown for real you will either open a document ready for knitting or replace the default demo text with your own. You should look at the logic of the document carefully. It consists of “chunks” of R code that produce output in the form of tables and figures embedded in text. The R code automatically produces output and adds it to the document after knitting. So if you have R code available that will run an analysis that you are interested in you don’t have to remember any other steps in order to run it. Simply ensure that the data that is being added to the analysis is appropriate for the type of analysis being run and you can obtain the same results with your own data. This will be the way R is used in this course. 5.6.2 Action: Clean out, rinse and repeat, for practice sake Close the project. Delete the whole folder in the files pane by selecting it and then choosing delete. Choose “clear project list” from the project menu at the top right. You are now back right where you started (apart from the .RHistory file in the home directory) Try all the above steps again to become familiar with the process before we begin more serious work. You will need to open a new project each week to keep all your work in good order and prevent confusion. 5.7 Conclusion You should now be able to start a clean project in RStudio. The code to actually run the analyses will be provided for you. So there is no need to fully understand R code to complete the unit. You will not be expected to write any code yourself. However, as you progress with R you will begin to alter code and adapt it for your own purposes. With time you will be able to write code yourself to run analyses. The key to the process is to become familiar with the general concept of data abstraction and to gain a feel for the nature of the data that is used for statistical analysis. "],
["basic-statistical-concepts-refresher-using-a-simple-example.html", "Chapter 6 Basic statistical concepts. Refresher using a simple example 6.1 Inferential statistics 6.2 Calcuate the standard error of the mean. 6.3 Test whether the statistics “work” 6.4 Planning an analysis 6.5 Quantitative analysis 6.6 What does quantitative data consist of? 6.7 Data visualisation vs statistical analysis 6.8 A simple example 6.9 Graphical data analysis 6.10 Forming a plot in R 6.11 A more complex time series 6.12 Some high level code to break down the data 6.13 Inferential statistics", " Chapter 6 Basic statistical concepts. Refresher using a simple example It may have been a while since you last learned about statistics. Statistical software now makes running the calculations themselves very easy. Although using R may seem rather challenging at first, it is really much faster to use R than to carry out statistics “by hand” or using non dedicated software such as Excel However the problem with using R for statistics is that many students concentrate attention on learning to use the software rather then remembering the concepts. In this class we will refresh the most basic ideas in statistics without using any software. We will then see how to use R to make life easier. 6.0.1 Getting some data Let’s look at the reaction times on a simple test for the students in the class. Click on the link below. Run the test. After a “burn in” period to get used to the idea, record your own time in ten trials. 6.0.2 Action: Get some personal data Click on the link, https://www.humanbenchmark.com/tests/reactiontime/ Try your reactions three times as a “burn in” Record ten trials and write down the times on a piece of paper. Calculate your mean reaction time using a calculator, a piece of paper or any other method. Hint: The mean is the sum of all the times divided by the number of trials Write your own results on the white bowrd. Ok we now have some data from the class. We will now go a step further. 6.0.3 Action: Summarise the class data Find the maximum. FInd the minimum FInd the median reaction time. To do this, place the results in descending order. Find the number in the middle of the table. If there is an even number of results then average the two numbers at an equal distance from the top and the bottom. Find the mean reaction time for the class. These are basic descriptive statistics. 6.1 Inferential statistics This part of the exercise is more subtle. We will spend a lot of time during the classes discussing the nature of statistical inference and the importance of testing assumptions. For the moment, let’s just do the maths. 6.1.1 Action: Calculate the standard deviation “by hand” Subtract the mean reaction time from each observation to produce a table of differences from the mean. Some will be positive, others negative. Try adding all these differences together. What do you get? Why? Calculate the square of each difference. Add together all the squared differences to find the sum of the squares. Divide the sum of the squares by n-1 (where n is the number of observations) Take the square root of this number. Just out of interest You don’t have to follow this yet, the following R code does this operation “by hand” data&lt;- c(440,340,350,456,470) ## Five observations of reaction time x&lt;-data-mean(data) # Subtract the mean from the data x&lt;-x^2 # Square the results x&lt;-sum(x)/4 # Divide by n.1 sqrt(x) # Take the square root ## [1] 61.45893 And again, just out of interest, statistical software does all this (and a lot more) with a simple function. sd(data) ## [1] 61.45893 6.2 Calcuate the standard error of the mean. The standard errror of the mean is the standard deviation divided by the square root of the sample size. The 95% confidence interval for the mean is approximately two times the standard error. 6.2.1 Action: Calculate the standard error and 95% confidence interval for the mean Divide the standard deviation by the square root of the sample size. Multiply this by two. Provide a range by adding this number to the mean and subtracting it from the mean. 6.3 Test whether the statistics “work” 6.3.1 Action: Rinse and repeat. Conduct the experiment again. This time just calculate the mean. Does the mean fall inside the confidence intervals you found? Discuss the results. 6.3.2 Conclusion There is a lot to discuss here. We will look at the nature of inferential statistics in more detail as we go on. Practice calculating standard deviations and standard errors “by hand” in order to get a feel for the process. Once you understand the calculations you can leave the rest to the computer. However you will have to understand the assumptions involved in calculating a standard error in order to apply statistics correctly. We will go thought this carefully later. 6.4 Planning an analysis The steps for any successful research project which has a quantitative component are; Decide on one or more interesting and tractable scientific questions. Design an experiment or field survey that will produce data that addresses the question(s). Decide how the data you will collect relates to your scientific question(s). This step will involve re-framing some or all of the questions as statistical hypotheses and/or statistical models. Decide on the planned analysis that you wish to conduct on your data. These should effectively summarise key features, describe patterns, test hypotheses, establish associations between variables and/or estimate effect sizes and parameter values. Test your analysis carefully (preferably using dummy data and/or power analysis based on a small sample) to ensure that you know how to run them. Critically evaluate whether your field work is likely to provide suitable data (sufficiently replicated) in a suitable format. Make contingency plans and decide on alternative analysis in case data do not meet the assumptions needed for the planned analysis. Collect the full data set. Investigate the properties of the data you have collected in the light of the assumptions used in your analysis. Run the planned analysis (or the contingency analysis) and obtain the appropriate data summaries, test results, model output, tables and figures. Interpret and discuss the results. Although this course concentrates on the analytical component of this research sequence, the initial steps rely on knowledge of the specific system you are studying. Quantitative methods allows you to apply powerful computational tools to your own research, but these tools will only be effective if the right questions are formulated from the start. It is often stated that the best questions are the simplest because they lead easily into the formulation of clear hypotheses or models. This is good advice. However ecological research inevitably deals with complex systems involving multiple variables. The most appropriate analysis for MSc research projects can involve computational methods that go beyond the statistical tests found in introductory texts. Without a working knowledge of the available techniques it can be difficult to extract all the information from your data and link the results to the motivating questions. 6.4.1 Understanding your data before you start colllecting them In a statistical text book aimed at undergraduates, Dytham (20014) makes the following suggestion. “Collect dummy data (i.e. make up approximate values based on what you expect to obtain). The collection of ‘dummy data’ may seem strange but it will convert the proposed experimental design or sampling routine into something more tangible. The process can often expose flaws or weaknesses in the data- collection routine that will save a huge amount of time and effort.” “I implore you to use this. I have seen countless students who have spent a long time and a lot of effort collecting data only to find that the experimental or sampling design was not quite right. The test they are forced to use is much less powerful than one they could have used with only a slight change in the experimental design. This sort experience tends to turn people away from statistics and become ‘scared’ of them. This is a great shame as statistics are a hugely useful and vital tool in science.” I completely agree with this author on this point. I have had exactly the same experience. Year after year I have been asked by students to help them analyse their data only to find that they did not plan the data collection with any particular analysis in mind. This never works. Not only is the analysis not likely to show much, but the data will rarely be structured in a manner that allows an analysis to be run at all! This leads to a pre-write up panic and post write-up anguish about what might have been. So, data collection and data analysis should be planned together. There is no realistic alternative. If you understand how an analysis works you will also understand what sort of data are needed to run it. As you are still learning the basics of quantitative ecology there is clearly something of a chicken and egg situation here. How can you know which analysis you are going to need if you still have not learned enough to choose from? This is a valid point. Nevertheless there are some simple concepts that you can use that avoid many problems later on. In addition to forming a “dummy” data set it is often possible to run some form of power analysis on a small subset of data collected as part of a pilot study. This can indicate how large a sample you may need in order to show statistically significant results. This step is often ignored, but it inmensely strengthens any research proposal and can also be used as a means of formally explaining why the data did not provide firm evidence in support of a scientific question even after the main study has been completed. 6.5 Quantitative analysis Anyone carrying out research is keenly aware of the importance of quantitative analysis. However many students, and even some researchers, approach quantitative analysis with some trepidation. It is a vast and potentially complex field. The most common worry revolves around statistics. Frequently asked questions include “Which test should I choose for my data?” “How do I know that I have chosen the right test?” “What does the test tell me?” “How do I interpret the results?” There are all important questions to ask. However some of the worries about statistics may be the result of over emphaisising the word “test”. The most important question is in fact “how can I use all the data I have available to address a meaningful research question?” This may, or it may not, involve applying statistical tests. It almost certainly will involve visualising your data, either in the form of maps, figures or both. It will always involve getting to know the nature of your data in depth and communicating your knowledge to other people in a direct and meaningful manner. Sometimes statistical tests and models are absolutely essential. In other cases they may not be useful at all. Sometimes you may be able to select an appropriate analysis quite easily from a set of standard, commonly applied methods. In other cases you may need to seek expert advice from an experienced researcher ,or statistician, in order to extract the most from your data, avoid pitfalls and fully justify your conclusions. Sometimes a simple shaded map, or a few well chosen figures, may answer the research question directly. Sometimes that approach would just be dismissed as being “merely descriptive” in which case you would be expected to have applied much more sophisticated inferential statistics. It all depends on the nature of the research and the nature of the data at hand. In a single semester it is impossible to teach all the methods that may be needed in order to analyse a data set of the sort collected for a dissertation. Observational data sets are typically very esoteric in nature, and so tend to be challenging to analyse and may require quite advanced techniques. Careful thought and some creativity is usually needed to conduct any meaningful analysis. The more statistical methods that you know about, the more likely it will be that you will find an appropriate analytical tool to address your research question. Often you will need some help to find an appropriate method, but if you are aware of the concepts behind inferential statistics you can frame better questions. 6.6 What does quantitative data consist of? This question appears at first sight rather trivial. Surely the key is in the name? Quantitative data consists of quantities, i.e. numbers, so we need to do some sort of maths to analyse them. However this simple definition hides many subtleties and potential pitfalls, as we will see It is often far from clear what sort of variables we are dealing with and how they are defined. Variables are the column headers in a correctly formatted data frame. There are fundamentally two sorts of entries that can be typed into the rows below the column headers. Text and numbers. Let’s keep it all very simple for now. If a column contains rows of text then the variable is categorical, so it will be considered to be a “factor”. If the column contains rows of numbers, then the variable is a numerical variable of some form. There are many variations on this basic theme to consider, but once you have gained some clarity about this basic concept, many elements of data analysis will fall into place. So: Text will (nearly) always be interpreted as a factor Numbers of any sort will (nearly) always be interpreted as a numerical variable Factors have as many levels as there are unique entries in the data column. So if the header were “gender” we would have a categorical variable (factor) with two levels. If it is site we will have as many levels as sites. We could go onto to a mroe detailed break down of the differences between various forms of numerical variables, but we’ll leave it at that until later. 6.7 Data visualisation vs statistical analysis Traditional statistical text books place probably place too much emphasis on the statistics, and too little emphasis on producing good clear data visualisations. If a picture is worth a thousand words it is also worth at least a thousand confusing and incomprehensible statistics. Good figures are often the most informative and useful elements in any modern data analysis. This is good news, as most people find figures much easier to understand than raw numbers. Once you become aware that a figure is not only easier to understand than a statistical test but that it is also often the best way to answer the key scientific question, some of the worry surrounding the issue of statistics can be removed. 6.8 A simple example Let’s frame a very straightforward scientific question and try to answer it with simple data. “How have global atmospheric CO2 concentrations changed since the start of the industrial revolution?” Data available from here. https://www.co2.earth/historical-co2-datasets 6.9 Graphical data analysis Let’s read the data into R. Its on the server in a folder beneath your working directory. This will read it. d&lt;-read.csv(&quot;/home/msc_scripts/data/co2.csv&quot;) 6.10 Forming a plot in R Throughout this course we will use the modern graphics facility in R provided by the ggplot2 library as much as possible. This provides a unified approach to forming figures, that is slightly more difficult to learn to use than base graphics, but much more powerful. These lines load the library and set up a theme for the plots. There are various themes that change the overall look of the plots. The default theme has a grey background, and it is often better to have a cleaner looking black and white theme. library(ggplot2) theme_set(theme_bw()) ## Set the theme to black and white Now, to set up a plot we first define the aesthetics. The aesthetics refer to the way the variables are mapped onto the plotting scheme. We often have an x and y axis. This line sets these up. g0&lt;-ggplot(d,aes(x=year,y=co2)) ## Set up the base plot Nothing seems to have happened. However the object is being built up ready for plotting. This can then be used in various ways. Now we can use a “geometry” to form a plot wth a line. Geom_line does this. g0&lt;-g0+geom_line() When we type the name of the object it will now be plotted. g0 However we have just the variable names along the axes. It might be better to be more explicit about what is being plotted. g0&lt;-g0+ylab(&quot;Atmospheric Co2 concentrations (ppm)&quot;) g0&lt;-g0+xlab(&quot;Year&quot;) g0 There are a wide range of different geometries that can be used to add to the plot. I will provide example script lines for many of these that can be reused to make other plots. Say we need a reference vertical line in red to mark the start of the industrial revolution. We can add that using another line. g0&lt;- g0 + geom_vline(xintercept =1790,colour=&quot;red&quot;) g0 This is not a very sophisticated figure, and a very similar figure could have been produced using Excel or similar software. However the nice thing about R is that the code lines set everything needed to make the graph and they are reusable. If the initial setup line used a different set of data the figure would all be redrawn in the same way. The point is that the figure alone does tell most of the story in this case. It is quite apparent that atmospheric CO2 concentrations have increased over time. The red line added to the figure represents the start of the industrial revolution and we can see a consistent increase in concentrations dates from around this time. We do not need to run any additional statistical tests in order to address the question using these particular data. All that is required is one clear figure, which conveys the message. This is not to imply that atmospheric scientists do not use inferential statistics, nor that a complete, formal analysis of the evidence for increasing levels of C02 in the atmosphere would not be based on some form of statistical modelling. However the raw numbers included in this particular data set are aggregated measures of global CO2 concentrations. They are not appropriate input to any form of inferential statistical test. We use formal statistical inference when we do need to formally quantify uncertainty regarding the evidence we are using to address a scientific question. In this particular case there are many areas of genuine uncertainty, but we cannot address these through analysing the raw numbers as provided in this simple data set alone. Uncertainty arises due to the use of proxy measurements for historical CO2 levels rather than direct instrumented measurement. This is totally invisible in the data set. The data as given provides no measure of potential variability in CO2 levels from place to place. The data form a simple time series. Although the frequently used statistical techniques that you will learn more about on this course, such as linear regression, can sometimes be applied to time series, time series data points are not independent of each other. If CO2 concentrations are increasing year on year then the levels one year will depend on the level the previous year. So, the series of points lacks both replication and independence. Replication and independence are basic requirements for many statistical methods, although there are tools available to deal with some lack of independence. Data such as these are not suitable as input to standard statistical tests nor to conventional statistical models. It is worth pointing this out right from the outset. Don’t jump to the use of inferential statistics until you need to use them. Similar situations often arise when data is collected for a dissertation but they may not be spotted as easily. “Which statistical test do I need to use to tell if there is a correlation between industrialisation and CO2 concentrations?” is totally the wrong question to ask of these data as they stand. Furthermore, even if it were possible to use a statistical test, setting up a null hypothesis of no difference between pre and post industrialisation atmospheric CO2 concentrations would make no sense, as there is a clearly understood and independently measurable process (CO2 emissions) that is responsible for the change. 6.11 A more complex time series To illustrate the point further, look at this more detailed time series. It also consists of measurements of atmospheric CO2 concentrations. There were taken at the Mauna Loa observatory in Hawaii. data(co2) plot(co2) You can easily see the general increasing trend. However there is also some consistent seasonal variability imposed on top of the trend. This arises because vegetation in the Northern Hemisphere takes up CO2 when it is growing in the spring and summer, and releases CO2 through decomposition of organic material during the winter. So we have a more complete data set and we may be able to answer more complex questions, such as how much interanual fluctuation exists in CO2 concentrations and whether these seasonal fluctuation have changed over time. These are not questions that involve purely random noise, so once again they are not suitable for simple, text book, introductory statistical tests. 6.12 Some high level code to break down the data If this course were in fact to be aimed at teaching methods for analysing seasonal dependence in time series we would now go into great depth regarding a set of appropriate and specific analyses that could be applied to these sort of data. It is not, so we won’t do that at this point. However, just to illustrate the power that comes from being able to get the data into R, look at the ease with which we can run quite a sophisticated visual breakdown of the data. It simply needs a single line of R code. plot(stl(co2,s.window=12)) We now have a complex figure that shows the raw data in the top panel, the fluctuating seasonal pattern in the second, the underlying trend in the third, and residual, random, “white noise” variability in the bottom panel. The only strictly stocahstic (random) element in this time series is shown in the lower panel. The analysis can be run with a range of different parameters and the resulting breakdown of the data can be fed into more detailed further analyses. We won’t go into any of that at this stage. However, imagine how difficult it would be to produce a similar figure in Excel. Knowledge of R allows you to quickly apply sophisticated methods that have been developed by domain experts in a specific field very easily. All that is needed to get started is a reasonably good understanding of data formats and a basic understanding of some simple R syntax. 6.13 Inferential statistics So, what is the difference between inferential statistical analysis and non inferential data analysis? Fundamentally, all inferential statistics are about estimating some key parameters from data, rather than precisely measuring them. If we do have precise measures then we don’t need any inferential statistics at all. In the ideal case we could do away with all inferential statistics completely. There is nothing intrinsically scientific about using statsitical analysis. Many physicists tend to avoid stats completely. However most ecological studies do need inferential statistics of some description simply because we don’t have precise measures and the systems we are looking at tend to be very variable. So they are a necessary evil. "],
["confidence-intervals-for-univariate-data.html", "Chapter 7 Confidence intervals for univariate data 7.1 Introduction 7.2 Reminder: Basic statistics 7.3 Measures of central tendency 7.4 Conclusion and take home messages", " Chapter 7 Confidence intervals for univariate data 7.1 Introduction How to calculate a standard error The difference between the standard error and the standard deviation The concept underlying the sampling distribution of the mean The use of the t-distribution for small (n&lt;30) samples How to calculate a confidence interval The logic underlying statistical inference from samples We will load some data into R and just work with a single variable called LShell. This is a vector of mussel shell lengths. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/mussels.csv&quot;) attach(d) 7.2 Reminder: Basic statistics 7.3 Measures of central tendency 7.3.0.1 The median Measures of central tendency are useful summaries of a variable that should already be familiar. They are often referred to as ``average’’ values. There are two main measures we can use. The mean and the median. The median is the value in the middle of the sorted data. Once the data are placed in order, a reasonable way to find the centre would be to locate the middle ranked value. In the case of an odd number of observations finding the middle is obvious. Say, the numbers are 3,5,6,7,9,: The median is 6 There is a slight problem in the case of an even number of observations. To see this think about four numbers, say 3,5,7,9 .The middle number isn’t either the second (5), nor the third (7). We take the mean of the two. If n is odd:The median is the number with the rank (n+1)/2 If n is even we take the mean of the numbers with ranks n/2 and n/2 +1 median(Lshell) ## [1] 106.9 7.3.0.2 The mean Finding the value of the mean in R is easy. Just type mean. mean(Lshell) ## [1] 106.835 The equation for calculating a mean is written. \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\) In words, this says “Add up all the numbers and divide by the number of entries”. To find n we ask R for the length of the vector. n&lt;-length(Lshell) n ## [1] 113 Now we can get the total using sum. Total&lt;-sum(Lshell) Total ## [1] 12072.35 So the mean is the total over n. Mean&lt;-Total/n Mean ## [1] 106.835 If you have followed this calculation in R you should be able to use the same technique to break down some more complex equations. However R has built in functions for everything you ever need to calculate, so there is never any need to calculate any statistics step by step except the improve your own understanding of the logic. 7.3.1 Measures of variability 7.3.1.1 Range The range is simply the distance between the maximum and minimum values. min(Lshell) ## [1] 61.9 max(Lshell) ## [1] 132.55 max(Lshell)-min(Lshell) ## [1] 70.65 7.3.1.2 Quantiles The median is a specific example of a quantile. Quantiles are points taken at regular intervals from the cumulative distribution function (CDF) of a random variable. In other words they divide ordered data into q equal-sized data subsets. The 2-quantile is called the median If the data are split into 4-quantiles they are called qua tiles (Notice that quartiles is spelt with an R rather than an N) If the data are split into 10-quantiles they are called deciles Another general term is percentiles. These are calculated in the same way depending on where the breaks fall. To get the qua tiles in R write summary(Lshell) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 61.9 97.0 106.9 106.8 118.7 132.6 At this point you should look back to boxplots. How do these numbers relate to the elements shown? The boxplot uses quartiles. How? The line in the centre of a boxplot shows the median. The box extends to the quartiles, so that it includes 50% of the data. The whiskers extend to the farthest data point that is not considered to be an outlier. Points beyond the whiskers may be outliers (we will see more about this later). More generally you can specify the specific quantiles you want to find. quantile(Lshell,c(2.5,5,95,97.5)/100) ## 2.5% 5% 95% 97.5% ## 76.04 80.74 128.16 130.82 7.3.1.3 The standard deviation The standard deviation is the most commonly used summary of total variability of a sample. In words, we calculate the standard deviation by subtracting the mean from each value, squaring the result and then adding up these values in order to find the sum of squares. We then divide by n-1 and take the square root. There is a slight complication regarding the standard deviation. When we collect our data we usually only obtain a sample from a larger population of observations we could have made. The procedures that fall under the heading of statistical inference attempt to estimate the properties of this larger (maybe even infinite) population from the sample we have taken from it. There is therefore a difference between the population standard deviation, that we rarely can ever know, and the sample standard deviation that we can calculate directly. The population standard deviation is given the symbol \\(\\sigma\\) and the sample deviation the symbol s. The formula for the sample standard deviation s that is an``unbiased estimator’’ of \\(\\sigma\\) is \\(\\sigma=sqrt{\\frac{1}{n-1}}\\sum_{i=1}^{n}(\\tilde{x}-x_{i})\\)\\(^{2}\\) The steps in calculating s are as follows. First the sum of the squared deviations from the mean. Notice that if we did not square the deviations they would sum to zero. Notice that in R the &lt;- operator assigns the result of a calculation to a variable. This can be useful if you wish to perform more calculations on the result. Writing the name of the variable prints out its value. sumsquare&lt;-sum((Lshell-Mean)^2) sumsquare ## [1] 24678.04 Now find the mean square. meansquare&lt;-sumsquare/(n-1) meansquare ## [1] 220.3397 And finally the root mean square. rootmeansquare&lt;-sqrt(meansquare) rootmeansquare ## [1] 14.84384 The mean square is also known as the variance and the root mean square is the standard deviation. We can check that they are the same. meansquare ## [1] 220.3397 var(Lshell) ## [1] 220.3397 rootmeansquare ## [1] 14.84384 sd(Lshell) ## [1] 14.84384 7.3.2 The normal distribution One of the main motivations for calculating the standard deviation is that if the observations form a symmetrical normal distribution we can predict where the bulk of the observations will lie if we know the mean and the standard deviation. A normal distribution is shown below If we shade in the area that extends out from the mean to a distance of one standard deviation we will have shaded in 68% of the area under the curve (Figure 4). So if (and this is often a big if) we are justified in assuming that the data were obtained from an approximately normal distribution we have a useful summary of the population in the form of the mean and the standard deviation. You should read more details regarding the properties of the normal distribution in Crawley. 7.3.3 Checking data properties with figures Both boxplots and histograms are very useful tools for checking whether the data values in a variable are approximately normally distributed. A normal distribution results in a more or less symmetrical histogram if the sample is large. If the sample is small the histogram is unlikely to be completely symmetrical as a result of sampling effects, even if the sample is taken from a population with an approximately normal distribution. Always be very careful when interpreting small samples. The whiskers of a boxplot extend (more or less) to the extreme that would be expected under a normal distribution. If there are many outliers beyond this point it is a good indication that the normal distribution may not be a good fit to the data, especially if all the outliers fall on one side. For example, lets see what some intentionally skewed data look like as a histogram and boxplot. x&lt;-rlnorm(100,mean=2,sd=1.2) hist(x) boxplot(x) A few outliers are to be expected in all data sets. They may be genuine “errors” or they may turn out to be the most interesting elements of the data. However in this case there is clear visual evidence of skew. Histograms and boxplots are good starting points for getting a feel for how the variability in data is distributed. There are other more sophisticated techniques for looking at data distributions and checking modelling assumptions that we will look at in detail later. 7.3.4 Calculating the standard error The standard deviation summarises the variability in the data (the deviations from the mean) in one single value. This can be very useful for descriptive purposes. A rather more subtle concept is the standard error. The subtlety arises because the standard error is used for inference. Remember that statistical inference involves estimating some properties of the population from which our sample has been drawn. The standard error (of the mean) provides us with information regarding the confidence we can place on the estimate of the population mean. The standard error is very easily calculated if we already have the sample standard deviation. It is simply the standard deviation divided by the square root of the sample size. \\(SE_{\\tilde{x}}=\\frac{s}{\\sqrt{n}}\\) n&lt;-length(Lshell) se&lt;-sd(Lshell)/sqrt(n) se ## [1] 1.396391 However the interpretation of the standard error is more subtle. The standard error is based on the concept of the sampling distribution. It is in effect the standard deviation of the distribution of the means we would get if we continually drew many new samples from a target population. 7.3.5 The sampling distribution of the mean The sampling distribution of the mean is the distribution that you would obtain if you took a large number of samples of the same size as the one you actually obtained and looked at the distribution of all the mean values. To make this clearer let’s try a simple experiment. In R we can draw random samples from a column of data easily. Lets take a sample of 10 mussel shells at random, look at their lengths and calculate the mean. samp&lt;-sample(Lshell,10) samp ## [1] 93.9 106.0 118.9 76.6 124.3 127.7 121.5 105.5 99.6 116.7 mean(samp) ## [1] 109.07 We could repeat this experiment many times. You could do it again by repeating the code. samp&lt;-sample(Lshell,10) samp ## [1] 91.5 102.8 108.2 94.9 121.5 125.9 97.0 99.5 103.3 101.8 mean(samp) ## [1] 104.64 You can see that the result is not the same, as the sample was taken at random. We can tell R to take 1000 samples like this from the original vector, calculate the mean from each and then look at the distribution of all the resulting thousand values. This would simulate the spread in values for the mean that we would obtain if we carried out a lot of small surveys that took only 10 mussel shells from the same population resamp&lt;-replicate(1000,mean(sample(Lshell,10))) hist(resamp,breaks=20,xlim=c(90,120),main=&quot;Sampling distribution of the means of 10 shells&quot;) Now if we draw samples of 30 shells instead of 10 and calculate the mean for each, what would we expect? Let’s try this and plot the result. resamp&lt;-replicate(1000,mean(sample(Lshell,30))) hist(resamp,breaks=25,xlim=c(90,120),main=&quot;Sampling distribution of the means of 30 shells&quot;) A sample size of 50 should reduce the spread of mean values still further. resamp&lt;-replicate(1000,mean(sample(Lshell,50))) hist(resamp,breaks=40,xlim=c(90,120),main=&quot;Sampling distribution of the means of 50 shells&quot;) Notice that in all cases the histogram suggests a symmetrical, normal distribution. The distribution becomes narrower as the sample size increases. So the outcome of all this is to demonstrate that the standard error is a manner of formalising this so called ``sampling’’ process mathematically. It provides a measure of the confidence we have in an estimate of the mean value calculated from our sample. For large samples, a 95% confidence interval for the mean is produced by multiplying the standard error by approximately 2 (the precise value is 1.96 for infinitely large samples). We expect the true mean for the population to fall within this interval 95% of the time. Notice that this is not a measure of variability (there is only one true mean). It is an estimate of a parameter expressed with uncertainty. So there is a very important distinction between the standard deviation and the standard error.. The standard error changes with sample size. The standard deviation does not (although the estimated standard deviation may change slightly at low sample sizes) To reiterate. We can reduce the standard error by increasing the sample size, as we divide by the square root of n. However we can’t reduce the standard deviation by drawing a larger sample. The sd is a measure of the natural variability which is always present. All that happens to the standard deviation when we take a small sample is that we have a worse estimate of it, as the differences between individuals may vary randomly depending on which we happen to pick. Recall the way we simulated some artificial data in R. To get each simulated height we used the mean + the simulated variability with known standard deviation mean&lt;-176 #This is the &quot;expected value&quot; for the population variability&lt;-rnorm(1,sd=6) #Each measurement will differ from the mean height&lt;-mean+variability #The values we get are a combination of the two The intrinsic variability is often referred to as the “error” in statistics, but it this is not necessarily an error in the colloquial sense of a “mistaken” measurement. The true population mean is fixed, it does not vary. However our estimate of it does, unless we measure every individual in the population. We draw inferences about it from the sample. This is the basis of many classical statistical tests. 7.3.6 Calculating confidence intervals Whenever you see a standard error in the published literature remember that the rough rule of thumb is that two standard errors represent a 95% confidence interval for the mean. However it is only a rough estimate. As samples become smaller you need to multiply the standard error by a larger value. Small sample (n&lt;30) inference relies on the t distribution. The t-distribution is wider than the normal distribution and corrects for the fact that we have to estimate the population standard deviation from the sample. The t distribution becomes “fatter” as the sample size becomes smaller (degrees of freedom = n-1) so we need to multiply the SE by a larger value in order to obtain the same confidence interval. Notice that small samples therefore lead to a “double whammy”. The SE is large because n is in the denominator when we calculate it from the sd, and the value for t is larger because we have a worse estimate of the standard deviation from the sample. Notice as well that with sample sizes of above 10 the adjustment is not very great, and once the sample size reaches 30 the adjustment flattens out and hovers at a value just over 1.96, which it will eventually reach at a sample size of infinity. Hence the approximate, but generally appropriate, statement that the confidence interval is more or less 2 standard errors around the mean. It is only really importantly larger than this if the sample size is below 10. So to get a 95% confidence interval with a sample of size 10 we multiply the standard error by t as found by the R function below, instead of using 1.96 which would be the case for a large sample. t&lt;-qt(0.975,10-1) t ## [1] 2.262157 This cuts off the tails of the t-distribution. To calculate a confidence interval step by step in order to illustrate the concept we need the following elements. SD&lt;-sd(Lshell) SD ## [1] 14.84384 Mean&lt;-mean(Lshell) Mean ## [1] 106.835 n&lt;-length(Lshell) n ## [1] 113 SE&lt;-SD/sqrt(n) SE ## [1] 1.396391 t&lt;-qt(0.975,n-1) t ## [1] 1.981372 The confidence interval is then calculated using the standard error of the mean and the value for t that corresponds to the cut off point of the distribution. Mean-SE*t ## [1] 104.0682 Mean+SE*t ## [1] 109.6017 See Crawley, chapter 4 for more details. To get a confidence interval in one step using R we can run a one sample t-test. t.test(Lshell) ## ## One Sample t-test ## ## data: Lshell ## t = 76.508, df = 112, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 104.0682 109.6017 ## sample estimates: ## mean of x ## 106.835 The output also shows the result of testing the null hypothesis that the true mean is actually zero. This is not a sensible test in this case but we can just ignore the ``test’’ part of the output and look at the confidence interval that has been calculated for us. The code below also finds the confidence interval. It involves fitting a simple statistical model. You will see why this works later in the course. confint(lm(Lshell~1)) ## 2.5 % 97.5 % ## (Intercept) 104.0682 109.6017 7.4 Conclusion and take home messages While it is good to know how the mathematical mechanism works,in order to design a study you only really need to remember the following rules. The sample standard deviation provides an estimate of the true population sample deviation. The standard deviation is root mean squared error of the deviations from the mean. The standard error of the sampling mean is the standard deviation divided by the square root of the sample size. The t distribution corrects for the fact that the sample standard deviation is only an estimate of the true value and so adds in a “fudge factor” that increases the width of the 95% confidence interval to be more than two standard errors around the mean. The t distribution adjustment is not very large once the sample size is greater than 10. The confidence interval shrinks to become narrower as the sample size increases, and it does so as a more or less linear function of the square root of the sample size (allowing for a correction for very small samples using the t statistic). So to reduce the width of the confidence interval by a half you need to take four times the number of samples (again, this rule applies after the sample size has reached around 10 in order to ignore the effect of the t correction) The reliability of the confidence interval remains unchanged at any sample size you use. What does change is the precision. Larger samples lead to narrower, hence more useful, confidence intervals. You can obtain a reliable 95% confidence interval using any sample size above 3. However it may be too broad to be useful if the sample is small. "],
["using-figures-to-understand-and-present-data.html", "Chapter 8 Using figures to understand and present data 8.1 Quick exploratory plots 8.2 Introduction to grammar of graphics plots 8.3 Simple bar charts 8.4 Simple Line charts 8.5 Histograms 8.6 Boxplots 8.7 Confidence interval plots 8.8 Scatterplots 8.9 Generalised linear models 8.10 Binomial data 8.11 Learning more about ggplots", " Chapter 8 Using figures to understand and present data Visualising data in the form of figures is the most essential element of all data analysis. Figures are used throughout the workflow of an analysis. We could break the process down into three main stages, although the bounudaries betweeen each are not hard and fast. The first figures in any anañysis are usually exploratory in nature. These sort of figures will not make their way into the final write up of the analysis. However they are essential for making sense of the data. It is very difficult to understand data simply as a set of numbers. It is much easier to understand data when the numbers are turned into some sort of visual representation. Although you should keep a copy of these figures and make some notes on them they are only really for your own reference. The second class of figures are important when choosing how to apply statistics to the data. These figures are diagnostic in nature. They allow the analyst to evaluate whether the assumptions of the model (or test) are met. Some of these figures should be included in appendices and supplementary materials as they can be used to justify the choice of statistics. Some may be used in a publication if it is essential to show how the analytical method was chosen. The final class of figures are those that will always make it into the main body of the final report. The important elements to think about when designing and selecting figures for the results section are clarity and relevance. The figures need to clearly display relevant elements of the data that help the reader to draw conclusions. The complexity of the conclusions depend on the nature of the study. Inferential statistics are not always possible nor necessary in order to tell the story that is contained in the data. Often the simplest figures prove to be the most effective. Libraries used library(tidyverse) library(aqm) library(sf) library(mgcv) library(plotly) 8.1 Quick exploratory plots Base graphics in R produce quick, simple figures that can useful in the intial stages of analysis. For example, if we have a variable called height in a data frame called d then just typing hist(d$height) in a code chunk or the console produces a quick histogram. Simple boxplots and scatter plots are just as easy. data(arne_2019) hist(pine_hts$HeCM) boxplot(pine_hts$HeCM) plot(pine_hts$DiMM,pine_hts$HeCM) If one of the variables is categorical then a call to a base plot adjusts to form a boxplot rather than a sctterplot pine_hts$Site&lt;-as.factor(pine_hts$Site) plot(pine_hts$Site, pine_hts$Age) Base graphics are really quick to use and they don’t involve typing long pieces of code. Although they can be customised to produce really neat figures, they tend to have a rather “retro” look and feel. So it is worth learning how to make modern looking graphics. 8.2 Introduction to grammar of graphics plots In order to produce figures that can be really honed up to publication quality we will use the package ggplots2. Grammar of Graphics plots (ggplots) were designed by Hadley Wickam, who also programmed dplyr. This is “tidy” approach to programming that is more powerful than base R. The syntax differs from base R syntax in various ways. It can take some time to get used to. However ggplots provides an extremely elegant framework for building really nice looking figures with comparatively few lines of code. 8.3 Simple bar charts The simplest figures of all are probably barcharts. Barcharts are used when the data consist of counts or percentage. Sometimes barcharts have been used to show means. These sort of barcharts with confidence intervals are inferential and are also known as “dynamite charts”. Although they are still used in some publications dynamite plots are best avoided. We will retun to this issue, but you may want to have a quick look at http://emdbolker.wikidot.com/blog:dynamite Genuine barcharts are non inferential. In other words no statistical tests are associated with them directly. They are simply used to display information. Let’s look at the simplest case possible where barcharts may be used. At the end of Spetember 2019 a group of Bournemouth University students monitored the traffic on the roads around the campus. They provided data on the counts of different types of vehicles passing each hour. Here is their data as they summarised it. data(butraffic) dt(butraffic) These very simple data have no replication at all. Therefore there is no possibility of conducting any form of inferential statistical analysis. Sometimes a Chi Squared goodness of fit test can be used to test whether there is a significant difference between counts in each group, but this would clearly not be appropriate in this case as the differences are very apparent. However we can show the results to the reader more clearly through a figure than a table. To form a standard barchart in gglots we first need to decide how the data will be mapped onto the elements that make up the plot. The term for this in ggplot speak is “aesthetics”- Personally I find the term aestehetics to represent mapping a bit misleading. I would instinctively assume that aesthetics refers to the colour scheme or other visual aspect of the final plot. In fact the aesthetics are the first thing to decide on, rather than the last. The way to build a ggplot is by first forming an invisible object which represents the mapping of data onto the page. The way these data are presented can then be changed through adding differnt geometries. The only aesthetics (mappings) that you need to know about for basic usage are x,y, colour, fill, group and label. The x and y mappings coincide with axes, so are simple enough. Remember that a histogram maps onto the x axis. The y axis shows either frequency or density so is not mapped directly as a variable in the data. The variable on the x axis in this case would be the Vehicle type. The count of the number of vehicles would be placed on the y axis. If we want to label the figure with the actual number counted (which is good practice for barcharts if feasible) we could add a label aesthetic as well. g0&lt;-ggplot(butraffic,aes(x=Vehicle,y=Count, label=Count)) Now let’s plot out the barplot. We do that by adding a geometry. There are two geometries that could be used heere. The simplest in this case is to use geom_col and add the label. We’ll assign the result to g1, then type the name g1 to plot it. That way if we want to continue modifying our plot we just add to g1. g1&lt;- g0 + geom_col() + geom_label() # An alternative which does the same thing is geom_bar with identity stat. # g1&lt;- g0 + geom_bar(stat=&quot;identity&quot;) + geom_label() g1 Note that if geom_bar is used then you need to tell R to use the “identity” stat if a table of counts is used. The default stat in this case is count, i.e. geom_bar itself will form a count table from raw data. This figure looks OK, but it would be much clearer to have the bars in a ranked order. To do this in R we use the follwing code. First tell R to arrange the data frame according to Count. To place in ascending order use -Count. Then a little trick is used to relevel the Vehicle factor levels to match the arrangement. butraffic %&gt;% arrange(-Count) %&gt;% mutate(Vehicle = factor(Vehicle, Vehicle)) -&gt; butraffic We are going to have to set up the aesthetics again to use this, as the data have changed. By default the background used for the first figure was grey. We can also change the look and feel of subsequent plots by setting the theme. A black and white theme might be better for printing. theme_set(theme_bw()) g0&lt;-ggplot(butraffic,aes(x=Vehicle,y=Count, label=Count)) g1&lt;- g0 + geom_col() + geom_label() g1 To show the results as percentages we can calculate them using another line of dplyr code. butraffic %&gt;% mutate(Percent=round(100*Count/sum(Count),1)) -&gt;butraffic g0&lt;-ggplot(butraffic,aes(x=Vehicle,y=Percent, label=paste(Percent, &quot;%&quot;, sep=&quot;&quot;))) g2&lt;-g0 + geom_col() + geom_label() g2 8.3.1 Exercise The following code chunk produces the number of votes cast for each party in Bournemouth west. data(ge2017) ge2017 %&gt;% filter(constituency==&quot;Bournemouth West&quot;) %&gt;% select(c(&quot;party&quot;,&quot;votes&quot;)) -&gt;bw ## Adding missing grouping variables: `constituency` dt(bw) Form barcharts using both counts and percentages. The following code simulates responses to a question on the Likert scale d&lt;-data.frame(table(rand_likert())) names(d)&lt;-c(&quot;Resonse&quot;, &quot;Count&quot;) Form a barchart using these data. Note that analysing a full questionaire would involve looking at the responses to many questions simultaneously.R has some additional graphical tools for this 8.4 Simple Line charts Do not confuse genuine line charts with scatterplots with a fitted line. Line charts are usually non inferential figures (i.e. they do not show confidence intervals). A common use for a line chart is to show a time series. We’ll extract a portion of data on rainfall from a larger data set. The code below produces a small data frame of annual rainfall measured at the Hurn meteorological station. data(&quot;met_office&quot;) met_office %&gt;% filter(station==&quot;hurn&quot; &amp; Year &lt; 2018 &amp; Year &gt; 1999) %&gt;% group_by(Year) %&gt;% summarise(rain=sum(rain))-&gt; rain dt(rain) We’ll set up the aesthetics. The x axis is the year, the rainfall goes on the y axis. We might want to use the number as a label. g0&lt;-ggplot(rain, aes(x=Year,y=rain,label=rain)) Now, adding geom_line produces a basic line chart. g1 &lt;- g0 + geom_line() g1 This might look better if the actual numbers are shown. This only works for short time series with few values. If there are more values the plot would look too cluttered. g1 &lt;- g1 + geom_label() g1 Now we might want to add a title and some text for the x and y axes. g1&lt;- g1 + labs(title = &quot;Annual precipitation recorded at Hurn meterological station, Dorset&quot;, x=&quot;Year&quot;, y=&quot;Total annual precipitation in mm&quot;) g1 This looks better, but its a bit difficult to see which year the number applies to. We can set the breaks on the continyuos x axis with scale_x_continuous(breaks = ) g1 &lt;-g1 + scale_x_continuous(breaks=1999:2018) g1 A final touch might be to rotate the text on xaxis. g1 &lt;- g1 + theme(axis.text.x=element_text(angle=45, hjust=1)) g1 8.4.1 Exercise These lines of code produce a data frame with the mean annual temperature. met_office %&gt;% filter(station==&quot;hurn&quot; &amp; Year &lt; 2018 &amp; Year &gt; 1999) %&gt;% group_by(Year) %&gt;% summarise(tmean=round(mean((tmax+tmin)/2),1))-&gt; tmean Form a line chart of the mean annual temperatures at Hurn. 8.5 Histograms Typical histograms use only one variable at a time, although they may be “conditioned” by some grouping variable. The aim of a histogram is to show the distribution of the variable clearly. Let’s try ggplot histograms in ggplot2 using some simple data on mussel shell length at six sites. data(mussels) d&lt;-mussels To produce a simple histogram we only need to provide ggplot2 with the name of the variable we are going to use as the x aesthetic. g0 &lt;- ggplot(data=d,aes(x=Lshell)) 8.5.1 Default histogram The default geom for the histogram is very simple. The goemetry is associated with statistical function that forms a histogram by counting the number of observation falling into the bins. g0 + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. There are several things to notice here. One is that the default histogram looks rather like some of the figures in Excel. For some purposes this can be useful. However you may prefer to change this. This is easy to change. You are also warned that the default binwidth may not be ideal. My own default settings for a histogram would therefore look more like this. g1&lt;-g0 +geom_histogram(fill=&quot;grey&quot;,colour=&quot;black&quot;,binwidth=10) + theme_bw() g1 This should be self explanatory. The colour refers to the lines. It is usually a good idea to set the binwidth manually. You can choose the binwidth through trying a range before decising on which looks the best for your readers. Notice that I have assigned the results to an object called g1. We can then work with this to produce conditional histograms. 8.5.2 Facet wrapping Conditioning the data on one grouping variable is very simple using a facet_wrap. Facets are the term used in ggplots for the panels in a lattice plot. There are two facet functions. Facet_wrap simply wraps a one dimensional set of panels into what should be a convenient number of columns and rows. You can set the number of columns and rows if the results are not as you want. g_sites&lt;-g1+facet_wrap(~Site) g_sites 8.6 Boxplots A grouped boxplot uses the grouping variable on the x axis. So we need to change the aesthetic mapping to reflect this. data(&quot;mussels&quot;) d&lt;-mussels g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_box&lt;-g0 + geom_boxplot(fill=&quot;grey&quot;,colour=&quot;black&quot;)+theme_bw() g_box 8.7 Confidence interval plots One important rule that you should try to follow when presenting data and carrying out any statistical test is to show confidence intervals for key parameters. Remember that boxplots show the actual data. Parameters extracted from the data are means when the data are grouped by a factor. When two or more numerical variables are combined the parameters refer to the statistical model, as in the case of regression. Grammar of graphics provides a convenient way of adding statistical summaries to the figures. We can show the position of the mean mussel shell length for each site simply by asking to plot the mean for each y like this. data(&quot;mussels&quot;) d&lt;-mussels g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_mean&lt;-g0+stat_summary(fun.y=base::mean,geom=&quot;point&quot;) g_mean This is not very useful. However you can add confidence intervals themselves by a call to a stat_summary. The syntax is slightly involved. g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) To save some typing I’ve added a ci function to the aqm package that will add both these geometries to a graphic in one call. g1&lt;-aqm::ci(g0) g1 If you want more robust confidence intervals with no assumption of normality of the residulas then you can use bootstrapping. In this case the result should be just about identical, as the errors are approximately normally distributed. g_mean+stat_summary(fun.data=mean_cl_boot,geom=&quot;errorbar&quot;) 8.7.1 Dynamite plots (Don’t use!) The traditional “dynamite” plots with a confidence interval over a bar can be formed in the same way. g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_mean&lt;-g0+stat_summary(fun.y=base::mean,geom=&quot;bar&quot;) g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) Most statisticians prefer that the means are shown as points rather than bars. You may want to look at the discussion on this provided by Ben Bolker. http://emdbolker.wikidot.com/blog:dynamite 8.7.2 Inference on medians One way to infer differences between medians is to plot boxplots with notches. g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_box&lt;-g0 + geom_boxplot(fill=&quot;grey&quot;,colour=&quot;black&quot;, notch=TRUE)+theme_bw() g_box The notch forms a confidence interval around the median. This is based on the median \\(\\frac {\\pm 1.57 IQR} {\\sqrt(n)}\\). According to Graphical Methods for Data Analysis (Chambers, 1983) although not a formal test the, if two boxes’ notches do not overlap there is ‘strong evidence’ (95% confidence) thet their medians differ. A more precise appraoch is to use bootstrapping to calculate confidence intervals. # Function included in aqm package # median_cl_boot &lt;- function(x, conf = 0.95) { # lconf &lt;- (1 - conf)/2 # uconf &lt;- 1 - lconf # require(boot) # bmedian &lt;- function(x, ind) median(x[ind]) # bt &lt;- boot(x, bmedian, 1000) # bb &lt;- boot.ci(bt, type = &quot;perc&quot;) # data.frame(y = median(x), ymin = quantile(bt$t, lconf), ymax = quantile(bt$t, # uconf)) # } This has been added to the aqm package. So using this function the medians and confidence intervals can be plotted. This is a valid apprach even for skewed data. g0+ stat_summary(fun.data = aqm::median_cl_boot, geom = &quot;errorbar&quot;) + stat_summary(fun.y = median, geom = &quot;point&quot;) You can now draw inference on the differences in the usual way. 8.8 Scatterplots Scatterplots can be built up in a similar manner. We first need to define the aesthetics. In this case there are clearly a and y coordinates that need to be mapped to the names of the variables. g0 &lt;- ggplot(d,aes(x=Lshell,y=BTVolume)) g0+geom_point() 8.8.1 Adding a regression line It is very easy to add a regression line with confidence intervals to the plot. g0+geom_point()+geom_smooth(method = &quot;lm&quot;, se = TRUE) Although the syntax reads “se=TRUE” this refers to 95% confidence intervals. 8.8.2 Grouping and conditioning There are various ways of plotting regressions for each site. We could define a colour aesthetic that will automatically group the data and then plot all the lines as one figure. g0 &lt;- ggplot(d,aes(x=Lshell,y=BTVolume,colour=Site)) g1&lt;-g0+geom_point()+geom_smooth(method = &quot;lm&quot;, se = TRUE) g2&lt;-g1+geom_point(aes(col=factor(Site))) g2 This can be split into panels using facet wrapping. g2+facet_wrap(~Site) 8.8.3 Curvilinear relationships Many models involving two variables can be visualised using ggplot. d&lt;-read.csv(system.file(&quot;extdata&quot;, &quot;marineinverts.csv&quot;, package = &quot;aqm&quot;)) str(d) ## &#39;data.frame&#39;: 45 obs. of 4 variables: ## $ richness: int 0 2 8 13 17 10 10 9 19 8 ... ## $ grain : num 450 370 192 194 197 ... ## $ height : num 2.255 0.865 1.19 -1.336 -1.334 ... ## $ salinity: num 27.1 27.1 29.6 29.4 29.6 29.4 29.4 29.6 29.6 29.6 ... g0&lt;-ggplot(d,aes(x=grain,y=richness)) g1&lt;-g0+geom_point()+geom_smooth(method=&quot;lm&quot;, se=TRUE) g1 g2&lt;-g0+geom_point()+geom_smooth(method=&quot;lm&quot;,formula=y~x+I(x^2), se=TRUE) g2 g3&lt;-g0+geom_point()+geom_smooth(method=&quot;loess&quot;, se=TRUE) g3 You can also use a gam directly. g4&lt;-g0+geom_point()+stat_smooth(method = &quot;gam&quot;, formula = y ~ s(x)) g4 The plots can be arranged on a single page using the multiplot function taken from a cookbook for R. multiplot(g1,g2,g3,g4,cols=2) 8.9 Generalised linear models Ggplots conveniently show the results of prediction from a generalised linear model on the response scale. glm1&lt;-g0+geom_point()+geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;poisson&quot;), se=TRUE) +ggtitle(&quot;Poisson&quot;) glm1 glm2&lt;-g0+geom_point()+geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;quasipoisson&quot;), se=TRUE) + ggtitle(&quot;Quasipoisson&quot;) glm2 library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select glm3&lt;-g0+geom_point()+geom_smooth(method=&quot;glm.nb&quot;, se=TRUE) +ggtitle(&quot;Negative binomial&quot;) glm3 multiplot(glm1,glm2,glm3,cols=2) 8.10 Binomial data The same approach can be taken to binomial data. ragworm&lt;-read.csv(system.file(&quot;extdata&quot;, &quot;ragworm_test3.csv&quot;, package = &quot;aqm&quot;)) str(ragworm) ## &#39;data.frame&#39;: 100 obs. of 2 variables: ## $ presence: int 1 1 0 0 1 1 1 0 1 1 ... ## $ salinity: num 2.55 2.21 3.39 2.96 1.88 ... g0 &lt;- ggplot(ragworm,aes(x=salinity,y=presence)) g1&lt;-g0+geom_point()+stat_smooth(method=&quot;glm&quot;,formula = y~ x, method.args=list(family=&quot;binomial&quot;))+ggtitle(&quot;Linear&quot;) g2&lt;-g0+geom_point()+stat_smooth(method=&quot;glm&quot;,formula = y~ x+I(x^2), method.args=list(family=&quot;binomial&quot;))+ggtitle(&quot;Polynomial&quot;) g3&lt;-g0+geom_point()+stat_smooth(method = &quot;gam&quot;, formula = y ~ s(x), method.args=list(family=&quot;binomial&quot;))+ggtitle(&quot;GAM&quot;) multiplot(g1,g2,g3,cols=2) 8.11 Learning more about ggplots GGplots can be used to display very much more complex data sets than those shown in this handout. A very useful resource if you want to try more advanced graphics is provided by the R cookbook pages. http://www.cookbook-r.com/Graphs/ A detailed tutorial of gggplots is provided in chapter 3 of R for data science https://r4ds.had.co.nz/data-visualisation.html Some additional ideas are available here http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html A flip book approach is taken here. https://evamaerey.github.io/ggplot_flipbook/ggplot_flipbook_xaringan.html#23 "],
["t-tests-and-hypothesis-testing.html", "Chapter 9 T tests and hypothesis testing 9.1 Statistical hypothesis testing", " Chapter 9 T tests and hypothesis testing What you must remember The difference between a null hypothesis and the working hypothesis The interpretation of a p-value The basic assumptions underlying parametric tests The strengths and weaknesses of hypothesis testing 9.1 Statistical hypothesis testing Statistical hypothesis testing works like this. Given that the measurements that we obtain from nature tend to vary in a random, unexplainable manner it may be very easy to be fooled into thinking that patterns exist that are really just down to chance. We look for patterns in our data that would indicate something interesting, but we mistrust what we have found until we have ruled out the null hypothesis that the pattern could have occurred anyway simply as a result of random variation. 9.1.1 The unpaired t-test For example, when using an unpaired t-test we are interested in the difference between the means for two groups. If you take a set of numbers with random variation and split them into two arbitrary groups the means will never be exactly the same. If you just calculate the two means and then report them in your dissertation, or paper, as representing a real difference between the groups (one that would be reproduced by another researcher following your methods) you would quite rightly be asked by any scientific referee to provide some justification that the difference you have found is not simply an artefact of the intrinsic variability in your data. So, in order to convince the sceptics, we set up a null hypothesis. The null hypothesis expresses the position that the difference could have arisen if the measurements were taken from the same population (an effect attributable to “chance”“). We then calculate a statistic that takes into account both the size of the difference and the random variability in the measurements. We calculate how likely it would have been to have got this statistic (or one more extreme) if the null hypothesis were in fact true. We want to be able to say that it is not very likely in order to convince the sceptics. If this statistic falls in the tail of its probability distribution under the null hypothesis we know that is unlikely that we have got the data we have (or data even more extreme) if the null hypothesis were true. We therefore now are justified in suspecting that the null hypothesis is false and we can reject it. The probability of getting a statistic (or one even more extreme) is called a p-value. So in order to reject the null we need large positive or negative values for the statistic (t), and a small p-value. The critical p-value for rejecting the null is typically set at 0.05 (one chance in twenty of getting the result (or one more extreme) if nothing were really happening (H0 is true). There may be a lot more work to do in order to understand how and why the difference occurred and what it implies, but at least we can rule out chance. A t-test is one form of null hypothesis test that looks to establish whether there is a real difference between the means of two groups. The null hypothesis is that there is no difference between the means (the data could have been drawn from a single population). 9.1.2 The t-statistic Our test of a null hypothesis is going to take into account all the following aspects. The differences between the means: Large differences are more likely to be detected easily. The size of the sample: Large samples are more likely to produce more reliable results. The intrinsic variability in the data: High variability will make it difficult to reproduce the results. Now, we have already seen that the standard error takes into account the last two elements that we want to include in our test Size of sample and intrinsic variability). So, the test statistic can be based on the number of standard errors by which the two sample means are separated. This will take into account the first element (differences between the means) as well. \\(t=\\frac{DifferenceBetweenMeans}{StandardErrorOfDifference}=\\frac{\\bar{x_{1}}-\\bar{x_{2}}}{SE_{diff}}\\) For two independent (i.e. non-correlated) variables, the variance of a difference is the sum of the separate variances. This important result allows us to write down the formula for the standard error of the difference between two sample means. \\(SE_{diff}=\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+}\\frac{s_{2}^{2}}{n_{2}}\\) Let’s run through this step by step in R W’ll load some data. library(aqm) data(mussels) d&lt;-mussels To extract the values of Lshell for two sites we use square brackets. This feature of the R language will be explained in more detail next term. Site1&lt;-d$Lshell[d$Site==&quot;Site_1&quot;] Site2&lt;-d$Lshell[d$Site==&quot;Site_2&quot;] Now we can get the means as we did before. Mean1&lt;-mean(Site1) Mean1 ## [1] 100.7692 Mean2&lt;-mean(Site2) Mean2 ## [1] 109.236 The lengths of the vectors give us the n values. R calculates the variances The formula above uses the population variance as calculated with a denominator of n rather than n-1. Here we have ignored this in order to use R’s defaults and keep it as simple as possible in order to demonstrate the principle behind the test. It will make a very minor difference when we compare the result to that produced by the software. N1&lt;-length(Site1) N2&lt;-length(Site2) Var1&lt;-var(Site1) Var1 ## [1] 153.6646 Var2&lt;-var(Site2) Var2 ## [1] 196.0907 We have what we need to calculate the standard error of the difference. SEdiff&lt;-sqrt(Var1/N1+Var2/N2) SEdiff ## [1] 3.708613 And we can now find the value for t t&lt;-(Mean1-Mean2)/SEdiff t ## [1] -2.283002 Finally we look up the p value. To do this we need the value of t and the degrees of freedom. The term degrees of freedom refers to the amount of independent replication in the study design. In this case the degrees of freedom are calculated by adding the two values of n. However we calculated a mean from each group. As a non technical rule of thumb we subtract a degree of freedom each time we calculate a mean. So, we have n1 plus n2 minus 2 degrees of freedom. We multiply the p value by two as we are carrying out a two tailed test. A one tailed test assumes that we are only interested in one side of the question (eg. H0 The mean for site 1 is not larger than the mean for site2). We rarely work with such hypotheses in ecology. In most practical cases we will choose a two tailed test. You should always use a two tailed test by default unless you can provide a very good reason not to. df&lt;-N1+N2-2 pval&lt;-2*pt(t,df) pval ## [1] 0.02680621 Of course we do never need to do all these calculations step by step unless we are trying to understand the logic as here. R does it for us with one command. t.test(Site1,Site2) ## ## Welch Two Sample t-test ## ## data: Site1 and Site2 ## t = -2.283, df = 47.762, p-value = 0.02692 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -15.924396 -1.009143 ## sample estimates: ## mean of x mean of y ## 100.7692 109.2360 Notice that the p-value and degrees of freedom are slightly different to the ones that we got when working through the steps. This is because R has made a correction for the fact that the two variances were different. This is known as Welch’s test. As this is the safest test to use, R uses it by default. Welch’s correction is a good one. It makes the test more robust than the classical method we have just gone through step by step. 9.1.3 Interpretation of a p-value So how do we interpret the result? We have a p-value of 0.0269. This is below 0.05, which is the traditional cut off point for statistical significance. So there is a less than one in twenty chance of having got this result (or one more extreme) if there really were no difference between the mean values of the two sites. In this case we can reject the null hypothesis and conclude that there is a significant difference in mean shell length(p&lt;0.05) between the two sites. But what about this case? Site3&lt;-d$Lshell[d$Site==&quot;Site_3&quot;] t.test(Site1,Site3) ## ## Welch Two Sample t-test ## ## data: Site1 and Site3 ## t = -0.68235, df = 8.1748, p-value = 0.5139 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -26.36357 14.28953 ## sample estimates: ## mean of x mean of y ## 100.7692 106.8063 If a p-value is larger that the conventional cut-off (0.05) we can’t reject the null hypothesis. You have to be careful here. The test does not really say that the mean values for the two sites are the same. It simply suggests that as things stand there is not enough evidence to claim that there is a real difference. If we increased the sample size in order to reduce the standard error we might still be able to show a significant difference. ``Absence of evidence is not evidence of absence’’. Notice that the variance for Site 3 is large compared to Sites 1 and 2. var(Site1) ## [1] 153.6646 var(Site3) ## [1] 578.936 The larger the overall variability in measurements, the harder it will be to find significant differences. 9.1.3.1 Testing the assumption of normality One of the assumptions of the t-test is that the observations within each group could have been taken from an approximately normal distribution. When working with large datasets minor violations of this assumption are not necessarily important. However major violations are almost always serious when we run simple tests on relatively small data sets (n&lt;30). As you gain more experience in data analysis it becomes possible to detect the gravity of a violation of normality easily using histograms and QQ-plots. However visual inspection works best with large data sets. When you are working with small data sets the assumption that the data could have been drawn from a normal distribution can be treated as a null hypothesis. A test of this hypothesis can therefore be used for diagnosis. If we have to reject the null hypothesis (p&lt;0.05) there is good evidence that one of the assumptions of the t test has not been met. We would therefore usually have to use an alternative statistical procedure. We can run tests of normality for each site using the Shapiro Wilks test. shapiro.test(Site1) ## ## Shapiro-Wilk normality test ## ## data: Site1 ## W = 0.98072, p-value = 0.8888 shapiro.test(Site2) ## ## Shapiro-Wilk normality test ## ## data: Site2 ## W = 0.92194, p-value = 0.05673 shapiro.test(Site3) ## ## Shapiro-Wilk normality test ## ## data: Site3 ## W = 0.90365, p-value = 0.3115 So, in all these cases the p-value is greater than 0.05. We cannot reject the null hypothesis, therefore the tests provides no evidence to suggest that the assumption of normality is not met. There are some quite important caveats to this procedure that will be explained later on in the course. Used carefully these tests help to justify the choice of statistical method. 9.1.3.2 Testing for differences in variances The traditional t test made the assumption that the variances in each group are the same. The contemporary procedure that R and other software adopts by default is known as Welch’s test. This slight modification does not require variances to be the same. We can use a t-test safely even if the assumption of homogeneity of variance is not met providing Welch’s procedure is followed. Reducing the number of assumptions we make when running a test is usually a good thing to do, providing that this does not affect the power of the test nor its interpret-ability. In this case it does not. We might still like to test whether the variances are significantly different. In R we can use a simple test based on the F ratio. SPSS uses another related procedure called the Levene’s test which is included by default in the output. var.test(Site1,Site3) ## ## F test to compare two variances ## ## data: Site1 and Site3 ## F = 0.26543, num df = 25, denom df = 7, p-value = 0.01276 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.06026177 0.75587865 ## sample estimates: ## ratio of variances ## 0.2654259 var.test(Site1,Site2) ## ## F test to compare two variances ## ## data: Site1 and Site2 ## F = 0.78364, num df = 25, denom df = 24, p-value = 0.5487 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.3471411 1.7570958 ## sample estimates: ## ratio of variances ## 0.7836404 Testing for differences in variance can be interesting in itself. There is no reason why we are only looking at differences between mean. We may also want to know if the shell lengths are more or less variable at one site or another. This may have important ecological implications. Here is appears that there is a significant difference in the variability of the shell lengths between sites 1 and 3, but not between sites 1 and 2. 9.1.4 Tests and confidence intervals If you looked at the output from tests you should have seen that in addition to the p-value from the test R also provides confidence intervals for the difference between the means. The confidence interval is almost always more useful than the p-value alone. The test for significance simply tells us that there is a difference that is not attributable simply to chance variation (if all the assumptions are met). The confidence interval shows the possible size of the difference together with a measure of our uncertainty. If a 95% confidence interval includes zero then the p-value will not be significant. If the limits of the confidence interval approach zero then the p-value will be just below 0.05. If the confidence interval is far from zero then the p-value will be vanishingly small. So the confidence interval includes information regarding statistical significance. If sample sizes are very large we can find significant differences even when these differences are in fact very small, or even meaningless within the context of the study. In many cases we expect to find a difference, so the p-value tells us nothing we didn’t already know. However, we may not know how large the difference is going to be. So you should always report confidence intervals where possible. In fact you should do a lot more than this when analysing your data fully. Good visualisation and diagnostics are essential. We will come on to these aspects as the course progresses. 9.1.5 Using hypothesis tests wisely Hypothesis testing is a powerful tool that allows you to produce formally defensible statements regarding your data. A good dissertation must include formal tests of all the hypotheses of interest. At the same time, statistical hypothesis testing can be one of the most misunderstood aspects of statistical analysis. The difficulty seems to arise when the limited goals of a statistical hypothesis testing are equated as a direct test of the broader scientific hypothesis that is really of interest. Well designed manipulative experiments can turn a statistical hypothesis test into a test of a scientific hypothesis. Unfortunately in ecological situations we are often analysing data derived from observations and surveys rather than carefully controlled experiments. In these situations the connection between the statistical hypothesis and the scientific hypothesis is usually less direct. Answering ecological questions using the available data often require running a series of statistical hypothesis tests together with more detailed quantitative analysis that extracts effect sizes and functional forms of relationships from the data. You will learn more about this next term. However these quotations from Dytham and Zuur set the scene and reinforce the point that ecological data analysis is always challenging. Dytham on ``The art of choosing a test’’ It may be a surprising revelation, but choosing a statistical test is not an exact science. There is nearly always scope for considerable choice and many decisions will be made based on personal judgements, experience with similar problems or just a simple hunch. In several years of teaching statistics to biology students it is clear to me that most students don’t really care how or why the test works. They do care a great deal that they are using an appropriate test and interpreting the results properly.(Dytham 2011) Zuur on “Which test should I apply?}” During the many years of working with ecologists, biologists and other environmental scientists, this is probably the question that the authors of this book hear the most often. The answer is always the same and along the lines of ‘What are your underlying questions?’, ‘What do you want to show?’. The answers to these questions provide the starting point for a detailed discussion on the ecological background and purpose of the study. This then gives the basis for deciding on the most appropriate analytical approach. Therefore, a better starting point for an ecologist is to avoid the phrase ‘test’ and think in terms of ‘analysis’. A test refers to something simple and unified that gives a clear answer in the form of a p-value: something rarely appropriate for ecological data. In practise, one has to apply a data exploration, check assumptions, validate the models, perhaps apply a series of methods, and most importantly, interpret the results in terms of the underlying ecology and the ecological questions being investigated.(Zuur, Ieno and Smith, 2007). Dytham is correct in stating that most students don’t care how a test works. Many useful procedures involve quite advanced mathematics. Most ecological researchers do not fully understand how complex multivariate methods “work” either, although they should always understand the concepts underlying them. It is quite wrong to assume that no statistical understanding is required to choose an analysis. In my experience it is impossible to select any analysis without having at least an intuitive notion of the underlying statistical mechanisms. Zuur’s emphasis on analysis rather than testing alone is a result of experience with large data sets and complex ecological questions. The simple “classical”&quot; hypothesis tests that are emphasised in Dytham’s book are most useful for answering questions with relatively small data sets. They are a good starting point for developing skills in quantitative analysis. In some cases they may be all you need. However note this warning from Dytham “A common tendency is to force the data from your experiment into a test you are familiar with even if it is not the best method”. In other words, if all you have is a hammer every problem looks like a nail. This will be the theme in AQM. "],
["introduction-to-one-way-anova.html", "Chapter 10 Introduction to one way ANOVA 10.1 Alternative to the one way test 10.2 Power analysis 10.3 Exercise 10.4 References", " Chapter 10 Introduction to one way ANOVA In the last class we looked at the basis for inferential statistics. You saw how the the standard deviation can be calculated for a sample of measurements taken from a larger population and how this can be used to “infer” a confidence interval within which we would expect the population mean to fall 95% of the time. This concept also was used to test hypotheses regarding the differences between two means using a t-test. This class introduces some simple model based analyses. We will look at some data on mussel shell lengths taken from 6 different sites. The simple question to answer is if any of the overall variability can be attributed to the differences between the sites. This is a simple example of one way analysis of variance. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/mussels.csv&quot;) 10.0.1 Multiple comparisons A t-test involves comparisons between two means. You can use R to find out how many combinations of pairs of sites there would be using binomial theory. levels(d$Site) ## [1] &quot;Site_1&quot; &quot;Site_2&quot; &quot;Site_3&quot; &quot;Site_4&quot; &quot;Site_5&quot; &quot;Site_6&quot; There are 6 levels. Choosing combinations of two from six. choose(6,2) ## [1] 15 Running 15 separate t-tests would be quite a lot of work. So in this sort of case we often ask a more general question first. We test whether there is any significant differences between any of the sites. 10.0.2 Visualising between group variation using boxplots A good first step is to look at the variation in the data for each site using boxplots. If we use the plotly library in R we obtain dynamic boxplots. library(ggplot2) library(plotly) theme_set(theme_bw()) g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_box&lt;-g0+ geom_boxplot() ggplotly(g_box) 10.0.3 Boxplot statistics Hovering over the boxplots shows some key statistic for each group. The central line in the boxplot is the median. The box holds the interquartile range (i.e. 50% of the observations fall within it). The whiskers extend out to the what is, informally speaking, either the upper or lower limits to the distribution or the upper or lower limits expected if the data are normally distributed. Thus boxplots are useful as diagnostic tools to identify outliers or skewed distributions. The problem with comparing data using boxplots is that they show all the variability in the data, so there is often a large overlap between the boxes. 10.0.4 Plotting confidence intervals for each group The standard error for the mean and the confidence intervals for the mean that are calculated from it are a function of sample size. So we may want to plot the confidence intervals for the mean in order to spot differences more clearly. g_mean&lt;-g0+stat_summary(fun.y=mean,geom=&quot;point&quot;) g_mean&lt;-g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) g_mean If there is clear space between confidence intervals a significance test will be significant. If confidence intervals overlap slightly the test may, or may not be significant depending on the extent of the overlap. If the overlap is large the tests will never be significant. Combining the two. This looks a bit messy visually, but helps to show the relationship between boxplots and confidence intervals. g_mean&lt;-g_box+stat_summary(fun.y=mean,geom=&quot;point&quot;,col=&quot;red&quot;) g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,col=&quot;red&quot;) 10.0.5 Fitting a model We can test a simple hypothesis using a technique called one way ANOVA. Could the variation in means between sites simply be due to chance? To do that we compare the variability in means to the overall variability in the data. Crawley’s R book provides an in depth explanation of the logic behind analysis of variance in chapter 9. I will not repeat all the details today. Next week we will look at where the sum of squares come from in linear models in the case of both anova and regression. In R to fit the analysis of variance as a model we can write aov(Lshell~Site) and then ask for a summary. mod&lt;-aov(data=d,Lshell~Site) summary(mod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Site 5 5525 1105 6.173 4.58e-05 *** ## Residuals 107 19153 179 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The result suggests that there is significant variation in mean shell length between sites, providing the assumptions of the test hold. 10.0.6 The F ratio and degrees of freedom The key to understanding analysis of variance is to understand the F ratio. The calculations produce a sum of squares and a mean square that are attributed to two sources of variation in the data. For this reason we often talk about partitioning the variance. The Site component is the amount of variation attributable to variations in the mean shell lengths between sites. The residual variation is the amount of variability around these mean values. You can see that the mean square for the Site term in the model is much larger than the mean square for the Residuals term. In fact it is just over 6 times larger. We know that because the table includes the F value, which is the ratio of the two mean squares. The table also contains information regarding the number of groups and the amount of replication. These are the degrees of freedom. The degrees of freedom for Site is n-1. There are six sites so there are five degrees of freedom. The degrees of freedom for the residuals are the total number of measurements minus the number of sites (factor levels). So we have 113-6=107. If you are wondering how this works, the simple explanation is that we subtract the number of mean values that we use in the calculation of the sum of squares. In the case of the site, one overall mean is used (nsites-1). In the case of the residuals the mean of each site is subtracted from the observations for each site (nobs-6). So we can now make a formal statement that we back up with the statistical analysis. We write that There is statistically significant variability in mean shell lengths between sites F(5, 107) = 6.17, p &lt;0.001. 10.0.7 Homogeneity of variance You might have spotted an issue with this test, particularly if you have been reading the text books thoroughly. The traditional analysis of variance assumes homogeneity of variances. The boxplots suggest that there is a lot of variation in between sites in the amount of variation in shell length. A test for homogeneity of variance that is often recommended is Bartlett’s test bartlett.test(d$Lshell~d$Site) ## ## Bartlett test of homogeneity of variances ## ## data: d$Lshell by d$Site ## Bartlett&#39;s K-squared = 18.528, df = 5, p-value = 0.002352 As always, a low p-value suggests that the null hypothesis can be rejected, which in this case is homogeneity of variances. So on technical grounds the test we have just conducted is not quite right. One of the assumptions is not met. This can be a major issue for more complex designs. It is easy to let R make a correction for this when running a one way anova by asking for a oneway test (Welch’s test). oneway.test(d$Lshell~d$Site) ## ## One-way analysis of means (not assuming equal variances) ## ## data: d$Lshell and d$Site ## F = 9.6559, num df = 5.000, denom df = 31.194, p-value = 1.207e-05 The test has now taken into account the issue and it still gives a significant result. Using Welch’s procedure is a useful backup to reinforce and defend your conclusions when the assumption of homogeneity of variance is violated. We will look at this and similar issues in more detail in the course. However you still need to look at the pattern of differences more carefully. 10.1 Alternative to the one way test Another slightly more sophisticated way of handling homogenity of variance in R is to use a procedure called White’s adjustment. library(sandwich) library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:boot&#39;: ## ## logit ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some mod&lt;-lm(Lshell~Site, data=d) Anova(mod,white.adjust=&#39;hc3&#39;) ## Coefficient covariances computed by hccm() ## Analysis of Deviance Table (Type II tests) ## ## Response: Lshell ## Df F Pr(&gt;F) ## Site 5 9.9682 7.541e-08 *** ## Residuals 107 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The F ratio after making an appropriate correction is actually rather higher. Write down a statement that you could include in a report that expresses this formally It should begin along these lines … “Bartlet’s test showed significant heteroscedasticity in the residuals for all metals. White-corrected covariance matrices were therefore be included in a linear model in order to adjust for lack of homogeneity. Corrected one way Anova showed …” 10.1.1 Determining where the differences lie There is a problem with the simple conclusion drawn from analysis of variance with or without correction. From a scientific perspective is it is hardly an unsurprising discovery. We would have expected some difference between sites, particularly if we look at so many different places. It is much more likely that we are really interested in specific differences between sites. However this raises an additional issue. If we have a hypothesis before we start regarding which site is most likely to be different we could run one single test. However if we are looking at fifteen separate comparisons that is a problem. The conventional significance value is set at 0.05. In other words one chance in twenty of obtaining the data (or more extreme) under the null hypothesis. If we run a lot of tests we increase the chances of at least one being significant even if the null holds. In fact it is quite easy to calculate the probability of getting at least one significant result if we run 15 tests. The easy way is to calculate the probability of all the tests being negative and subtract from 1. 1-0.95^15 ## [1] 0.5367088 So there is a 54% chance of getting at least one significant result at the 0.05 cut off level even if the null is true for every comparison. This is a bit like buying a large number of lottery tickets. If you buy enough you increase your overall chances of winning even though the chances of any single ticket winning remains the same. What is the probability of getting at least one significant result at the 0.05 level if you run twenty tests? Without thinking it is tempting to say that it is one. However this is not the correct answer. Try calculating it. One way of reducing the number of tests is to compare all the means with a control group. This is done in an experiment with treatments, and is the contrasts are thus known as treatment contrasts. We can obtain these in R from the output of aov using summary.lm. summary.lm(mod) ## ## Call: ## lm(formula = Lshell ~ Site, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.906 -8.340 1.031 9.231 30.550 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 100.769 2.624 38.405 &lt; 2e-16 *** ## SiteSite_2 8.467 3.748 2.259 0.0259 * ## SiteSite_3 6.037 5.409 1.116 0.2669 ## SiteSite_4 -3.619 5.409 -0.669 0.5049 ## SiteSite_5 18.697 3.925 4.763 6.02e-06 *** ## SiteSite_6 2.471 3.748 0.659 0.5111 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.38 on 107 degrees of freedom ## Multiple R-squared: 0.2239, Adjusted R-squared: 0.1876 ## F-statistic: 6.173 on 5 and 107 DF, p-value: 4.579e-05 This shows that only site2 and site5 are significantly different from site 1. If you go back to the confidence interval plot and look at the overlap you can see how this pattern emerges. 10.1.2 Bonferoni corrections Treatment contrasts method assumes that there is a planned contrast and there is something unique about site1. Another way of looking at the issue is to effectively make comparisons between all the pairs of sites. In this case we MUST compensate in some way for all the test. There are a lot of ways of doing this. The simplest is called the Bonferoni correction. This just involves changing the critical value by dividing by the number of tests. So if we call a p-value of 0.05 significant, but run 15 tests we would look for values of 0.05/15= 0.0033 before claiming a significant result. 10.1.3 Tukey’s honest significant difference A slightly more subtle method is called Tukey’s Honest Significant Difference (HSD) test. We can run this in R for all pairwise comparisons and plot the results. The confidence intervals and p-values are all adjusted. This produces a lot of output, as you would expect. It effectively runs the fifteen separate t-tests while making allowances for multiple comparisons. The output also includes 95% confidence intervals for the differences between the means. If these confidence intervals include zero then the test will not be significant. The easiest way to see the pattern is to plot the results. mod&lt;-aov(data=d,Lshell~Site) TukeyHSD(mod) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Lshell ~ Site, data = d) ## ## $Site ## diff lwr upr p adj ## Site_2-Site_1 8.466769 -2.408905 19.342443 0.2201442 ## Site_3-Site_1 6.037019 -9.660664 21.734702 0.8737518 ## Site_4-Site_1 -3.619231 -19.316914 12.078452 0.9849444 ## Site_5-Site_1 18.697436 7.305950 30.088922 0.0000867 ## Site_6-Site_1 2.470769 -8.404905 13.346443 0.9859123 ## Site_3-Site_2 -2.429750 -18.201132 13.341632 0.9976925 ## Site_4-Site_2 -12.086000 -27.857382 3.685382 0.2355928 ## Site_5-Site_2 10.230667 -1.262165 21.723498 0.1103764 ## Site_6-Site_2 -5.996000 -16.977781 4.985781 0.6105029 ## Site_4-Site_3 -9.656250 -29.069479 9.756979 0.7004668 ## Site_5-Site_3 12.660417 -3.470986 28.791819 0.2123990 ## Site_6-Site_3 -3.566250 -19.337632 12.205132 0.9862071 ## Site_5-Site_4 22.316667 6.185264 38.448069 0.0015143 ## Site_6-Site_4 6.090000 -9.681382 21.861382 0.8718474 ## Site_6-Site_5 -16.226667 -27.719498 -4.733835 0.0011239 plot(TukeyHSD(mod)) Notice that the comparison that we made between site1 and site2 that was significant using either a single t-test or treatment contrasts now is not shown as significant after the correction has been made for making numerous unplanned comparisons. If you really wanted to claim that the difference was significant you would have to be able to justify that you had thought that site2 ought to be particularly different before obtaining the data, and that the result was not just obtained by so called ``data dredging’’. So, there are some rather subtle aspects of data analysis. Under the more rigorous procedure only site 5 really stands out as being different from the rest. The fact that different ways of looking at the data can apparently lead to different conclusions is one of the aspects of statistics that worries many students (and researchers). Next term we will look at ways of avoiding common statistical pitfalls, together with methods to extract all the important information from a data set in order to fully address your scientific question. 10.1.4 The Kruskal Wallace non parametric test. The Kruskal Wallace test is often used in place of a one way anova when researchers are worried about the assumption of normality and want to use a non-parametric procedure. The disadvantage of a Kruskall Wallace test is that it is not easy to communicate the absolute size of any differences that are detected. The test does not lead to the development of a statistical model. kruskal.test(d$Lshell~d$Site) ## ## Kruskal-Wallis rank sum test ## ## data: d$Lshell by d$Site ## Kruskal-Wallis chi-squared = 27.884, df = 5, p-value = 3.835e-05 The diffferences refer to the location parameter, which is similar to the median, but not exactly the same measurement. This is difficult to communicate. Another tactic is to bootstrap the median itself. We saw how bootstrapping can be used last week. I have provided a small function to do this that can be used with a ggplot. median_cl_boot &lt;- function(x, conf = 0.95) { lconf &lt;- (1 - conf)/2 uconf &lt;- 1 - lconf require(boot) bmedian &lt;- function(x, ind) median(x[ind]) bt &lt;- boot(x, bmedian, 1000) bb &lt;- boot.ci(bt, type = &quot;perc&quot;) data.frame(y = median(x), ymin = quantile(bt$t, lconf), ymax = quantile(bt$t, uconf)) } g0+stat_summary(fun.data=median_cl_boot,geom=&quot;errorbar&quot;) + stat_summary(fun.y = median, geom = &quot;point&quot;, colour = &quot;red&quot;) How do you interpret this figure? 10.2 Power analysis Power analysis should always be caaried out when designing any study that involves one way analysis of variance. It is very easy to do. The idea behind a power analysis is to “reverse engineer” the analysis of variance. Usually when we carry out analysis of variance we want to show that there is some difference between groups. However if the sample size is too small we might not be able to reject the null hypothesis. The null hypothesis is that the observations could all have been drawn from the same population. Any difference between means is just down to chance variability that arises from variability within the groups. Power analysis asks a “what if” question. If you have a small pilot study you might be able to provide the parameters for the question. Let’s set up a scenario involving four groups. We can either guess some rough estimates of the group means why might expect, find some literature on the subject or carry out a small pilot study to provide some data. groupmeans &lt;- c(120, 130, 140, 150) n&lt;-length(groupmeans) bvar&lt;-var(groupmeans) Notice that we need to calculate the variance between the group means and get the number of groups. We then need to estimate the standard deviation. The best way to do this is through a short pilot study, but we can also make some informed assumptions regarding the value. The within sample variance is just this value squared. sdev&lt;-20 wvar&lt;-sdev^2 So the number of groups is 4. The variance between the the group means is calculated from the four observations and is 166.7. We estimate the standard deviation (think in terms of how far from the mean would you expect 66% of the observations to lie). Now we can set the power to the alpha level we want. If the power is set to 0.9 we would expect to find a significant difference in 90% of the studies, but we might obtain a type 2 error 10% of the time. A type two error is when we fail to reject the null hypothesis when it is in fact false, or in other words not find a significant difference even though it does exist. alpha&lt;-0.9 power.anova.test(groups = n, between.var = bvar, within.var = wvar, power = alpha) ## ## Balanced one-way analysis of variance power calculation ## ## groups = 4 ## n = 12.3635 ## between.var = 166.6667 ## within.var = 400 ## sig.level = 0.05 ## power = 0.9 ## ## NOTE: n is number in each group 10.3 Exercise Design a study to test whether there is a significant difference between the lengths of oak leaves collected from trees on the campus. Your first task is to collect a small sample and design a power analysis. 10.4 References Long, J. S. and Ervin, L. H. (2000) Using heteroscedasity consistent standard errors in the linear regression model. The American Statistician 54, 217–224. http://www.jstor.org/stable/2685594 White, H. (1980) A heteroskedastic consistent covariance matrix estimator and a direct test of heteroskedasticity. Econometrica 48, 817–838. Fox, J. (2008) Applied Regression Analysis and Generalized Linear Models, Second Edition. Sage. "],
["regression-as-a-statistical-modelling.html", "Chapter 11 Regression as a statistical modelling 11.1 The general linear model 11.2 Regression 11.3 Using ggplot2 for confidence intervals. 11.4 Where do the sum of squares come from? 11.5 Where does R squared (coefficient of determination) come from?", " Chapter 11 Regression as a statistical modelling One way anova is based on an underlying statistical model. Statistical modelling can allow you to extract more information from your data than some simpler “test” based methods that you may have learned previously. Most contemporary papers in ecology use models of some kind. Even if the nature of the data you collect limits your options, it is very important to learn to fit and interpret statistical models in order to follow the literature. 11.0.0.1 What is a statistical model? To some extent statistical modelling is easier than other forms of ecological modelling. Building process based models requires a great deal of understanding of a system. Statistical models are built from the data themselves. Providing you do have some data to work with you can let the combination of data and prebuilt algorithms find a model. However there are many issues that make statistical modelling challenging. A statistical model is a formal mathematical representation of the “sample space”“, in other words the population from which measurements could have been drawn. It has two components. An underlying “deterministic”&quot; component, that usually represents a process of interest A stochastic component representing “unexplained” variability So, a statistical model effectively partitions variability into two parts. One part represents some form of potentially interesting relationship between variables. The other part is just “random noise”. Because statistics is all about variability, the “noise” component is actually very important. The variability must be looked at in detail on order to decide on the right model. Many of the challenges involved in choosing between statistical models involves finding a way to “explain” the “unexplained” variability. This fundamental contradiction is why statistical concepts are so hard to understand. 11.0.1 Uses of models The literature on statistical modelling frequently uses the terms “explanation”&quot; and “prediction”&quot; to describe the way models are used. Although the same model can have both roles, it is worth thinking about the difference between them before fitting and interpreting any models. 11.0.1.1 Prediction Models can be used to predict the values for some variable when we are given information regarding some other variable upon which it depends. An example is a calibration curve used in chemistry. We know that conductivity and salinity are directly related. So if we measure the conductivity of liquids with known salinities we can fit a line. We can then use the resulting model to predict salinity at any point between the two extremes that we used when finding the calibration curve. Notice that we cannot easily extrapolate beyond the range of data we have. We will see how the same concept applies to the models we use in quantiative ecology. Predictions are more reliable if a large portion of the variability in the data can be “explained” through relationships with other variables. The more random variability in the data, the more difficult prediction becomes. 11.0.1.2 Explanation The idea that the variability in the data can be “explained” by some variable comes from the terminology that was used when interpretating experimental data. Experiments usually involve a manipulation and a control of some description. If values for some response differ between control and intervention then it is reasonable to assume that the difference is “explained” by the intervention. If you wish to obtain data that are simple to analyse and interpret you should always design an experiment. However, experiments can be costly. Ecological systems are often slow to respond to interventions, making experiments impossible within the time scale of a master’s dissertation. We are often interested in systems that cannot be easily modified anyway on ethical or practical grounds. Thus in ecology we often have to interpret associations between variables as evidence of process that “explain” a relationship. Correlation is not causation. Although patterns of association can provide insight into causality they are not enough to establish it. So, when you read the words “variance explained” look carefully at the sort of data that are being analysed. In an ecological setting this may only suggest close association between variables, not a true explanation in the everyday sense of the word. When there is a great deal of variability in the data we may want to ask whether any of the variability at all is “explained” by other variables. This is where statistical tests and p-values have a purpose. When faced with small amounts of noisy data it can be useful simply to ask whether more variability is associated with some variable than would occur by chance. 11.1 The general linear model General linear models lie behind a large number of techniques. These have different names depending on the type of data used to explain or predict the variability in a numerical response variable. Regression (Numerical variable) Analysis of variance (One or more categorical variables) Analysis of covariance (Categorical variable plus numerical variable) Multiple regression (Multiple numerical variables) Although these analyses are given different names and they appear in different parts of the menu in a program such as SPSS, they are all based on a similar mathematical approach to model fitting. In R the same model syntax is used in all cases. The steps needed to build any linear model are… Look at the data carefully without fitting any model. Try different ways of plotting the data. Look for patterns in the data that suggest that they are suitable for modelling. Fit a model: The standard R syntax for a simple linear regression model is mod&lt;-lm(y~x) However model formulae may be much more complex. Look at whether the terms that have been entered in the model are significant: The simplest R syntax is anova(mod). Again, this part of the process can be become much more involved in the case of models with many terms. Summarise the model in order to understand the structure of the model. This can be achieved with summary(mod) Run diagnostics to check that assumptions are adequately met. This involves a range of techniques including statistical tests and graphical diagnoses. This step is extremely important and must be addressed carefully in order to ensure that the results of the exercise are reliable. 11.2 Regression 11.2.1 Theory The regression equation is .. \\(y=a+bx+\\epsilon\\) where \\(\\epsilon=N(o,\\sigma^{2})\\) In other words it is a straight line with a as the intercept, b as the slope, with the assumption of normally distributed errors (variability around the line) The diagram below taken from Zuur et al (2007) illustrates the basic idea. In theory, if we had an infinite number of observations around each point in a fitted model, the variability would form a perfect normal distribution. The shape and width of the normal curves would be constant along the length of the model. These normal curves form the stochastic part of the model. The strait line is the deterministic part. This represents the relationship we are usually most interested in. The line is defines by its intercept with the axis and its slope. For any observed value of y there will be a fitted value (or predicted value) that falls along the line. The difference between the fitted value and the observed value is known as the residual. In reality we do not have infinite observed values at each point. However if we collected all the residuals together we should get a normal distribution. 11.2.2 Example The example is taken from Crawley’s R book. It is very simplified and idealised, but serves to explain the concepts. Crawley’s own ecological research involves herbivory. Here he presents an example in which the growth of insect larvae on leaves with different concentrations of tannin has been measured. Tannin concentrations slow insect growth (Crawley unfortunately does not provide the units in which this is measured). The question is, how much is growth inhibited by an increase of 1% in tannin concentration? The first step after loading the data is to produce a scatterplot. larvae&lt;-read.csv(&quot;/home/msc_scripts/data/larvae.csv&quot;) names(larvae) ## [1] &quot;growth&quot; &quot;tannin&quot; attach(larvae) plot(growth~tannin) The scatterplot is produced using the syntax that we will use in the model. Growth is a function of (~) tannin. We now fit a model and assign the results to an object in R. We can call this “mod”. This contains all the information about the fitted model. mod&lt;-lm(growth~tannin) We can now look at the properties of the model. If we ask for an “anova”. R will produce the following output. anova(mod) ## Analysis of Variance Table ## ## Response: growth ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## tannin 1 88.817 88.817 30.974 0.0008461 *** ## Residuals 7 20.072 2.867 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This is similar to the table produced for an ANOVA involving categegorical variables (factors). It can be interpreted in a similar way. Just as for an ANOVA we have an F ratio that represents the amount of variability that falls along the regression line divided by the residual variation. The numerator degrees of freedom for a simple regression is always one. The numerator degrees of freedom is n-2 as we have estimated two parameters, the slope and the intercept. Thus we have found a very significant effect of tannin concentration on growth F(1, 7) =31, p &lt;0.001. You should report the R² values along with the p-value. A correlation test would have shown the same significant relationship, but without as much detail. cor.test(growth,tannin) ## ## Pearson&#39;s product-moment correlation ## ## data: growth and tannin ## t = -5.5654, df = 7, p-value = 0.0008461 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9796643 -0.5972422 ## sample estimates: ## cor ## -0.9031408 The additional information that we have with a regression concerns the two elements in the regression equation. The slope and the intercept. summary(mod) ## ## Call: ## lm(formula = growth ~ tannin) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4556 -0.8889 -0.2389 0.9778 2.8944 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.7556 1.0408 11.295 9.54e-06 *** ## tannin -1.2167 0.2186 -5.565 0.000846 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.693 on 7 degrees of freedom ## Multiple R-squared: 0.8157, Adjusted R-squared: 0.7893 ## F-statistic: 30.97 on 1 and 7 DF, p-value: 0.0008461 Remember the equation we used. \\(y=a+bx+\\epsilon\\) The intercept (a) is 11.76 The slope (b) is -1.22 Which means that growth (whatever this was measured in by Crawley) is reduced by 1.22 for each increase of 1% in tannin concentration. 11.2.3 Confidence intervals There is an issue with this way of summarising the results. If we only report the coefficients as given we have ignored the fact that there was unexplained variation in the model. This variation produces uncertainty regarding the true values of the parameters. The greater the unexplained variation (scatter or noise) around the line, the less confident we can be regarding their values. The statistical mechanism used in the calculations (that is not particularly complex, but can safely be taken on trust) allows us find confidence intervals for the parameters. If the confidence intervals do not include zero then the parameters are significant. There are only a few cases where this is very meaningful for the intercept. We are usually more interested in the slope. confint(mod) ## 2.5 % 97.5 % ## (Intercept) 9.294457 14.2166544 ## tannin -1.733601 -0.6997325 So now we can go a bit further. Instead of giving a point estimate for the effect of tannin on growth we can state that the 95% confidence interval for the effect of a 1% increase of tannin on growth lies between -1.73 and -0.7 11.2.4 Prediction The equation can be used to predict the most likely value of y given a value of x. If we just ask R to “predict” we get the fitted values for the values of the explanatory variable that we used when fitting the model. predict(mod) ## 1 2 3 4 5 6 7 ## 11.755556 10.538889 9.322222 8.105556 6.888889 5.672222 4.455556 ## 8 9 ## 3.238889 2.022222 So we can plot out the data again and show the predicted values as red points and draw the regression line. plot(growth~tannin) points(tannin,predict(mod),pch=21,bg=2) lines(tannin,predict(mod)) 11.2.4.1 Prediction with confidence intervals As we have seen, a statistical model takes into account uncertainty that arises as a result of variability in the data. So we should not simple look at the line of best fit as summing up a regression. We should add some indication of our confidence in the result to the figure. To achieve this nicely in R requires a couple of extra steps. After plotting the data with plot(growth~tannin) we set up a sequence of 100 x values that lie between the minimum and the maximum. Now if we pass these to the predict function in R and ask for confidence intervals (that by default are 95%) we get the figure below. plot(growth~tannin) x&lt;-seq(min(tannin),max(tannin),length=100) matlines(x,predict(mod,list(tannin=x),interval=&quot;confidence&quot;)) The confidence bands refer to the fitted model. They show uncertainty regarding the regression line and are calculated from the standard error. 11.3 Using ggplot2 for confidence intervals. library(ggplot2) g0&lt;-ggplot(data=larvae, aes(x=tannin,y=growth)) g1&lt;-g0+geom_point() g1+geom_smooth(method=&quot;lm&quot;) 11.3.1 Prediction intervals There is another way to look at uncertainty. If we know the value for tannin, where would we expect a single measured value for growth to lie. Notice that this is different. The confidence bands show where the mean value migh lie if we measured growth many times. But in this case we want to know where we might expect a data point to fall. This is a much broader interval, and is based on the idea of theoretical normal curves falling around the regression line with a standard deviation estimated from the data. We cut off the tails of these curves. plot(growth~tannin) x&lt;-seq(min(tannin),max(tannin),length=1000) matlines(x,predict(mod,list(tannin=x),interval=&quot;prediction&quot;)) 11.3.2 Diagnostics It is not enough to simply fit a regression line, or any other model. We have to justify our choice of model and convince those who might use it that the assumptions have been adequately met. The question of how close the data are to meeting the assumptions requires model diagnostics. The basic assumptions for regression Normally distributed errors Identically distributed errors over the range (homogeneity of variance) No undue influence of points with high leverage An underlying linear relationship Independent errors In this rather artificial example all the assumptions are met. However real ecological data is rarely as simple as this. In order to justify the use of a regression model you must be able to show that the assumptions are not seriously violated. We will come on to what “seriously”&quot; means later. 11.3.2.1 Normality It is important to remember that the assumption of normality applies to the residuals after a model has been fitted. You do not test the for normality of the variable itself, as the relationship that you are modelling influences the distribution of the variable. You can look at the distribution of the residuals by plotting a histogram. hist(residuals(mod),col=&quot;grey&quot;) This looks good enough. A slightly more sophisticated way of spotting deviations from normality is to use a qqplot. If we ask R to plot a fitted model, we actually get a set of diagnostic plots. There are six of these. By default R will produce four of them. The qqplot is the second in the series. It is used as a visual check of the normality assumption. If the assumption is met the points should fall approximately along the predicted straight line. plot(mod,which=2) Qqplots often have a sigmoid shape. This occurs when some of the extreme values fall further along the tail of the normal distribution than expected. Providing this effect is slight it is not a problem. However deviations from the line away from the extremes does show a major departure from the assumption. Interpretation of QQplots requires some experience, but they can be very useful. If you have doubts about your ability to interpret the plot you could try using a statistical test of normality. shapiro.test(residuals(mod)) ## ## Shapiro-Wilk normality test ## ## data: residuals(mod) ## W = 0.98794, p-value = 0.9926 The problem with this test is that it becomes more sensitive as the number of observations increase. Thus you are more likely to get a p-value below 0.05 if you have a lot of data. However violations of normality become much less serious as the number of observations increase. So it can be safe to ignore a significant test result if the QQplot and histogram do not suggest major problems providing you have more than around 30 observations. If the histogram shows clear skew then you should think about a data transform, weighted regression or using a generalised linear model. More about this later. 11.3.2.2 Homogeneity of variance The assumption here is that the distance a point is likely to fall from the fitted line does not change along the x values. This is very often violated. For example if the values represent counts of individuals it is constrained to not fall below zero and will tend to be some function of the expected value. This results in residuals that follow a poisson distribution or more likely, a negative binomial. The characteristic of both these distributions is that the scatter increases as the expected value increases. The following lines produce some data with this characteristic in order to illustrate the pattern. set.seed(5) library(MASS) x&lt;-sample(0:10,20,rep=T) y&lt;-rnegbin(20,mu=x,theta=2) plot(y~x) mod.negbin&lt;-lm(y~x) You should be able to spot a “fan” effect. The third diagnostic plot in the series is used to see this effect more clearly. This plot shows the square root of the standardised residuals plotted against the fitted values. Because the square root is used all the values are positive. If there is an increasing (or decreasing) trend in the absolute size of the residuals it should show up on the plot. plot(mod.negbin,which=3) The red line is meant to guide the eye, but you should look carefully at the pattern rather than the line itself. This is the pattern for the original model, that does not have a problem with heterogeneity of variance. plot(mod,which=3,main=&quot;Growth model&quot;) If you are having difficulty interpreting the results you could try a test for the significance of the trend using a correlation test between the variables shown on the diagonistic plot. cor.test(sqrt(stdres(mod.negbin)),predict(mod.negbin)) ## Warning in sqrt(stdres(mod.negbin)): NaNs produced ## ## Pearson&#39;s product-moment correlation ## ## data: sqrt(stdres(mod.negbin)) and predict(mod.negbin) ## t = 4.8093, df = 7, p-value = 0.001945 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5071470 0.9737071 ## sample estimates: ## cor ## 0.8761687 cor.test(sqrt(stdres(mod)),predict(mod)) ## Warning in sqrt(stdres(mod)): NaNs produced ## ## Pearson&#39;s product-moment correlation ## ## data: sqrt(stdres(mod)) and predict(mod) ## t = -0.50019, df = 2, p-value = 0.6666 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9803574 0.9236405 ## sample estimates: ## cor ## -0.3334484 The same sort of caveats apply to the literal interpretation of this test as a decision rule as the normality test. Although most violations of homogeneity of variance have to be taken seriously, large data sets may show statistically significant, but inconsequential violations. 11.3.2.3 Leverage In the artificial example provided by Crawley all the points are equally spaced along the x axis. This is the “classic” form for a regression and prevents problems with leverage. However in ecology we often have to analyse data that does not have this property. Some of the x values on a scatterplot may fall a long way from the centre of the distribution. Such points potentially could have an excessive influence on the form of the fitted model. The influence a point has on the regression is a function of leverage and distance from a line fitted using all the other points. alt text To illustrate this let’s add a point to the data at a high tannin density and assume that zero growth was measured at this value. How much does this new point influence the model? tannin&lt;-c(tannin,15) growth&lt;-c(growth,0) We can plot the new data and show the effect of this one point on the regression line. It is shown in red. new.mod&lt;-lm(growth~tannin) plot(growth~tannin) lines(tannin,predict(mod,list(tannin))) lines(tannin,predict(new.mod,list(tannin)),col=2) This data point has shifted the regression quite a lot due to a combination of its distance from the others (which gives it high leverage) and the fact that it lies a long way from the fitted line (high residual deviation). These two effects are captured by a quantity known as Cook’s distance. The diagnostic plot shows leverage on the x axis and standardised residuals on the y axis. Large residuals with low leverage do not affect the model, so the safe area with low values for Cook’s distance (below 1) forms a funnel shape. Points falling outside this “safe area” can be flagged up. There are no problem points in the original model. plot(mod,which=5) However the point that we added with both a high leverage and a high deviation from the original model shows up clearly when we diagnose the model with this point included. plot(new.mod,which=5) Another way of spotting this influential points is by looking at Cook’s distance directly. Values over 1 are typically considered to be extreme. plot(new.mod,which=4) larvae$cooks&lt;-cooks.distance(mod) g0&lt;-ggplot(data=larvae, aes(x=tannin,y=growth)) g1&lt;-g0+geom_point() g1&lt;-g1+geom_smooth(method=&quot;lm&quot;) g1+geom_point(size=larvae$cooks*10,col=&quot;red&quot;) 11.3.3 Lack of independence The value and sign of the residual deviation should not be related in any way to that of the previous point. If residual values form “clusters” on one side of the line or another it is a sign of lack of independence. There are two causes of this. The most serious is intrinsic temporal or spatial autocorrelation. This is discussed in the next section. A less serious matter is that the shape of the model is not appropriate for the data. We can see this in the last example. While the strait line might have been OK for the range of data originally given by Crawley, a straight line is a poor model when an additional data point is added at a high tannin level. At some point tannin stops all growth, so the underlying model must be asymptotic. Thus the model with the extra point has correlation in the raw residuals. High tannin values have positive residual deviation as a result of the poor fit of the model. This can be seen by using the first diagnostic plot that R produces. plot(new.mod, which=1) A common way of testing for lack of independence is the Durbin Watson test for serial autocorrelation. library(lmtest) ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric dwtest(growth~tannin) ## ## Durbin-Watson test ## ## data: growth ~ tannin ## DW = 2.0168, p-value = 0.3679 ## alternative hypothesis: true autocorrelation is greater than 0 11.3.3.1 Putting it all together If you just ask R to plot a model you get all four plots. With some practice these should help you spot problems that need attention. Notice that to get four plots on one page you use par(mfcol=c(2,2)). par(mfcol=c(2,2)) plot(mod) 11.4 Where do the sum of squares come from? Contemporary approaches to analysis of variance and regression place the two techniques together under the heading of General Linear Models.This is the approach used in R. In both cases the formula that is dropped in to the call to fit a linear model is similar. The difference is that one way ANOVA uses a factor to predict fitted values and calculate residuals whereas regression uses a numerical variable. In order to demonstrate the fundamental similarity between the two models we will set up some simulated data and then calculate the sum of squares from first principles. 11.4.0.1 ANOVA Let’s make up a predictor variable. This is a factor with three levels, call them A, B and C. Now, let’s assume that the fitted values (in other words thede terministic pattern of response to the three levels of this factor) are mean values of 10, 15 and 20. These are the expected responses if there were no variability apart from that between groups. set.seed(1) predictor&lt;-as.factor(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),each=10)) fitted&lt;-rep(c(10,12,20),each=10) Of course, there is going to be variability within each group. So let’s add this in. We will assume that the variability can be modelled as a normal distribution with mean zero and standard deviation of 5. So the results we actually get are derived by adding these two components together. residuals&lt;-rnorm(30,mean=0,sd=5) results&lt;-fitted+residuals d&lt;-data.frame(predictor,results) boxplot(results~predictor,data=d) Of course, because this was a simulation experiment the actual means will be slightly different to the invested values. We can set up a data frame with true fitted and residual values like this. mod&lt;-lm(results~predictor,data=d) d&lt;-data.frame(predictor,results,fitted=fitted(mod),residuals=residuals(mod)) head(d) ## predictor results fitted residuals ## 1 A 6.867731 10.66101 -3.7932830 ## 2 A 10.918217 10.66101 0.2572027 ## 3 A 5.821857 10.66101 -4.8391570 ## 4 A 17.976404 10.66101 7.3153901 ## 5 A 11.647539 10.66101 0.9865250 ## 6 A 5.897658 10.66101 -4.7633558 11.4.0.2 The numerator sum of squares OK, so where do the numbers in the Anova table come from? The first step is to understand that the numerator sum of squares represents all the deterministic variability in the system. This is the variability attributable to differences between groups. The sum of squares is the sum of the squared deviations around the mean. But, which mean? In this case it is the overall mean value for the respnse. So, look at the boxplot again, but this time remove the variability and just plot the means. We can show the difference for each mean from the grand mean using arrows. boxplot(fitted~predictor,data=d) abline(h=mean(results),lwd=3,col=2) arrows(1,10.66,1,mean(results),code=3) arrows(2,13.24,2,mean(results),code=3) arrows(3,19.33,3,mean(results),code=3) OK, so for each mean there are 10 fitted values. The sum of squares is simply nsqrs&lt;-(d$fitted-mean(results))^2 nsumsqrs&lt;-sum(nsqrs) 11.4.0.3 Denominator sum of squares The denominator sum of squares in an Anova table represents all the variability that can be attributed to the stochastic component of the model. In other words, the residual variablity. We produced that when simulating the data. The values are simply the residuals once the means for each group have been subtracted. g0&lt;-ggplot(d,aes(x=predictor,y=results)) g0+geom_point(pch=21,bg=2)+stat_summary(fun.y=mean,geom=&quot;point&quot;,size=4) So each residual is the distance between the red points and the large black point representing the mean for that group. dsqrs&lt;-d$residuals^2 dsumsqrs&lt;-sum(dsqrs) mod&lt;-lm(results~predictor) anova(mod) ## Analysis of Variance Table ## ## Response: results ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## predictor 2 396.36 198.18 8.9192 0.001062 ** ## Residuals 27 599.93 22.22 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 nsumsqrs ## [1] 396.3639 dsumsqrs ## [1] 599.9315 The degrees of freedom for the munerator sum of squares are the number of free parameters in the model. This is one less than the number of groups because we need to calculate the grand mean from the data, and thus use up degree of freedom. In order to calculate the residuals we need to calculate three means in this case, so the denominator degree of freedom is the total sample size minus three. 11.4.1 Regression The set up for regression is in effect identical, apart from the fact that the fitted values are continuous rather than group means. So if we invent some data x&lt;-11:40 y&lt;-10+x*2+rnorm(30,0,10) plot(y~x) mod&lt;-lm(y~x) d&lt;-data.frame(x,y,fitted=fitted(mod),residuals=residuals(mod)) head(d) ## x y fitted residuals ## 1 11 45.58680 32.84861 12.738183 ## 2 12 32.97212 34.88166 -1.909533 ## 3 13 39.87672 36.91470 2.962016 ## 4 14 37.46195 38.94774 -1.485794 ## 5 15 26.22940 40.98079 -14.751383 ## 6 16 37.85005 43.01383 -5.163776 plot(y~x) points(x,fitted(mod),pch=21,bg=2) abline(h=mean(y),lwd=4,col=3) arrows(x,fitted(mod),x,mean(y),code=3) nsqrs&lt;-(d$fitted-mean(y))^2 nsumsqrs&lt;-sum(nsqrs) 11.4.2 Residuals plot(y~x) points(x,fitted(mod),pch=21,bg=2) arrows(x,fitted(mod),x,y,code=2) dsqrs&lt;-d$residuals^2 dsumsqrs&lt;-sum(dsqrs) mod&lt;-lm(y~x) anova(mod) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 9289.5 9289.5 141.99 1.759e-12 *** ## Residuals 28 1831.9 65.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 nsumsqrs ## [1] 9289.517 dsumsqrs ## [1] 1831.902 11.5 Where does R squared (coefficient of determination) come from? The “explained variability” in the data is often reported using R squared. High values suggest that a large proportion of the variability is “explained” by the model, whether the model is a regression or an ANOVA. Where does that fit in? Well the total sum of squares is the sum of all the squared distances that we have calculated. If we divide the sum of squares attributable to the model by the total we get the proportion of the variability attributable to the model. totsumsqrs&lt;-nsumsqrs+dsumsqrs nsumsqrs/totsumsqrs ## [1] 0.8352817 We can see that this is the same as R provides by asking for a summary. summary(mod) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.751 -5.120 -1.579 5.365 18.129 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.4851 4.5945 2.282 0.0303 * ## x 2.0330 0.1706 11.916 1.76e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.089 on 28 degrees of freedom ## Multiple R-squared: 0.8353, Adjusted R-squared: 0.8294 ## F-statistic: 142 on 1 and 28 DF, p-value: 1.759e-12 11.5.1 Violations of assumptions What do you do if your diagnostic analysis shows that there is a problem with the model? The first thing to realise is that all models are approximations of reality. As the statistician G.E. Box famously stated “All models are wrong … but some are useful.” If journal editors and referees only admitted papers which used the “correct” model ecological publishing would stop in its tracks. The important point to be aware of is that not all violations of the assumptions are equally serious. Almost all of them can be handled by more advanced modelling, but sometimes it is simply enough to point out that a minor violation was noticed, but it was not considered serious enough to prevent further analysis, together with a clearly stated justification. 11.5.1.1 Normality of the residuals This is considered to be the least important assumption of the model. Slight deviations from normality will not usually affect the conclusions drawn. Furthermore some influential authors, eg Sokal and Rolf, state that the central limit theorem means that the assumption can safely be ignored for large (n&gt;30) samples. However, be careful with this, as the assumption is that the residuals are non-normal, but homogeneous. This is unlikely to occcur. 11.5.1.2 Fixed values of the independent variable This assumption is in fact nearly always violated in ecological studies. We never measure anything without error. The main point to be aware of is that it is error relative to the range that causes an issue. So, if we measure a variable such as tree height with an accuracy of 10cm and the range of values falls between 5m and 20m there is no problem. However if the range of heights were to be only between 10m and 11m an issue may arise. You should always aim to have a large range of values for the explanatory variable with respect to measurement errors. 11.5.1.3 Homogeneity of variance Violations of this assumption can be quite serious. However you should be aware that fixing the problem using techniques such as weighted regression will not affect the slope of the regression line. It will affect the p-values (making them larger) and confidence intervals (making them wider). If the p-value from the uncorrected model is very small (p&lt;0.001) then a correction for heterogeneity of variance is unlikely to result in a loss of significance. If the p-value is close to the conventional cut off (p&lt;0.05) then the correction will almost certainly change your conclusions. In some cases the apparent heterogenity is the result of outliers that need removal. Doing this will of course change the slope. 11.5.1.4 Incorrect model form A regression is a straight line. In many ecological situations the true responses take the form of a curve of some kind. Asymptotes are very commonly predicted both from theory and “common sense”. For example, adding more nitrogen fertilizer beyond a certain point will not produce any more growth. The biggest problem for regression in these circumstances is not that the model does not fit. It may approximate quite well to one part of the curve. The issue is that the functional form is misleading and does not represent the underlying process. 11.5.1.5 Influential outliers We have seen that an outlier does not necessarily influence the regression unless it also has high leverage. You have to think carefully before simply removing these points. If points at the extreme ends of the x axis have high residual values it may be that the model is badly formed. We have seen the potential for this in the example above. In this case you may want to restrict a linear regression to the linear part of the relationship and remove data at the ends. Flexible models such as GAMs which we will see later can deal with the issue. You may also use a data transformation to pull the extreme values in. It may well turn out that the issue simply is due to mistaken measurements, in which case the extreme values are rejected after checking. There are also other methods to formally handle the problem such as robust regression. 11.5.1.6 Independence of the errors This is the big issue. Violations of this assumption are always serious. The main problem is that if observations are not independent the model is claiming many more degrees of freedom (replication) than is justified. This means that the model cannot be generalised. The issue affects many, if not all, ecological studies to some extent. Measurements are rarely truly independent from each other in space and time. The issue affects the interpetation of the p-value and confidence intervals of the model. While the model may still be suitable for prediction within its sample space it will not generalise beyond it. In the next class we will look at an example in detail that may help to clarify this. "],
["advice-on-regression.html", "Chapter 12 Advice on regression 12.1 Introduction 12.2 Predictive modelling: Calibration 12.3 Explanatory modelling: 12.4 Testing for a relationship 12.5 Diagnostics show assumptions not met 12.6 Skewed independent variable 12.7 Spearman’s rank correlation 12.8 Non linear response", " Chapter 12 Advice on regression 12.1 Introduction In the last class we looked at the theory underlying linear regression. To recap, there are three related reasons to use regression. The same analysis is used in all three cases, but different aspects of the output are emphasised in order to address the scientific question. To calibrate some form of predictive model. This is most effective when a large proportion of the variance is explained by the regression line. For this to be effective we require reasonably high values of R² and emphasis falls on the form of the model and confidence intervals for the key parameter, which is the slope of the fitted line. This is a measure of how much the dependent variable (plotted on the y axis) changes with respect to each unit change in the independent variable (plotted on the x axis). To investigate the proportion of variance “explained” by the fitted model. The R² value provides a measure of the variance explained. In the context of explanation this is the quantity that is emphasised. The R² value is a measure of how much scatter there is around the fitted relationship. To test whether there is any evidence at all of a relationship. In this context the statistical significance provided by the p-value is the useful part of the output, in the context of rejecting the null hypothesis if no relationship at all (slope=0). If the sample size is small the evidence for a relationship will be weaker than with a large sample size. Low R² values (i.e. high levels of scatter) also make it more difficult to detect any evidence of a trend. 12.2 Predictive modelling: Calibration If everything goes well you could be looking a very clear, linear trend with narrow confidence intervals. This may occur with morphological data where the width of some feature is closely related to the length, or in the context of some calibration experiment. The estimated regression equation is Y= 102.34 + 1.95X, R² = 0.96 P-value &lt; 0.001 12.2.1 Advice on writing up In this situation you should concentrate your write up on interpreting the implications of the slope of the line. You have found a very clear relationship between x and Y. State this. Include the (narrow) confidence intervals for the parameters of the model. Discuss how the quantiative relationship may be used in research. ## 2.5 % 97.5 % ## (Intercept) 97.719605 106.96516 ## x 1.868064 2.02701 State the R² value and also make sure that you have clearly stated the sample size, defined the sample space and the methods used to measure the variables in the methods section. You should also state that the p-value was &lt;0.001 but you don’t need to say very much about significance, as you are not interested in the probability that the data may be the result of chance when the scatterplot shows that the relationship is so clear cut. The important part of your discussion should revolve around the fitted line itself. You may suspect that the functional form for the relationship is not in fact a straight line. In future studies you might use nonlinear regression to model a response, but at this stage you do not yet know how to fit such models formally, so you should just discuss whether a straight line is appropriate. 12.3 Explanatory modelling: In many situations the scatter around the line may be much greater that the previous case. This will lead to much less of the variance being explained by the underlying relationship. However there is still a clear relationship visible, providing the sample size is large enough. The estimated regression equation is Y= 139.36 + 1.28X, R² = 0.13 P-value &lt; 0.001 12.3.1 Advice on writing up In this situation the emphasis falls on the R² value, which here is comparatively low. You should state the p-value in order to confirm that the relationship is in fact significant. You should think about the reasons behind the scatter. It may be that the measurements themselves included a large amount of random error. However in ecological settings the error is more likely to be either simply unexplainable random noise, or some form of process error. The random noise around the line may in fact be associated with additional relationships with other variables which are not included in the simple model. This can be handled by multiple regression which uses more than one variable for prediction, and by a range of other techniques including modern machine learning. However these are more advanced, so you should concentrate on discussing how the model could be improved rather than finding ways to improve it at this stage. It is always very important to discuss the sample size used in the study in all cases. Small samples may cause type 2 error. Type 2 error is a failure to reject the null hypothesis of no relationship even though there is some relationship there. It occurs when the sample size is too small to detect a relationship, even though one does in fact exist. Notice that we always fail to reject a null hypothesis. We do not state that there is no relationship. Instead we state that the study failed to provide evidence of a relationship. 12.4 Testing for a relationship ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 23680 23680 1.8018 0.2163 ## Residuals 8 105136 13142 A smaller sample taken from the same sample space as the previous data set now does lead to an inconclusive study. The estimated regression equation is Y= 135.5 + 1.69X, R² = 0.18 P-value = 0.216 12.4.1 Advice on writing up In the case of a p-value over 0.05 you may want to note that there may be some indication of a relationship but that the results are statistically inconclusive. There is insufficient evidence provided by this particular study to confirm any relationship. The write up should emphasis the p-value and the study size. Include the F ratio 1.8 and the degrees of freedom of 1 on 8 . You should make suggestions regarding how future studies may be improved. 12.5 Diagnostics show assumptions not met There are many ways in which the assumptions of regression are not met. This is why diagnostics are needed. You should look through the handout for the last class in order to understand this more deeply. What do we do when assumptions are not met? There is no simple answer to this. A large number of alternative techniques have been designed to analyse data that do not meet the assumptions used in regression. In this course we cannot look at all of them. So we need some simple rules that will help to make the best of the data without introducing a large number of new concepts. 12.6 Skewed independent variable Sometimes the independent variable is heavily right skewed. The estimated regression equation is Y= 55.42 + 2.02X, R² = 0.61 P-value &lt; 0.001 Although you can still run a regression on data such as these, the diagnostic plots will show that many of the assumptions are not met. A common solution is to transform the independent variable by taking either the square root or the logarithm and then re-run the analysis. d$logx&lt;-log10(x) Now the regression looks much better and diagnostic plots will confirm that more assumptions are met. The estimated regression equation is Y= 50.09 + 96.18log(X), R² = 0.32 P-value = 0.009 12.6.1 Advice on writing up After a transformation you can write up the results in the same way that you would for any other regression model. However be careful how you interpret the slope and intercept given that you are now using a logarithmic scale. 12.7 Spearman’s rank correlation A very simple non-parametric analysis is very commonly used when regression assumptions are not met. This is Spearman’s rank correlation. 12.7.1 Taking ranks Instead of transforming the dependent variable we can take the ranks of both of the variables and use these in a regression. d$rankx&lt;-rank(d$x) d$ranky&lt;-rank(y) R² = 0.51 P-value &lt; 0.001 12.7.2 Advice on writing up Although the relationship between the ranked data can be shown on a scatter-plot with a fitted line, the slope and intercept of this line has little meaning. You therefore emphasise the R² value after noting that it refers to Spearmans rank correlation, not regression, and the p-value. A simple correlation test in R provides the p-value and the correlation coefficient (rho) cor.test(d$y,d$x,method=&quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: d$y and d$x ## S = 376, p-value = 0.0005407 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7172932 So the value of rho is 0.72 and the R squared is 0.51 12.8 Non linear response In many data sets the response may not follow a strictly linear trend. Take for example the yearly mean temperature data we looked at previously. You can fit a linear model to these data and report the results. The estimated regression equation is Y= -8.39 + 0.01log(X), R² = 0.21 P-value &lt; 0.001 However if you look carefully at the scatter-plot and model diagnostics you will see that many of the points fall in clusters on either side of the line. This known as serial autocorrelation in the residuals. It is an indication that the model may be ill formed, in other words that a straight line may not be the best way to describe the underlying relationship. One solution may be to use a non-linear smoother (note .. this is not the same as a non-linear model for a functional response). There are many forms of smoothers available. The theory behind their use can be quite complex, but in general terms they will tend to follow the empirical pattern in the response. library(mgcv) g0&lt;-ggplot(TMean,aes(x=Year,y=temp)) g0 + geom_point() + geom_smooth(method=&quot;gam&quot;, formula =y~s(x)) mod&lt;-gam(data=TMean,temp~s(Year)) anova(mod) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## temp ~ s(Year) ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Year) 5.885 7.044 8.14 5.07e-08 12.8.1 Advice on writing up In this case you need to discuss the pattern of the response as shown in the scatter-plot with fitted spline. The significance of the overall model can be quoted, but the emphasis falls on the shape of the response and “local” features such a break points and changes in slope. In this case not how there was a very clear linear trend between 1975 and 2010. There was a short period of slight cooling between 1940 and 1975. In a data set with very interpretable single values such as this one you may want to mention extremely cool and warm years explicitly. "],
["crib-sheet-for-regression-and-anova.html", "Chapter 13 Crib sheet for regression and ANOVA 13.1 Introduction 13.2 Loading the data 13.3 Finding out about the data 13.4 Making your data available to others 13.5 Subsetting 13.6 Data summaries for individual variables 13.7 Individual statistics for a single variable 13.8 Simple boxplot of one variable 13.9 Simple histogram of one variable 13.10 Neater histogram of one variable 13.11 Regression 13.12 Scatterplot without fitted line 13.13 Scatterplot with fitted line and labels 13.14 Fitting a model 13.15 Extracting residuals 13.16 Model summary 13.17 Model anova table 13.18 Confidence intervals for the model parameters 13.19 Spearman’s rank correlation 13.20 One way ANOVA 13.21 Histograms for each factor level 13.22 Confidence interval plot 13.23 Fitting ANOVA 13.24 Tukey corrected pairwise comparisons 13.25 Anova with White’s correction", " Chapter 13 Crib sheet for regression and ANOVA 13.1 Introduction This document provides all the code chunks that may be useful in the context of the data analysis component of the assignment. The data set used to illustrate is the mussels data, that can be analysed using one way ANOVA and regression in the context of calibrating a relationship. You should look through ALL the handouts provided on these techniques to understand the underlying theory. This “crib sheet” simply shows the most useful code. ALWAYS CHECK THAT THE STEPS HAVE BEEN TAKEN IN THE RIGHT ORDER. LOOK AT THE DATA YOU HAVE LOADED FIRST USE YOUR OWN VARIABLE NAMES PASTE IN CODE CHUNKS CAREFULLY; LEAVING GAPS BETWEEN EACH CHUNK COMMENT ON ALL THE STEPS 13.1.1 Packages needed Include this chunk at the top of you analysis to ensure that you have all the packages. It also includes the wrapper to add buttons to a data table if you want to use this. Remember that data tables can only be included in HTML documents. library(ggplot2) library(dplyr) library(mgcv) library(DT) theme_set(theme_bw()) dt&lt;-function(x) DT::datatable(x, filter = &quot;top&quot;, extensions = c(&#39;Buttons&#39;), options = list( dom = &#39;Blfrtip&#39;, buttons = c(&#39;copy&#39;, &#39;csv&#39;, &#39;excel&#39;), colReorder = TRUE )) 13.2 Loading the data Use read.csv with your own data set d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/mussels.csv&quot;) 13.3 Finding out about the data Checking the structure of the data. str(d) ## &#39;data.frame&#39;: 113 obs. of 3 variables: ## $ Lshell : num 122.1 100.1 100.7 102.3 94.9 ... ## $ BTVolume: int 39 21 23 22 20 22 21 18 21 15 ... ## $ Site : Factor w/ 6 levels &quot;Site_1&quot;,&quot;Site_2&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... You can also look at your data by clicking on the dataframe in the Global Environment window in R Studio. 13.4 Making your data available to others 13.4.1 NOTE THIS ONLY WORKS AS AN HTML DOCUMENT dt(d) 13.5 Subsetting If you want to run an analysis for a single site (factor level) at a time, you can get a dataframe for just factor level. s1&lt;-subset(d,d$Site == &quot;Site_1&quot;) s1&lt;-droplevels(s1) 13.6 Data summaries for individual variables Change the name of the variable to match a numerical variable in your own data set. The command removes NAs just in case you have them summary(d$Lshell,na.rm=TRUE) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 61.9 97.0 106.9 106.8 118.7 132.6 13.7 Individual statistics for a single variable Mean, median, standard deviation and variance. mean(d$Lshell, na.rm=TRUE) ## [1] 106.835 median(d$Lshell, na.rm=TRUE) ## [1] 106.9 sd(d$Lshell, na.rm=TRUE) ## [1] 14.84384 var(d$Lshell, na.rm=TRUE) ## [1] 220.3397 13.8 Simple boxplot of one variable Useful for your own quick visualisation. boxplot(d$Lshell) 13.9 Simple histogram of one variable Useful for your own quick visualisation. hist(d$Lshell) 13.10 Neater histogram of one variable This uses ggplot. Change the bin width if you want to use this. g0&lt;-ggplot(d,aes(x=d$Lshell)) g0+geom_histogram(color=&quot;grey&quot;,binwidth = 5) 13.11 Regression In this data set there are two numerical variables. So we can run a linear regresion. 13.12 Scatterplot without fitted line g0&lt;-ggplot(d,aes(x=Lshell,y=BTVolume)) g0+geom_point() 13.13 Scatterplot with fitted line and labels Type the text you want for the x and y axes to replace the variable names g0&lt;-ggplot(d,aes(x=Lshell,y=BTVolume)) g1&lt;-g0+geom_point() + geom_smooth(method=&quot;lm&quot;) g1 + xlab(&quot;Some text for the x asis&quot;) + ylab(&quot;Some text for the y axis&quot;) 13.14 Fitting a model Change the names of the variables in the first line. mod&lt;-lm(data= d, BTVolume~Lshell) 13.15 Extracting residuals d$residuals&lt;-residuals(mod) 13.16 Model summary summary(mod) ## ## Call: ## lm(formula = BTVolume ~ Lshell, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.828 -2.672 0.147 2.235 17.404 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -36.02385 3.33917 -10.79 &lt;2e-16 *** ## Lshell 0.59754 0.03096 19.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.864 on 111 degrees of freedom ## Multiple R-squared: 0.7704, Adjusted R-squared: 0.7684 ## F-statistic: 372.5 on 1 and 111 DF, p-value: &lt; 2.2e-16 13.17 Model anova table anova(mod) ## Analysis of Variance Table ## ## Response: BTVolume ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Lshell 1 8811.4 8811.4 372.49 &lt; 2.2e-16 *** ## Residuals 111 2625.7 23.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.18 Confidence intervals for the model parameters confint(mod) ## 2.5 % 97.5 % ## (Intercept) -42.6406346 -29.4070662 ## Lshell 0.5361881 0.6588891 13.18.1 Model diagnostics Look at the regression handout to understand these plots. plot(mod,which=1) plot(mod,which=2) plot(mod,which=3) plot(mod,which=4) plot(mod,which=5) 13.19 Spearman’s rank correlation Used if all else fails. Not needed with these data, but included for reference. g0&lt;-ggplot(d,aes(x=rank(Lshell),y=rank(BTVolume))) g0+geom_point() + geom_smooth(method=&quot;lm&quot;) cor.test(d$Lshell,d$BTVolume,method=&quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: d$Lshell and d$BTVolume ## S = 26143, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8912809 13.19.1 Fitting a spline Only use if you suspect that the relationship is not well described by a straight line. library(mgcv) g0&lt;-ggplot(d,aes(x=Lshell,y=BTVolume)) g1&lt;-g0 + geom_point() + geom_smooth(method=&quot;gam&quot;, formula =y~s(x)) g1 + xlab(&quot;Some text for the x asis&quot;) + ylab(&quot;Some text for the y axis&quot;) In this case the line is the same as the linear model. Get a summary using this code. mod&lt;-gam(data=d, BTVolume~s(Lshell)) summary(mod) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## BTVolume ~ s(Lshell) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.8142 0.4557 61.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Lshell) 1.493 1.847 198.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.77 Deviance explained = 77.3% ## GCV = 23.993 Scale est. = 23.463 n = 113 If you do use this model remember that its only needed if you can’t use linear regression. Report the ajusted R squared value, the estimated degrees of freedom and the p-value for the smooth term (not the intercept). You must include the figure in your report, as that is the only way to show the shape of the response. 13.20 One way ANOVA The purpose of one way anova is Test whether there is greater variability between groups than within groups Quantify any differences found between group means 13.20.1 Grouped boxplots Exploratory plots g0&lt;-ggplot(d,aes(x=Site,y=Lshell)) g0+geom_boxplot() 13.21 Histograms for each factor level g0&lt;-ggplot(d,aes(x=d$Lshell)) g1&lt;-g0+geom_histogram(color=&quot;grey&quot;,binwidth = 5) g1+facet_wrap(~Site) +xlab(&quot;Text for x label&quot;) 13.22 Confidence interval plot g0&lt;-ggplot(d,aes(x=Site,y=Lshell)) g1&lt;-g0+stat_summary(fun.y=mean,geom=&quot;point&quot;) g1&lt;-g1 +stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) g1 +xlab(&quot;Text for x label&quot;) + ylab(&quot;Text for y label&quot;) 13.23 Fitting ANOVA mod&lt;-aov(data=d,Lshell~Site) summary(mod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Site 5 5525 1105 6.173 4.58e-05 *** ## Residuals 107 19153 179 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.24 Tukey corrected pairwise comparisons Use to find where signficant differences lie. This should confirm the pattern shown using the confidence interval plot. mod&lt;-aov(data=d,Lshell~Site) TukeyHSD(mod) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Lshell ~ Site, data = d) ## ## $Site ## diff lwr upr p adj ## Site_2-Site_1 8.466769 -2.408905 19.342443 0.2201442 ## Site_3-Site_1 6.037019 -9.660664 21.734702 0.8737518 ## Site_4-Site_1 -3.619231 -19.316914 12.078452 0.9849444 ## Site_5-Site_1 18.697436 7.305950 30.088922 0.0000867 ## Site_6-Site_1 2.470769 -8.404905 13.346443 0.9859123 ## Site_3-Site_2 -2.429750 -18.201132 13.341632 0.9976925 ## Site_4-Site_2 -12.086000 -27.857382 3.685382 0.2355928 ## Site_5-Site_2 10.230667 -1.262165 21.723498 0.1103764 ## Site_6-Site_2 -5.996000 -16.977781 4.985781 0.6105029 ## Site_4-Site_3 -9.656250 -29.069479 9.756979 0.7004668 ## Site_5-Site_3 12.660417 -3.470986 28.791819 0.2123990 ## Site_6-Site_3 -3.566250 -19.337632 12.205132 0.9862071 ## Site_5-Site_4 22.316667 6.185264 38.448069 0.0015143 ## Site_6-Site_4 6.090000 -9.681382 21.861382 0.8718474 ## Site_6-Site_5 -16.226667 -27.719498 -4.733835 0.0011239 plot(TukeyHSD(mod)) 13.25 Anova with White’s correction This will give you the overall Anova table if there is heterogeneity of variance. library(sandwich) library(car) mod&lt;-lm(Lshell~Site, data=d) Anova(mod,white.adjust=&#39;hc3&#39;) ## Analysis of Deviance Table (Type II tests) ## ## Response: Lshell ## Df F Pr(&gt;F) ## Site 5 9.9682 7.541e-08 *** ## Residuals 107 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["crib-sheet-for-gis-in-r.html", "Chapter 14 Crib sheet for GIS in R 14.1 Introduction 14.2 Reading in a shapefile 14.3 Transforming to British National Grid 14.4 Mapview of a polygon layer 14.5 Adding an overview map 14.6 Adding a measurement tool 14.7 Reading in points from a CSV 14.8 Reading raster data 14.9 Making colour ramps for raster data 14.10 Extracting raster values for points", " Chapter 14 Crib sheet for GIS in R 14.1 Introduction The code assumes that the GIS layers exist in a folder called gis_data, which has a path to it on the folder as defined below. Otherwise It won’t actually run. However the code chunks can be adapted to other data sources easily enough. library(sp) library(raster) library(rgeos) library(rgdal) library(RColorBrewer) library(leaflet.extras) library(mapview) library(sf) path&lt;-&quot;/home/rstudio/webpages/Quantitative_and_Spatial_Analysis/Quantitative_and_Spatial_2018/Arne_project/gis_data&quot; 14.2 Reading in a shapefile This uses the sf package. You need to provide the path to the directory and the name of the shapefile, without extension. Remember that a “shapefile” actually consists of four files. You need all to be present in the folder. polys&lt;-read_sf(path,&quot;sssi&quot;) 14.3 Transforming to British National Grid The EPSG code for British national grid is 27700. If your shapefile is in another CRS you may need to transform it. polys&lt;-st_transform(polys,27700) st_crs(polys) ## CHeck the CRS ## Coordinate Reference System: ## EPSG: 27700 ## proj4string: &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs&quot; 14.4 Mapview of a polygon layer If you want to change the order of the basemaps or add more options change this chunk, that you run before making any maps. mapviewOptions(basemaps =c(&quot;OpenStreetMap&quot;,&quot;Esri.WorldImagery&quot;, &quot;OpenTopoMap&quot;, &quot;OpenStreetMap.BlackAndWhite&quot;, &quot;OpenStreetMap.HOT&quot;)) You can find more named basemaps from here. http://leaflet-extras.github.io/leaflet-providers/preview/ In this particular shapefile there is a column for id. That’s not very interesting, but we can shade on it. You can find a palette from here. http://colorbrewer2.org Or use this chart library(RColorBrewer) display.brewer.all() Set the number of levels and find the code letters for the palette and change the line used to make the pal object. Set the alpha variable to a number between 0 (invisible) and 100 (opaque) pal&lt;-brewer.pal(9, &quot;Set1&quot;) alpha=100 mp&lt;-mapview(polys, zcol = &quot;id&quot;,col=pal, col.regions = pal,legend = TRUE,alpha.regions=alpha/100) mp 14.5 Adding an overview map The code above forms a map object that can be modified through adding components. It is a good idea to add a full screen control. A mini map provides an overview. You can change the tile and zoom level. The best position is usually the bottomleft. tile&lt;-&quot;CartoDB.Positron&quot; ## Change if needed zoom&lt;- - 4 ## Larger negative values to zoom out pos&lt;-&quot;bottomleft&quot; mp@map %&gt;% addFullscreenControl() %&gt;% addMiniMap(position=pos,zoomLevelOffset = zoom,tiles=tile) 14.6 Adding a measurement tool mp@map %&gt;% addFullscreenControl() %&gt;% addMeasurePathToolbar() %&gt;% addMeasure( primaryLengthUnit = &quot;meters&quot;, primaryAreaUnit = &quot;hectares&quot;) # 14.7 Reading in points from a CSV Point data often is provided as a simple table with X and Y coordinates. This can be easily conversted to a spatial object, However you must know the CSV. pnts&lt;-read.csv(paste(path,&quot;study_points.csv&quot;,sep=&quot;/&quot;)) #coordinates(pnts) = ~X+Y # Set coordinates #pnts&lt;-st_as_sf(pnts) # Make sf object #st_crs(pnts) &lt;-27700 ## Set CRS pnts&lt;-st_as_sf(pnts, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 27700) ## Better way The points can then be plotted as a mapview. The col.region setting determines the fill color. If the points are in fact in latitude and longitude then declare this, and if necessary transform. #st_crs(pnts) &lt;-4326 #pnts&lt;-st_transform(pnts, 27700) Mapview maps are additive. so the points can be placed on the previously saved map. T mp&lt;-mp + mapview(pnts,col.regions=&quot;red&quot;) ## mp was built above mp@map %&gt;% addFullscreenControl() %&gt;% addMiniMap(position=pos,zoomLevelOffset = zoom,tiles=tile) Note that one slightly annoying feature of mapview is that legends do not disappear when a layer is not selected. So be careful when adding maps together. For clarity it may be preferable to make separate maps when using them to illustrate some aspect of the study as a static figure. A column can also be used to shade points with a legend as was used for the polygons. pal&lt;-brewer.pal(11, &quot;YlOrBr&quot;) mapview(pnts, zcol = &quot;npines&quot;,col.regions=pal,legend=TRUE) 14.8 Reading raster data slope&lt;-raster(paste(path,&quot;slope.tif&quot;,sep=&quot;/&quot;)) aspect&lt;-raster(paste(path,&quot;aspect.tif&quot;,sep=&quot;/&quot;)) dtm&lt;-raster(paste(path,&quot;dtm.tif&quot;,sep=&quot;/&quot;)) 14.9 Making colour ramps for raster data Displaying raster data can be quite challenging, as you need to find a good colour scheme. You may be able to use the brewer palettes, but they need to be turned into a ramp using an extra line of code. Some trial and error may be needed to find a suitable colour mix. pal&lt;-brewer.pal(11, &quot;PuOr&quot;) pal &lt;- colorRampPalette(pal) mapview(aspect,col.regions=pal, legend=TRUE) There are also some pre built palettes in R. The terrain colours can be useful. pal&lt;-terrain.colors(100) pal &lt;- colorRampPalette(pal) mapview(dtm,col.regions=pal, legend=TRUE) As previously, you can add elements such as a minimap and a full screen control to raster maps. mp&lt;-mapview(dtm,col.regions=pal, legend=TRUE) mp@map %&gt;% addFullscreenControl() %&gt;% addMiniMap(position=pos,zoomLevelOffset = zoom,tiles=tile) 14.10 Extracting raster values for points There are some complex elements involved in spatial overlays. However the simplest idea of taking the values from a raster layer and adding them to a data frame of points is very straightforward. Simple feature layers need to be transformed to the older spatial class in R. The following lines adds aspect derived from the raster layer to the points object. pnts&lt;-as(pnts, &quot;Spatial&quot;) pnts$aspect&lt;-raster::extract(aspect,pnts) pnts&lt;-st_as_sf(pnts) "],
["hengistbury.html", "Chapter 15 Hengistbury", " Chapter 15 Hengistbury library(raster) library(mapview) library(tmap) library(giscourse) library(sf) library(tidyverse) conn&lt;-connect() mapviewOptions(basemaps =c(&quot;Esri.WorldImagery&quot;,&quot;OpenStreetMap&quot;,&quot;Esri.WorldStreetMap&quot;, &quot;CartoDB.Positron&quot;, &quot;OpenTopoMap&quot;, &quot;OpenStreetMap.BlackAndWhite&quot;, &quot;OpenStreetMap.HOT&quot;,&quot;Stamen.TonerLite&quot;, &quot;CartoDB.DarkMatter&quot;, &quot;Esri.WorldShadedRelief&quot;, &quot;Stamen.Terrain&quot;)) query&lt;-&quot;select geom,lnr_name from local_reserves where lnr_name like &#39;Hengist%&#39;&quot; lnr&lt;-st_read(conn,query=query) bound&lt;-as(lnr, &quot;Spatial&quot;) mapview(lnr, alpha.regions = 0, legend=FALSE, lwd =3) %&gt;% extras() hh_habitat&lt;-sprintf(&quot;select main_habit mhabitat, s1habtype,s2habtype,s3habtype habitat, st_intersection(h.geom,s.geom) geom from (%s) s, ph_v2_1 h where st_intersects(h.geom,s.geom)&quot;,query) habitats&lt;-st_read(conn, query=hh_habitat) habitats$area &lt;- st_area(habitats) habitats%&gt;% st_buffer(0.001)%&gt;% group_by(mhabitat) %&gt;% summarise(area=sum(area))-&gt; habitats qtm(habitats, &quot;mhabitat&quot;) +tm_style(&quot;classic&quot;) mapview(habitats, zcol=&quot;mhabitat&quot;, burst=TRUE,legend=FALSE, hide = TRUE) %&gt;% extras() dsm&lt;- pgGetRast(conn, &quot;dsm2m&quot;,boundary=bound) dtm&lt;- pgGetRast(conn, &quot;dtm2m&quot;,boundary=bound) dsm[dsm&lt;0.1]&lt;-NA dtm[dtm&lt;0.1]&lt;-NA slope&lt;-terrain(dtm, opt=&#39;slope&#39;, unit=&#39;degrees&#39;) aspect&lt;-terrain(dtm, opt=&#39;aspect&#39;,unit=&#39;degrees&#39;) sloper&lt;-terrain(dtm, opt=&#39;slope&#39;, unit=&#39;radians&#39;) aspectr&lt;-terrain(dtm, opt=&#39;aspect&#39;,unit=&#39;radians&#39;) hillshade&lt;-hillShade(sloper,aspectr,angle=5) plot(hillshade, col = gray(0:100 / 100), legend = FALSE) plot(dtm, col = terrain.colors(25), alpha = 0.3, legend = TRUE, add = TRUE) "],
["arne-data-quick-load.html", "Chapter 16 Arne data quick load 16.1 Introduction", " Chapter 16 Arne data quick load 16.1 Introduction This handout includes code that will quickly load spatial data for the Arne are into memory. library(aqm) library(giscourse) library(mapview) library(sf) library(raster) data(arne) mapview(arne_quads) %&gt;% extras() "],
["arne-2019.html", "Chapter 17 Arne 2019 17.1 Loading the waypoints", " Chapter 17 Arne 2019 17.1 Loading the waypoints library(mapview) library(sf) library(dplyr) library(giscourse) path&lt;-&quot;/home/arne&quot; a&lt;-dir(path,pattern=&quot;gpx&quot;) a&lt;-sprintf(&#39;%s/%s&#39;,path,a) f&lt;-function(x){ d&lt;-st_read(x,layer = &quot;waypoints&quot;) d$group&lt;-x d } d&lt;- do.call(&quot;rbind&quot;,lapply(a,f)) ## Reading layer `waypoints&#39; from data source `/home/arne/D.gpx&#39; using driver `GPX&#39; ## Simple feature collection with 9 features and 23 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -2.048178 ymin: 50.68701 xmax: -2.038586 ymax: 50.68853 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## Reading layer `waypoints&#39; from data source `/home/arne/Group_A_Waypoints.gpx&#39; using driver `GPX&#39; ## Simple feature collection with 8 features and 23 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -2.048392 ymin: 50.6872 xmax: -2.038436 ymax: 50.6887 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## Reading layer `waypoints&#39; from data source `/home/arne/GroupB_Waypoints.gpx&#39; using driver `GPX&#39; ## Simple feature collection with 7 features and 23 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -2.048389 ymin: 50.68722 xmax: -2.038567 ymax: 50.68886 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## Reading layer `waypoints&#39; from data source `/home/arne/GroupC.gpx&#39; using driver `GPX&#39; ## Simple feature collection with 11 features and 23 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -2.048479 ymin: 50.68676 xmax: -2.038719 ymax: 50.68857 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs d %&gt;% select(c(&quot;name&quot;, &quot;group&quot;,&quot;time&quot;)) -&gt;d ## Error in (function (classes, fdef, mtable) : unable to find an inherited method for function &#39;select&#39; for signature &#39;&quot;sf&quot;&#39; mapview(d, zcol=&#39;group&#39;,burst=TRUE, legend=FALSE) -&gt;mp mp@map %&gt;% leaflet.extras::addFullscreenControl() coords&lt;-st_coordinates(d) clus&lt;-hclust(dist(coords)) plot(clus) d$site&lt;-as.factor(cutree(clus,2)) mapview(d, zcol=&#39;site&#39;,burst=TRUE, legend=FALSE) -&gt;mp mp@map %&gt;% leaflet.extras::addFullscreenControl() "],
["regression-analysis-using-measurements-on-individual-pines-from-the-arne-2019-data.html", "Chapter 18 Regression analysis using measurements on individual pines from the Arne 2019 data 18.1 Introduction 18.2 Scatterplots 18.3 Regression vs correlation analysis 18.4 Fitting a regression 18.5 Fitting a GAM", " Chapter 18 Regression analysis using measurements on individual pines from the Arne 2019 data 18.1 Introduction In today’s class you will learn how to run a regression analysis in R using the data collected from Arne as an example. There are several handouts in both the online course book for this unit and AQM that cover the theory of regression in more depth. The purpose of this class is to provide the tools in R to run a regression and interpret the output. 18.1.1 Load R libraries The libraries that are almost always used in any R analysis include ggplot2 (for graphics) dplyr (for data manipulation) and readr/readxl for data import. library(ggplot2) library(aqm) library(dplyr) library(readr) library(readxl) 18.1.2 Load the data The data collected at Arne this year has been collated into an Excel spreadsheet that you should have placed in the directory that you are using for the analyis. ## Loads in the first sheet in the excel spreadsheet. d&lt;-read_excel(&quot;Collated Pine Data.xlsx&quot;) If you are missing this file I have placed the data inthe aqm package for easy loading. ## This loads data from the aqm package data(arne_2019) d&lt;-pine_hts This line provides you with the data. However you should practice getting data into R from Excel yourself, as you will need to do this in future to run regression analyses and other statistical procedures on data that you will collect. 18.1.3 Looking at the data As you saw last session the import data set feature in Rstudio includes a command to view the data. This commend will not run within a markdown document. However the dt function in the aqm package does produce a very handy table of data that is placed within the compiled html document. You can use this to sort, filter and export your data. This is particularly useful if you are working with others who do not use R, as the data can be passed back into a spreadsheet. dt(d) 18.1.4 Simple base R figures When conducting any analysis the most important step is visualsing your data. This cannot be stressed enough. If you haven’t looked carefully at the patterns in your data then the statistics that you produce will be very difficult to understand. A rigorous regression analysis involvesconducting formal diagnostics which test the assumptions used in the model. Most of these diagnostic tests involve visualising patterns in the data. There are therefore two types of figures that you will produce when analysing your data. The first type are produced to help you understand your own data. Typically these figures are not included in the final write up of the analysis. They are steps along the path towards understanding your data. So they do not need to be formatted and labelled in a manner that would be required for publication. An example would be to produce a simple histogram of pine heights. hist(d$Age) boxplot(d$Age) summary(d$Age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 3.000 5.000 4.617 6.000 9.000 18.1.4.1 Exercise Produce histograms and boxplots of pine ages, heights and diameters. 18.2 Scatterplots A very simple scatter plot can be produced by simply asking to plot the variable to be placed on the x axis and the variable on the y axis. This is a quick and easy way of looking at the pattern. plot(d$DiMM, d$HeCM) A more sophisticated approach that can produce figures suitable for inclusion in a report uses ggplot. fig1&lt;-ggplot(d,aes(x=DiMM,y=HeCM)) + geom_point() fig1 18.3 Regression vs correlation analysis The word “correlation” is often used in a very loose sense by students to refer to associations between any sort of variables. In statistics correlation analysis involves looking at the (co)relationship between two numerical variables. Parametric correlation analysis is part of regression analysis. However the statistical significance of a correlation is not always of any interest. In this case we know a priori that there is a relationship between pine height and pine diameter. Pine trees increase both in girth and height as they grow. So we would never want to test the statistical significance of the relationship. However we do want to quantify the relationship and look at the pattern. 18.3.1 Spline and line analysis In many cases the purpose of regression analysis is not to detect whether a relationship exists. It is to look at both the shape and the parameters of the relationship. In the AQM unit we will look at this aspect of statistical modelling in some detail. At this stage we need some simple, intuitive, methods based on data visualistion. When we form a scatterplot we can often think of the pattern in terms of two components. An underlying relationship (signal) and scatter around the relationship (noise). Sometimes the signal takes the form of a straight line. In other situations it may take other forms. The ggplots package in R ia a very useful tool for detecting the underlying pattern in a scatterplot. The way to do this is to add geom_smooth to the points. fig1 +geom_smooth() By default the smoother used in a loess curve, which aims to follow the empirical pattern as closely as possible. The theory behind this is covered in aqm. The smoother will detect non linear patterns. Very few underlying patterns really follow the form a straight line. Regression analysis is useful if the pattern approximates to a straight line. In this case it does (up to a point). There is a slight curve in the trend tha would probably become more pronounced if larger pines were included in the sample. However we might argue that a straight line relationship is a reasonable aprpromation for the data in our particular size class range. fig1 + geom_smooth(method=&quot;lm&quot;) There is in fact a notable issue here. Very small pines all tend to fall below the regression line. We will come back to this. For the moment let’s (falsely) assume that it is OK to fit a relationship 18.4 Fitting a regression reg_mod&lt;-lm(data=d,HeCM~DiMM) summary(reg_mod) ## ## Call: ## lm(formula = HeCM ~ DiMM, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -89.094 -13.230 -2.461 11.893 105.200 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.485 2.345 12.57 &lt;2e-16 *** ## DiMM 3.165 0.108 29.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21.75 on 254 degrees of freedom ## Multiple R-squared: 0.7716, Adjusted R-squared: 0.7707 ## F-statistic: 858.3 on 1 and 254 DF, p-value: &lt; 2.2e-16 The regression equation takes the form of \\(y = a + bx\\) In this case the equation is y = 29.5 + 3.2x The R2 value is 77.2 % Notice that there is a problem with this equation. The intercept of th regresion line represents the y value when x = 0. We would expect this to be zero. In many situations the intercept is not particularly important, as the interpretation of a regression equation is that it is an empirical fit to the data within the bounds of the data collected. In other words you can’t use the equation to predict the heights of trees either larger or smaller than those found in the sample. However in this particular case the model does not fit the data very well at low diameter values, so the high value for the intercept is the result of the fact that the relationship is only approximately represented by a straight line. 18.5 Fitting a GAM library(mgcv) gam_mod&lt;-gam(data=d,HeCM~s(DiMM)) summary(gam_mod) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## HeCM ~ s(DiMM) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 85.453 1.265 67.56 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(DiMM) 4.106 5.096 202.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.802 Deviance explained = 80.5% ## GCV = 417.93 Scale est. = 409.6 n = 256 fig2&lt;-fig1 + geom_smooth(method=&quot;gam&quot;, formula=y~s(x)) + xlab(&quot;Diameter of juvenile pines (mm) measured at 5 cm above ground level&quot;) + ylab(&quot;Height of juvenile pines (cm)&quot;) fig2 "],
["analysing-the-residuals-from-regression-analysis-on-individual-pines-from-the-arne-2019-data.html", "Chapter 19 Analysing the residuals from regression analysis on individual pines from the Arne 2019 data 19.1 Introduction 19.2 Adding the GPS points to the pines 19.3 Looking at the residual", " Chapter 19 Analysing the residuals from regression analysis on individual pines from the Arne 2019 data 19.1 Introduction In today’s class you will learn how to run a regression analysis in R using the data collected from Arne as an example. There are several handouts in both the online course book for this unit and AQM that cover the theory of regression in more depth. The purpose of this class is to provide the tools in R to run a regression and interpret the output. 19.1.1 Load R libraries The libraries that are almost always used in any R analysis include ggplot2 (for graphics) dplyr (for data manipulation) and readr/readxl for data import. library(ggplot2) library(aqm) library(dplyr) library(mgcv) library(mapview) library(sf) 19.1.2 Load the data When the data is loaded from the aqm package it also contains the gps way points that you collected. ## This loads data from the aqm package data(arne_2019) mapview(gps) You can look at the points as a data frame gps %&gt;% st_set_geometry(NULL) %&gt;% dt() Notice that the waypoints alone do not contain a variable that indicates the site. One way to obtain this (there are a lot of possibilities) is to cluster the points by proxmity. clust&lt;-hclust(dist(st_coordinates(gps))) ## Default option for a distance matrix uses euclidaen distance which is what we want. plot(clust) There are clearly two main clusters of points. The cutree function in R cuts this tree into a given number of groups and returns an identifying number for the group. So we can now find the two sites easily and map the results out. gps$site&lt;-as.factor(cutree(clust,2)) mapview(gps,zcol=&quot;site&quot;, burst=TRUE) 19.2 Adding the GPS points to the pines When we have two different data tables in R of any type we can always join them together if we can find a unique identifying feature that the two data sets have in common. Let’s look at the names of the variables in the gps data str(gps) ## Classes &#39;sf&#39; and &#39;data.frame&#39;: 35 obs. of 5 variables: ## $ name : Factor w/ 11 levels &quot;001&quot;,&quot;002&quot;,&quot;003&quot;,..: 1 2 3 4 5 6 7 8 9 1 ... ## $ time : POSIXct, format: &quot;2019-10-30 11:35:43&quot; &quot;2019-10-30 11:55:18&quot; ... ## $ group : Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 4 4 4 4 4 4 4 4 4 1 ... ## $ geometry:sfc_POINT of length 35; first list element: &#39;XY&#39; num -2.04 50.69 ## $ site : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 2 2 2 1 ... ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA ## ..- attr(*, &quot;names&quot;)= chr &quot;name&quot; &quot;time&quot; &quot;group&quot; &quot;site&quot; The element that uniquely identifies the point is the name of the waypoint combined with the group that collected the data. The pine_hts data has similar fields. str(pine_hts) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 256 obs. of 6 variables: ## $ Group: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Site : num NA NA NA NA NA NA NA NA NA NA ... ## $ WP : num 1 1 1 2 2 2 3 3 3 4 ... ## $ Age : num 8 4 7 5 7 7 4 5 4 5 ... ## $ DiMM : num 26.77 6.39 14.41 35 25.65 ... ## $ HeCM : num 100 43 72 125 95 77 95 99 75 141 ... Notice that the Site column is currently blank. We can make an id column for each set of data, then merge the results. gps$id&lt;-paste(gps$group,as.numeric(gps$name), sep=&quot;_&quot;) pine_hts$id&lt;-paste(pine_hts$Group,pine_hts$WP, sep=&quot;_&quot;) pine_hts_gps&lt;-merge(gps,pine_hts) mapview(pine_hts_gps,zcol=&quot;site&quot;) 19.3 Looking at the residual Let’s make a working data frame to simplify typing. d&lt;-pine_hts_gps These data are the same as we used previously, so the scatterplot should look the same. fig1&lt;-ggplot(d,aes(x=DiMM, y=HeCM)) +geom_point() + geom_smooth() fig1 19.3.1 Plotting the data by site If we add a colour aesthetic to our ggplot then the data will be split into two labeled groups which correspond with the site. fig2&lt;-ggplot(d,aes(x=DiMM, y=HeCM, colour=site)) +geom_point() + geom_smooth() fig2 What can you see from this figure? 19.3.2 Obtaining residuals Last time we tried fitting both lines and splines to the height diameter relationship. Regression lines were found to be useful for summarising the reslationship, but splines provided a closer fit to the actual data. So we can take the residuals from the spline. d$residuals&lt;-residuals(gam(data=d,HeCM~s(DiMM))) This is the equivalent of measuring the distance between the fitted line shown in figure one and each data point. Some will be positive and others negative. Negative values imply that the pine height is lower than expected for a given diameter. 19.3.3 Plotting the residuals by site fig3&lt;-ggplot(d,aes(x=site,y=residuals)) fig3 + geom_boxplot() 19.3.4 Confidence intervals aqm::ci(fig3) 19.3.5 Simple unpaired t-test t.test(d$residuals~d$site) ## ## Welch Two Sample t-test ## ## data: d$residuals by d$site ## t = -5.884, df = 248.55, p-value = 1.289e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -18.556135 -9.248877 ## sample estimates: ## mean in group 1 mean in group 2 ## -6.842640 7.059866 "],
["mapping-and-analysing-pine-densities-at-arne.html", "Chapter 20 Mapping and analysing pine densities at Arne 20.1 Introduction 20.2 Loading the data 20.3 ——————————————————- 20.4 Investigating factors affecting pine densities on the heathland site 20.5 Spatial data 20.6 Raster algebra 20.7 Raster aggregation 20.8 Adding the values to the quadrats 20.9 Distances to trees 20.10 Data table 20.11 Writing out the data", " Chapter 20 Mapping and analysing pine densities at Arne 20.1 Introduction This handout aims to step through most of the work for the assignment. 20.1.1 Loading libraries The first step as always is to set up a chunk which loads the libraries. Some of the libraries send warning messages that are printed in the knitted document. To avoid them set some knit options by copying the line below into the very first chunk. knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE) library(aqm) library(giscourse) library(raster) library(ggplot2) library(sf) library(tmap) library(mapview) library(dplyr) library(mgcv) 20.2 Loading the data The data we are going to use consists of numbers of pines counted in circular quadrats over three years of field work. The first survey took place in 2017. At that time pines were growing throughout the heathland. The students decided to sample them through a “random walk” transect design in order to cover as much of the area as possible. As we saw in the field, many of the pines have been removed from Coombe heath since then. So in 2018 and 2019 the survey concentrated on smaller areas that had not been pulled. data(&quot;arne_pines&quot;) What do these data consist of? We can answer the question by clicking on the object in the global Environment window. Or we can add the data to the knitted document itself using the dt function dt(arne_pines) 20.2.1 Points to notice Notice that the site is a factor with two levels. Heath and restoration. In other words all the quadrats in Coombe’s heath are labeled as coming from heath and those in the restoration site are labeled. The restoration site was only measured in the last survey, so this will be treated rather differently in the analysis. The pine_density variable has been calculated by dividing the number of pines by the area of the quadrat. Last year the students decided to use smaller quadrats, so the counts of pine numbers are not directly comparable until they have been standardised in this way. The pine density multipied by an area in square meters produces an estimate of the total number of pines. There are coordinates of the quadrats as longitude (lon) and lattitude (lat). This is the most universal way of storing spatial data. It is easy to convert these to national grid when we need to measure areas and distances. 20.2.2 Making a spatial object in R The data as they stand are not yet in a GIS format. They are just a data frame that can be opened in Excel, or any other similar program. There is spatial information in the form of the coordinates. To map the data we need to give the points a geometry. We do that by telling R which columns hold the coordinates. We also need to set the CRS. In this case it is EPSG code 4326. quadrats&lt;-st_as_sf(arne_pines,coords = c(&quot;lon&quot;,&quot;lat&quot;),crs=4326) This code line can be used to transform any data held in a spreadsheet with known coordinates into a spatial layer. Now we can investigate the data using a mapview map. Notice that setting the zcol to year and adding burst = TRUE produces a map that can show each year’s data as if it were a separate layer. The “extras”&quot; are added using a function in the giscourse package. This adds a full screen button, a measuring tool and a collapsable mini map to the view. These are from the leaflet.extras package and can be added separately if required. mapview(quadrats, zcol=&quot;Year&quot;, burst=TRUE) -&gt; map1 map1 %&gt;% extras() 20.2.3 Action You should spend some time investigating this map. Go full screen and change some of the options. You can experiment with the measuring tool. For the purposes of this assignment if you want to use this or any leaflet web map as a static figure in a word document you may take a screen shot of the map. There are other more formal ways of making static maps directly in R but these take time and some nowledge of R to get right. A screenshot is good enough for the asssignment. 20.2.4 Comparing density in the heathland and restoration site. The restoration site was only measured this year. If we only want to use this year’s data then we will need to apply a filter. quadrats %&gt;% filter(Year==2019) -&gt; quadrats_2019 If you look in the Environnment panel in RStudio you will now see an object with only 35 observations. These are the data that you collected. 20.2.5 Question: How does the pine density differ between the heathland and the restoration site? This is a site specific question. The analysis may be relevant tothe assignment. Finding an answer to this question does not directly answer any broader scientific questions regarding the processes taking place. However it may still be useful for management at Arne and there may be broader implications that are suggested by the answer. If the conversion of former pine forest to heathland is difficult as a result of rapid regeneration this has broader implictaions. So how do we address this question statistically? The first step is always to visualise the data. Let’s use ggplots. See the chapter on making figures. 20.2.6 Boxplot of pine density g0&lt;-ggplot(quadrats_2019,aes(x=site,y=pine_density)) g0 +geom_boxplot() 20.2.7 Action Write a brief explanation of the pattern you observe in the boxplots. Are the data approximately normal? How can you tell from the boxplots? Should you remove any outliers? If not, why not? 20.2.8 Inferential confidence interval plot The quick way to make this is to use the ci function that has been included in the aqm package. Just use the ci funcion on the base plot with the aesthetics set (g0). This adds 95% confidence intervals based on the assumption that the variability around the means follows an approximately gaussian (normal) distribution. g1&lt;-ci(g0) g1 You can add labels and customize this plot in other ways. See the handout on ggplots. You should label the axes and caption the figure with an explanation of how confidence intervals have been calculated. 20.2.9 Action Write your own interpretation of the figure. Run an appropriate statistical test. (Hint .. there is a traditional and widely used test that finds the statistical significance of a difference between two means) What is being assumed by this test? Are these assumptions reasonable? t.test(quadrats_2019$pine_density~quadrats_2019$site) ## ## Welch Two Sample t-test ## ## data: quadrats_2019$pine_density by quadrats_2019$site ## t = -2.6065, df = 15.122, p-value = 0.01974 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.6627076 -0.1672924 ## sample estimates: ## mean in group Heath mean in group Restoration ## 0.5911429 1.5061429 20.3 ——————————————————- 20.4 Investigating factors affecting pine densities on the heathland site The assignment instructions ask you to look at whether the density of pine regeneration on the heathland is associated with elements such as slope, topographic wetness, direct beam insolation and distance to potential seed source. To do this we first need to filter out only the quadrats that are on the heathland site. quadrats %&gt;% filter(site==&quot;Heath&quot; ) -&gt; heath_quadrats The coordinates are in latitude and longitude, However the Lidar data that we are going to combine with these data is held in British national grid. So we need to transform the quadrat data. heath_quadrats&lt;-st_transform(heath_quadrats,crs=27700) Note that the EPSG code is 27700. 20.5 Spatial data The raster layers for Arne and some other data are bundled in a data object in the aqm package. The line below loads the data into memory. data(arne_lidar) Notice that after running this line of code you will see some lidar derived raste layers appear in the Envir0nment panel. dtm The lidar derived digital terrain model at 2m resolution. dsm The lidar derived digital elevation model at 2m resolution. There are many ways of making quick static maps in R from raster data. One possibility is to use qtm in the tmap package. Contours can be derived from thh dtm with one line. contours&lt;-rasterToContour(dtm, levels=seq(0,60,5)) The results can be turned into a quick default printable map with the qtm function. qtm(dtm) + qtm(contours) +qtm(quadrats) The data included a polygon drawn around the study are. mapview(study_area,alpha.regions = 0,lwd=4) 20.5.1 Cropping A very common GIS operation is to crop a large raster layer to the bounding box of a smaller area. This is very easy in R. We can use the study area polygon for cropping. pol&lt;-as(study_area,&quot;Spatial&quot;) ## Convert to the older spatial classes dsm&lt;-raster::crop(dsm,pol) dtm&lt;-raster::crop(dtm,pol) We can now build a mapview to look at the cropped dsm and dtm quite easily. The default colours don’t look very good, so the code also includes a palette. contours&lt;-rasterToContour(dtm, nlevels=10) map2&lt;- mapview(dsm,col.regions=terrain.colors(1000),legend=TRUE) + mapview(dtm,col.regions=terrain.colors(1000)) + mapview(contours) + mapview(heath_quadrats) map2 %&gt;% extras() 20.5.2 Terrain analysis There are a large number of algorithms built into R that can be used to conduct terrain analysis. These produce the same results as the comparable options in QGIS. Working in R has the advantage of being quicker once a script has been built, and also reproducible. For example deriving slope and aspect in degrees involve running the following lines. slope&lt;-terrain(dtm, opt=&#39;slope&#39;, unit=&#39;degrees&#39;) aspect&lt;-terrain(dtm, opt=&#39;aspect&#39;,unit=&#39;degrees&#39;) Notice that when these lines of code are run the results aren’t immediately apparent. New objects appear in the Envoronment pane. You need to plot them out are use mapview to see the results directly. plot(slope) plot(aspect) Making a hillshade layer requires slope and aspect to be calculated in radians, rather than degrees. sloper&lt;-terrain(dtm, opt=&#39;slope&#39;, unit=&#39;radians&#39;) aspectr&lt;-terrain(dtm, opt=&#39;aspect&#39;,unit=&#39;radians&#39;) hillshade&lt;-hillShade(sloper,aspectr,angle=5) plot(hillshade,col=grey.colors(1000),legend=FALSE) The dtm can be draped over the plot by making it semi transparent by setting an alpha level. plot(hillshade,col=grey.colors(1000),legend=FALSE) plot(dtm, add=TRUE, col=terrain.colors(1000),alpha=0.4) R can also call some more advanced algorithms, such as those which calculate topographic wetness index and insolation. These algorithms effectively combine elements such as slope and aspect into more interpretable variables. twi&lt;-giscourse::twi(dtm) sol&lt;- giscourse::insol(dtm, day = &quot;11/06/2019&quot;) The topographic wetness index measures the upslope area draining through a certain point per unit contour length. In other words values of the topographic wetmess index (which is unitless) can be used to compare where a quadrat lies on a slope. Water drains from low values to high values, so on average the soil will be moister although we may not know by how much) when the index is low. plot(twi) The insolation layer is caluculated precisely using mechanistic preinciples. When a date is provided for the algorithm the routine steps through each hour of the day and calculates the solar energy that each pixel will recieve on a totally clear day through direct beam insolation. All other things being equal, pixels with high values will warm up more than pixels with low values on a sunny day. The pattern will change over the year. plot(sol) 20.6 Raster algebra Carrying out raster algebra (i.e. sums) operations in R is very simple and inuitive. To obtain the height of vegetation (and maybe some buildings ) we just need to subract the digital terrain model from the digital surface model. chm&lt;-dsm-dtm plot(chm) 20.7 Raster aggregation We often need to coarsen the resolution of a raster map and use some statistic calculated from all the pixels within a window. For example, if we are interested in the vegetation height around a study point we would not want to know the precices height at the point as it might just happen to fall into a small gap in a forest canopy. The orginal raster is at 2m resolution, so if we want mean vegetation height at 10m resolution we use factor of 5. chm10&lt;-aggregate(chm, fact=5, fun=mean, expand=TRUE, na.rm=TRUE) We may want to set all values below a low threshold to NA chm[chm&lt;0.1]&lt;-NA chm10[chm10&lt;0.2]&lt;-NA mapview(chm10,col.regions=terrain.colors(100)) -&gt; map3 map3 %&gt;% extras() 20.7.1 Action Investigate this map. Compare the canopy height model with the Esri satelite image. Does the canopy height model reliably pick out trees and taller vegetation? 20.8 Adding the values to the quadrats IN the context of the assignment the reason for producing the raster maps is to find the values for the variables that coincide with the quadrats that we placed on the ground. This is like placing your finger on each of the maps at the position of the quadrat, reading off the value, then adding the value to the original spreadheet of results. Rather than go through this very tedious process we can run some lines of R code to do the same. hq&lt;-as(heath_quadrats,&quot;Spatial&quot;) heath_quadrats$twi&lt;-raster::extract(twi,hq) heath_quadrats$sol&lt;-raster::extract(sol,hq) heath_quadrats$dtm&lt;-raster::extract(dtm,hq) heath_quadrats$slope&lt;-raster::extract(slope,hq) 20.9 Distances to trees To calculate the distance between the quadrats and the nearest tree we can use the canpoy height model. Assuming that the heights represent vegetation we can first make polygons that represent groups of pixels over a given height. chm[chm&lt;2]&lt;-NA ## Set below 2m as blanks (NA) chm[chm&gt;=2]&lt;-1 ## Set above 2m to a single value trees&lt;-rasterToPolygons(chm, dissolve=TRUE) trees&lt;-st_as_sf(trees) st_crs(trees)&lt;-27700 trees&lt;-st_cast(trees$geometry,&quot;POLYGON&quot;) map1 + mapview(trees) Now once we have the objects that represent the “trees” we can make a distance matrix. This contains all the distances measured between the quadrats and the trees. distances&lt;-st_distance(heath_quadrats,trees) dim(distances) ## [1] 184 1666 Finally to calculate the distances, find the minimum between each quadrat and the trees. heath_quadrats$min_dist&lt;-apply(distances,1,min) Now we can check if the distances are right by using the measuring tool mapview(heath_quadrats, zcol=&#39;min_dist&#39;) + mapview(trees) -&gt;map4 map4 %&gt;% extras() 20.10 Data table The resulting data table can be written out as a data frame. This can form the basis of your statistical analysis. See the next handout. heath_quadrats %&gt;% st_set_geometry(NULL) -&gt; quads dt(quads) 20.11 Writing out the data The resulting data table can be written out as a data frame. This can form the basis of your statistical analysis. See the next handout. write.csv(quads, file=&quot;arne_quadrats.csv&quot;, row.names=FALSE) #arne_quads &lt;-quads #save(arne_quads,file=&quot;~/rstudio/aqm/data/arne_quads.rda&quot;) "],
["statistical-analysis-of-the-pine-density-data.html", "Chapter 21 Statistical analysis of the pine density data 21.1 Introduction 21.2 Running analyses", " Chapter 21 Statistical analysis of the pine density data 21.1 Introduction In the last handout we extracted variables that may (or may not) be associated with the densities of pines that were measured in the quadrats at Arne. To do this we used R as a Geographical Information system. Now that we have built a data table we can return to using R for statistical analysis. It is probably a good idea to start a new markdown document for this part of the assignment work. To prevent confusion clear the Environment using the “sweeping brush” in the top right panel. 21.1.1 Loading libraries knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE) We are not going to make maps, so we don’t need as many libraries. library(aqm) library(ggplot2) library(dplyr) library(mgcv) You now just need to read in the data. d&lt;-read.csv(file=&quot;arne_quadrats.csv&quot;) # Alternative data load. Only use these lines if you have problems loading the csv # Uncommenting (removing the # activates the code) # data(arne_quads) # d&lt;-arne_quads dt(d) 21.2 Running analyses OK, so now you have the data. It is up to you to design some relevant analyses. These do not need to be very complex for this assignment. You should produce relevant scatterplots with fitted lines. Don’t worry if you can’t actually find any clear cut relationships between variables. This is a common occurence when conducting real life field work. If there are no clear relationships then you need to think carefully about why they are not clear. As an example of one possible analysis I have included code for plotting figure to investigate the relationship between pine density and topographic wetness index. g0&lt;-ggplot(d, aes(x=twi,y=pine_density)) g1&lt;-g0 + geom_point() g1 + geom_smooth() g1 + geom_smooth(method=&quot;lm&quot;) mod&lt;-lm(data=d, pine_density~twi) summary(mod) ## ## Call: ## lm(formula = pine_density ~ twi, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3920 -0.8437 -0.1104 0.8288 2.7921 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.7592 0.3890 4.522 1.1e-05 *** ## twi -0.1152 0.1010 -1.140 0.256 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.955 on 182 degrees of freedom ## Multiple R-squared: 0.007094, Adjusted R-squared: 0.001638 ## F-statistic: 1.3 on 1 and 182 DF, p-value: 0.2557 "]
]
