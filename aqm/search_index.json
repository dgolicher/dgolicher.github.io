[
["introduction.html", "Advanced Quantitative methods Chapter 1 Introduction 1.1 The new statistics 1.2 Statistical models vs statistical tests 1.3 Bayesian vs frequentist approaches to inference 1.4 Using p-values with discretion 1.5 Common pitfalls", " Advanced Quantitative methods Duncan Golicher 2020-03-02 Chapter 1 Introduction In this unit you wil learn to apply a range of statistical methods that are considered to be more advanced than are typically taught on an introductory course in data analysis. However the unit aims to provide more than a simple set of recipes for applying methods. Advances in computing power have resulted in an explosion of new methods for analysing ecological data. It would be impossible to design a course that includes examples of all the possible methods that might be relevant to the analysis of a data set that forms the subject of a master’s dissertation. Data analysis does not simply involve applying statistical methods. Good data management and effective data manipulation are just as important as good analysis. So an important element of the course wil focus on understanding the nature of data and the ways data can be manipulated. 1.1 The new statistics The underlying philosophy of the course is based on a contemporary concept of good statistical practice. This has sometimes been called “the new statistics” (Hobbs and Hilborn 2006) The aim of the new statistics is to evaluate the relative strength of evidence in data for hypotheses represented as models. Traditionally, models used by ecologists for statistical inference have been limited to a relatively small set of linear forms. The functional forms and definitions of parameters in these models were chosen for statistical reasons; that is, they were not constructed to explicitly symbolize biological states and processes. Consequently, composing models to represent hypotheses has traditionally played a relatively minor role in developing testable statements by most ecological researchers. Statistical models were used to represent verbal hypotheses, and little thought was applied to the model building that ultimately supported inference. The new statistics require deliberate, thoughtful specification of models to represent competing ecological hypotheses 1.2 Statistical models vs statistical tests “New statistics” has developed not just as a response to advances in computing power, although that has played an important role. A fundamental element of contemporary thinking with regard to the use of statistics is the avoidance, or at the least the downplaying, of null hypothesis **tests* (NHST). This can be confusing for students who have taken introductory courses in statistics in which the words “test” and “p-value” constantly were emphasised. Many students (and many researchers) may be unaware that the whole basis of null hypothesis testing was in fact controversial from the outset. Many prominent statisticians have argued against NHST (Nickerson et al. 2000). Some of the criticisms that can be found in Nickerson’s excelent review have been severe. For example “Null hypothesis testing provides researchers with no incentive to specify either their own research hypotheses or competing hypotheses…. it is surely the most bone-headedly misguided procedure ever institutionalized in the rote training of science students” (Gigerenzer 1998). The biggest problem with NHST is that it does not actually test a hypothesis of genuine interest. Some of the other arguments against NHST made by statisticians are quite technical in nature, although they are well worth trying to understand. A reasonably non-technical review is provided by Goodman (2008) in which 12 common misconceptions are laid out clearly. A very influential and highly cited paper that restates many of the criticisms of NHST made by statisticians in the context of ecology is Johnson (1999). I won’t repeat all the arguments made by Johnson. It it well worth reading the paper. 1.3 Bayesian vs frequentist approaches to inference It is my belief (and the word belief is important in this context) that all students should be aware that a vigorous debate arose between two schools of thought regarding the nature of statistical inference at the time that many of the conventional, text book, statistical tests were being developed. It is fascinating to read the words of R A Fisher writing in the “Design of experiments”, first published in 1935 . Fisher stated that “I shall not assume the truth of Bayes’ axiom”. What is most remarkable about that particular statement is that Bayes’ axiom is a simple mathematical consequence of applying the rules of probability. There is no controversy whatsoever regarding the axiom’s mathematical validity. Fisher was aware that implementing what was known as “inverse probability” would be mathematically unfeasible in the context of the sort of problems he was working on at the time. Integration formulas to calculate marginal likelihoods are horrendously complex in even very simple cases involving continuous ditributions, and are totally intractable for most designs. However his main objection to the use of Bayes’ theorem was that to him it appeared to introduce an element of subjectivity, and thus did not allow researchers to reach clear conclusions. It is unfortunate that this led to what some studies of the history of scientific thought have described as a “holy war” between two schools of thought. The unfortunate consequence of this was that efforts to remove the incorporation of subjective prior beliefs into analyses led to the conventional analytical device of NHST in which all prior information was ignored. This was never intended by Fisher himself. It occurred almost by accident, as a result of the development of NHST by Pearson and Neyman. Fisher originally intended p-values to be indications that further research was worth conducting, and not as decision rules regarding the truth of either a null hypothesis nor any alternative hypothesis. However for a while this got overlooked in statistical courses that emphasised the logic and mathematical basis of NHST without placing the null in the context of any field of study. Students being told to state null hypotheses in the form \\(H_0\\) “There is no difference in species richness between heathland and grassland” would be quite correct in pointing out the illogical and unhelpful nature of \\(H_0\\). We already know a priori that there must be a difference, so why on earth would we ever test this? The answer is that we clearly should not. In an applied setting we would aim to develop a much richer and more informative analysis of the difference between the two vegetation types, conditional on the data that we have available. There is no need to place subjective prior probabilities on the size of the difference in order to do this, but to ignore all relevant knowledge regarding the two systems would clearly be quite wrong. Finding evidence against \\(H_0\\) is only worth stating as an objective of a study such if \\(H_0\\) is inherently credible. This can be reasonable in the context of a well planned experiment, but In all other cases there will be much more information to be obtained from the data when \\(H_0\\) is not considered to be an objective of the analysis. 1.4 Using p-values with discretion In the quantiative and spatial analysis unit I introduced a pragmatic approach towards NHST that will continue to be used on this unit. This approach emphasises confidence intervals as the most important result of applying technique based on frequentist inference. It accepts the use of p-values associated with null hypothesis tests, but only as the “statistic of last resort”. The actual numerical values of confidence intervals match those produced by Bayesian methods with non-informative priors in many cases. So, confidence intervals can informally be interpreted as measures of uncertainty regarding the parameters of interest, even if formally they are not. When taking this approach, when running any regresion analysis the most important result would be the slope of the line, and associated confidence interval. The next most valuable output is the coeficient of determination (R squared) as a measure of the signal to scatter ratio. If all else fails, then a p-value can be reported, with due discretion and care with regard to the wording of the interpretation, as a “test” of any detectable association between variables conditional on the data obtained. Although power analysis is highly advisable when planning any study in order to assess whether the level of replication is likely to be sufficient to provide statistical signficance, when adopting this pragmetic approach you should not set out with the initial intention of “testing” a null hypothesis. Instead you should aim to estimate effect sizes. The most important element of any study is the scientific question that is of interest. Many questions can only be answered through estimating the size of effects and/or the shape and form of any response. Simply detecting a difference, an association or a correlation does not usually answer the question fully. We should, and usually can, do much better than that. This implies that non parametric methods will not be taught on this course, although it is still worth being aware of them as potential fall back methods of last resort when all else fails. Non parametric tests are easy to run and understand in R. The R code to run them is provided in this crib sheet for reference. http://r.bournemouth.ac.uk:82/AQM/AQM_2018/Crib_sheets/Classical_statistical_tests.html Always follow the advice of your supervisor with regards to the presentation of p-values. Some supervisors will insist on p-values being reported for all analyses. As they are included in the output of all inferential methods it is simple enough to include them in a report, even when they are not particularly informative. Just make sure that confidence intervals are also provided and used when answering questions regarding effect sizes and meaningful comparisons. 1.5 Common pitfalls Some of the advice provided in the literature regarding the validity of statistical tests when assumptions are not met is potentially misleading. In non experimental studies the assumptions of almost any inferential method are rarely met in full. So technically almost all statistical methods might be considered to be “invalid”. In reality it is not the violation of an assumption that is important, it is the degree to which an assumption is violated and the influence that any violation may have on the validity of the substantive findings that is really important. If an analysis produces a p-value of &lt; 0.0001 when using a method that leads to minor violations of assumptions, almost any correction will still lead to rejection of the null hypothesis. So a conclusion based only on NHST will not change at all. On the other hand confidence intervals may have to be broadened when the violations are corrected for, so a modification as a result of making a correction for unmet assumptions will take place. This is a powerful additional argument against emphasising NHST alone (naked p-values) that is not mede clear in all the papers cited here. Many students have learned to test “the data”&quot; for normality. This is poor advice, for many reasons, not least of which is that students often test the wrong element in the data The assumption of normality in regresion analysis applies to the residuals, not to the raw date (Zuur, Ieno, and Elphick 2010). Rigorous diagnostics are a very important part of building statistical models, but these are best conducted through careful inspection of the patterns shown in the residuals rather than through automated statistical tests. A statistical test of normality does not actually test the data anyway. It is, in reality, testing whether the data could have been obtained from an underlying normal distribution. The only way to “test” the data themselves is to look at them carefully. As suggested by Steel et al. (2013), plot the data early and often. When looking at observational data that were not derived from a conventional experimentla design, you should adopt an incremental approach to choosing an analysis. There is nothing to prevent you using the “wrong” models as initial tools for understanding the data and (hopefully) finally choosing a better, more appropriate model as the analysis progresses. References "],
["using-the-rstudio-server.html", "Chapter 2 Using the RStudio server 2.1 Introduction 2.2 Getting started with the RStudio server 2.3 RStudio server concepts 2.4 Finding your way around the interface 2.5 Using projects in RStudio 2.6 Uploading data 2.7 Working with markdown documents. 2.8 Forming a markdown document. 2.9 Reading in your data 2.10 Adding analysis chunks 2.11 Compiling a report", " Chapter 2 Using the RStudio server 2.1 Introduction RStudio is a complete environment for working with the R language. Once you have got used to it you will find that it makes working with R far more productive than using the R console version. However some of the concepts involved in using RStudio may be new. RStudio provides an interface for working with R code, rather than an interface for running analyses directly. 2.2 Getting started with the RStudio server The RStudio server version runs directly through any web browser. There is no need to install any software on your laptop, PC or tablet Access to the server is through the following URL. This works both on and off campus. http://r.bournemouth.ac.uk:8789/ http://r.bournemouth.ac.uk:8789/ 2.2.1 Log into the RStudio server Click on the URL http://r.bournemouth.ac.uk:8789/ in a browser. Use Firefox or Chrome. You will see a log in page. Log in using the username and the password you have been provided 2.3 RStudio server concepts The RStudio server is an integrated platform for doing the following … Saving and sharing data files Running analyses Compiling reports Connecting to data stores Sharing analyses with others. Advanced features can be used without any programming skills through sharing scripts. However you do need to become familiar with some new concepts in order to use the server. The RStudio server is ideal for collaborative work. You have your own permanent space on the server for saving your own work and building up a portfolio of useful analyses. Only one person can be logged in at any one time under your username. However I can always log into your user space at any time in order to help correct any errors and to give you advice on the analysis. 2.4 Finding your way around the interface Once you are logged in you will see three sections of the interface by default. This will change to four sections when you begin using scripts in the interface. Figure 2.1: Animated gif tour of the Rstudio interface. Look carefully at the interface and learn to recognise the sections. The RConsole. This is showing up on the left hand side when you first log in. The console can be used for running R code interactively. There is a tab showing up labelled “terminal” as well. You won’t use this, as it is for more advanced programming. The environment, history and connections pane is at the top right of the screen. The environment tab is the one that is most used. This tab will show the data that is in the active workspace in R. The concept will only become clear after beginning to use R. The files, plots, packages, help and viewer tab at the bottom right. The files tab is the most important to understand at this stage. There will be no files in your home directory yet, nor will there be any folders. A key concept to understand when using the server is that your home directory on the server is like a directory (folder) on your PC. It is rather like the university H drive. However it is all “encapsulated” on the server, which is also running R. So it is distinct from your H drive and it is not directly linked. In order to move data files and scripts into your home directory you must upload them. You will see buttons labelled New Folder, Upload, Delete, Rename and More. If you click on the More button you will also find an option to Export your files. The upload and the export buttons are frequently used to move files onto the server and to directly move files off the server. It is very important to be aware of this concept. Files saved on the server will always be available for use later. In contrast active analyses that take place in the server memory, as opposed to the server’s hard disk space, will be temporary and will be lost between sessions. 2.5 Using projects in RStudio You can use Rstudio without opening projects. However, projects make organising your work much simpler. A project is a set of instructions to restore the server to the same state that it was in when you closed the project. So if you are analysing a range of data sets you can use one project per data set to keep your work organised. To form a new project and add a new folder Click on the file menu at the top left of the interface. Go to New Project You will see a window with three options to create a project. Choose the first option labelled New Directory The next window will show a range of advanced options. Ignore them and just select New Project You will now see a window with a prompt for the Directory name (and some other options). In this example the project will contain data on sleep in mammals, so “sleep” could be used as the directory name. Click create project Look at the files pane in the bottom right corner. You will now see that after Home there is the word sleep. You can also see a file called sleep.Rproj in the folder. Click on home. You can see a folder called sleep in your home directory. So .. you have created a new project and placed the project file within the folder. Figure 2.2: Animated gif showing the steps taken when opening a new project. (Note the gif will be static in a PDF version of this document) 2.6 Uploading data The course material generally uses data that is already placed on the server. However in order in order to conduct your own data analyses you will need to upload data files to your folders. The easiest way to ensure that the data and the data analysis are kept unified is to upload the data into the same folder you opened for the analysis project. That way there will be no need to specify a path when the data are read into R. Although R can read data from many different formats, the data files that you upload must be in some form of conventional format. The easiest format to use is to save each table as a single comma separated variable (.csv) file. The first line should contain short variable names with no spaces. The variable definitions should be kept separately and referred to when writing figure labels and captions, but not used in the column headers. Data files are added to the project using the upload button in the files pane (bottom right). If you want to upload multiple files at once (e.g shapefiles) you should first compress them into a zip file. The zip file will expand when uploaded. 2.7 Working with markdown documents. This course will concentrate on the use of markdown documents as a way of running R code. There are many advantages of using markdown. Embedded code can be either revealed to other users to show how the results were obtained or hidden to simply produce a report with embedded figures and statistics. Annotation of the results of an analysis can be embedded around the results to explain the key results. Very limited knowledge of the R language and syntax is necessary to adapt markdown documents in order to analyse your own data. With a little more knowledge and experience of R complex methods can be applied by altering markdown found on-line. 2.8 Forming a markdown document. Go to file on the top menu bar Choose “New file” Choose “R Markdown” You will now see a window in which you can type in a name for the title of your analysis. By default the name is “untitled”. Change that to some title that makes sense for the analysis you are going to run. It is easy to change the title later. You will now see an untitled markdown document added to the top pane in RStudio. Rather confusingly it is still untitled, even though you’ve just typed a title! The reason for this is that the title you typed is used as the first line of the data report, but so far you still have not saved the file as a named document. Click on save to save the report. Now give the file itself a name. The steps are shown in the animated gif below Now try pressing the “knit” button on the top right pane. You will see the default demonstration document that was produced as a template “knit” into a simple data report. This is not yet using your data of course. Figure 2.3: Compiling the demonstration markdown document. The steps above will always produce the default “demo” markdown document. Every time you start a new markdown Rstudio will start off with this one. You should take a look at the logic of the demo document carefully. It consists of “chunks” of R code that produce output in the form of tables and figures embedded in text. The R code automatically produces output and adds it to the document after knitting. So if you have R code available that will run an analysis that you are interested in you don’t have to remember any other steps in order to run it. Simply ensure that the data that is being added to the analysis is appropriate for the type of analysis being run and you can obtain the same results with your own data. This will be the way R is used in this course. Figure 2.4: Removing the default text from a new markdown document 2.9 Reading in your data If you are new to R you may be tempted to look around for a button on the RStudio interface to “load” the data file. You won’t find one. Although there is a way to load data interactively you really must not do this. Always make sure that you include a line of code that loads the data at the start of your R script. Do not load data using the import data feature when building a markdown document. It will not work, as the data will not be loaded when you compile. In this case the working directory that R is using coincides with the project directory. So there is no need to include the path to the data file. This line will read the file into R and assign the data to a data frame called “d”. d&lt;-read.csv(&quot;sleep.csv&quot;) To form a code chunk click on the button on the interface labelled “Insert”. Alternatively the keyboard short-cut control alt I will work. Then type the code very carefully into the chunk. Make sure that the code sits within the body of the chunk and that you do not disturb the dashes that separate the chunk from the rest of the document. You should type the line into a single block of code that loads the data. Some types of data, such as GIS layers or SPSS data files require packages to be loaded first. You should include a code chunk to load these packages first if you need them. When conducting an analysis with just a single data frame I often call the data object “d”. This is just for brevity. As there are no other data objects present there is no ambiguity. If you load several data frames you should give them more informative names. When you have finished typing the line of code to load the data, click on the run button of the chunk. You will see a data object appear in the environment pane in the top right corner of the RStudio server. If you click on this object it will open as a spreadsheet like table in the main panel. Figure 2.5: Clicking on the name of the data frame in the Environment tab opens up a spreadsheet like interface to inspect the data 2.10 Adding analysis chunks Once you have the data loaded you can begin to build up an analysis. You should use a separate chunk for each step and write some text between each chunk that explains what you are doing. Code from the course documents and crib sheets will form the basis for most of your analyses. It is very important to run all the code in the right order. Code chunks often depend on actions that are taken previously. For example in fig 2.6 the animated gif shows that two code chunks have been added after the data were loaded. The first produces a new variable which is the log transformed body weight. The second inspects the relationship between mean time spent sleeping and the log transformed variable. If the code were not run in order the last chunk would not work. The downward facing button on a code chunk runs all the chunks above it. You should use this frequently in order to check that everything is in the right order. Figure 2.6: Building up an analysis in a markdown document. Notice how the run all chunks above button is used to ensure that all the necessary code is being run 2.11 Compiling a report Once you have written all the code needed for your analysis and tested it by stepping through each chunk in the correct order you can compile your report into a document. This whole book has been written and compiled in this way. The idea of using markdown is to ensure that all the code to produce an analysis is reproducible and that the results of the analysis are annotated with comments that explain them both as a reminder to yourself and potentially as a report read by others. Figure 2.7: Knitting the markdown into an HTML document "],
["reading-data-into-r-from-a-web-site.html", "Chapter 3 Reading data into R from a web site 3.1 Introduction 3.2 Example. Reading data from the met office historical data site 3.3 Reading the raw file 3.4 Reading the data to a data frame 3.5 Adding names 3.6 Cleaning the columns 3.7 Making a date column 3.8 Looking at the raw data 3.9 Plotting the data 3.10 Long to wide conversion 3.11 Disadvantages of the wide format 3.12 Using dplyr to summarise 3.13 Repeating the operation 3.14 Get lattitude and Longitude", " Chapter 3 Reading data into R from a web site 3.1 Introduction Even research projects which aim to collect original field data frequently need to incorpoporate analyses of subsidiary data sets. It is common practice to download files and manipulate the data in Excel. This is acceptable. If you are familiar with Excel it can be convenient. However there are several drawbacks to using Excel The data manipulation is not clearly documented. If data have to be “cleaned” in Excel it is easy to lose track of the steps taken to achieve this. Using Excel can lead to multiple files and confusion regarding which is suitable for direct analysis. If the process needs to be repeated many times it can get very tedious and time consuming. So, learning to useR to prepare data for analysis rather than Excel can eventually save a lot library(DT) library(readr) library(ggplot2) library(dplyr) library(lubridate) 3.2 Example. Reading data from the met office historical data site Historical climate data for the UK is currently provided by the met office on this site. https://www.metoffice.gov.uk/public/weather/climate-historic/#?tab=climateHistoric If data is provided online in the form of a text table it is quite often possible to read it straight into R without downloading any files. Here is an example. We will load the data for the nearest station with a complete record directly. 3.3 Reading the raw file If we read the file as lines we can see the structure, but we can not yet work directly with the data as lines. d&lt;-read_lines(&quot;https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/hurndata.txt&quot;) head(d,10) ## [1] &quot;Hurn&quot; ## [2] &quot;Location 411700E 97800N, Lat 50.779 Lon -1.835, 10 metres amsl&quot; ## [3] &quot;Estimated data is marked with a * after the value.&quot; ## [4] &quot;Missing data (more than 2 days missing in month) is marked by ---.&quot; ## [5] &quot;Sunshine data taken from an automatic Kipp &amp; Zonen sensor marked with a #, otherwise sunshine data taken from a Campbell Stokes recorder.&quot; ## [6] &quot; yyyy mm tmax tmin af rain sun&quot; ## [7] &quot; degC degC days mm hours&quot; ## [8] &quot; 1957 1 9.1 2.5 7 73.0 ---&quot; ## [9] &quot; 1957 2 9.6 2.7 8 100.5 ---&quot; ## [10] &quot; 1957 3 12.9 5.5 4 61.3 ---&quot; 3.4 Reading the data to a data frame The header of the file has some useful information. The first 5 lines are text. Then there is information for the column headers. However the info is split over two lines and line 7 has some gaps. The file is separated by tabs, not commas. We can see that because the text lines up with uniform gaps. So we choose the R command read_table to load the data, skipping the first 7 lines. Notice that the header also states that some additional symbols are used to mark missing data and estimated values. This quite often happens, but it can lead to unexpected and frustrating results. If a column being read into R contains any non numerical value at all then R will convert the values into a factor. We can check this by using “str” d&lt;-read.table(&quot;https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/hurndata.txt&quot;,skip=7) str(d) ## &#39;data.frame&#39;: 757 obs. of 7 variables: ## $ V1: int 1957 1957 1957 1957 1957 1957 1957 1957 1957 1957 ... ## $ V2: int 1 2 3 4 5 6 7 8 9 10 ... ## $ V3: Factor w/ 197 levels &quot;0.4&quot;,&quot;10.0&quot;,&quot;10.1&quot;,..: 188 193 31 44 62 125 119 109 79 60 ... ## $ V4: Factor w/ 157 levels &quot;-0.1&quot;,&quot;-0.2&quot;,..: 83 85 114 103 114 146 70 58 154 124 ... ## $ V5: Factor w/ 27 levels &quot;0&quot;,&quot;1&quot;,&quot;10&quot;,&quot;10*&quot;,..: 25 26 22 14 2 1 1 1 2 14 ... ## $ V6: Factor w/ 559 levels &quot;0.4&quot;,&quot;0.5&quot;,&quot;0.9&quot;,..: 452 11 404 341 310 221 469 453 466 388 ... ## $ V7: Factor w/ 546 levels &quot;---&quot;,&quot;101.0&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 3.5 Adding names As we skipped the column headers we need to add them back. names(d)&lt;- c(&quot;Year&quot;, &quot;Month&quot;, &quot;tmax&quot;, &quot;tmin&quot;, &quot;af&quot;,&quot;rain&quot;, &quot;sun&quot;) ## Add the headers 3.6 Cleaning the columns If you are used to using Excel or Word you will be familiar with search and replace functions. Replacing characters in R is slightly more difficult, but much more powerful. The function to find and replace in a vector is gsub. Gsub works just like find and replace but can also use regular expressions. These are rule based expressions that can be written for specific purposes such as replacing all non-numeric characters. Here is a useful little function that uses a regular expression to strip out all non numeric characters. Notice that the function also coerces factors first to a characters then to numerical values. ### A function to strip out any non numerical characters apart from the - and . clean&lt;-function(x)as.numeric(gsub(&quot;[^0-9.-]&quot;,&quot;&quot;,as.character(x))) Now we can clean up the columns that should only contain numbers. d$tmin&lt;-clean(d$tmin) d$tmax&lt;-clean(d$tmax) d$af&lt;-clean(d$af) d$rain&lt;-clean(d$rain) d$sun&lt;-clean(d$sun) ## Warning in clean(d$sun): NAs introduced by coercion write.csv(d,&quot;/home/aqm/data/hurn.csv&quot;) ## Save locally on the server in case the web site goes offline str(d) ## &#39;data.frame&#39;: 757 obs. of 7 variables: ## $ Year : int 1957 1957 1957 1957 1957 1957 1957 1957 1957 1957 ... ## $ Month: int 1 2 3 4 5 6 7 8 9 10 ... ## $ tmax : num 9.1 9.6 12.9 14.2 16.1 22.2 21.6 20.6 17.8 15.9 ... ## $ tmin : num 2.5 2.7 5.5 4.4 5.5 8.8 12.9 11.7 9.6 6.6 ... ## $ af : num 7 8 4 2 1 0 0 0 1 2 ... ## $ rain : num 73 100.5 61.3 5.5 43.7 ... ## $ sun : num NA NA NA NA NA NA NA NA NA NA ... 3.7 Making a date column If data has a temporal component it should be turned into date format for use in R. There are many advantages to this, as d$Month&lt;- as.factor(d$Month) ## Change the numeric variable to a factor d$date&lt;-as.Date( paste(d$Year,d$Month , 15 , sep = &quot;/&quot; ) , format = &quot;%Y/%m/%d&quot; ) The lubridate package provides many useful tricks for handling dates. For example, we may want to use names to the months. d$month&lt;-lubridate:::month(d$date,label=TRUE) 3.8 Looking at the raw data The dt function is a wrapper to the datatable function in the package DT. It is very useful for providing the data you are analysing in a downloadable format for others to use. When a document is knitted in RStudio the data itself is embedded in the document. If you are struggling to find a way to manipulate the data in R you can download it into a spreadsheet, change the data there, and start an analysis afresh with the new data. This is inefficient, but when you are starting out in R it may save time if you are used to spreadheets. dt(d) 3.9 Plotting the data ggplot(d,aes(x=date,y=tmin)) + geom_line() +geom_smooth(col=&quot;red&quot;) 3.10 Long to wide conversion The data provided by the met office is in the correct “long” dataframe format. This makes data handling in R much simpler. However spreadsheet users often analyse data in a wide format. It is fairly simple to convert between the two data structures in R using the reshape2 package. The wide format for tmin looks like this library(reshape2) d %&gt;% melt(id=c(&quot;Year&quot;,&quot;month&quot;),m=&quot;tmin&quot;) %&gt;% dcast(Year~month) %&gt;% dt() 3.11 Disadvantages of the wide format If the raw data had been provided in this wide format we would have had to import multiple tables, one for each variable. If data is held in spreadheets in a wide format it may be necessary to “reverse engineer” the data on each sheet to put it back into the correct data frame format. This can make data preparation a more lengthy process than it needs to be. So always hold your own data in the correct, long format, even if from time to time you may want to convert it to a wide format to share with others who do not use R and are used to calculating means and sums using Excel. 3.12 Using dplyr to summarise The dplyr logic for summarising data is incredibly powerful once you get used to it. Modern R data manipulation makes a lot of use of the %&gt;% operator. This is known as a pipe operator. What it does is to feed the data through a series of steps. The commonest use of dplyr involves grouping the dataframe then summarising it to produce a new data frame. To get yearly summaries we first group_by year then summarise the variables using functions. library(dplyr) d %&gt;% group_by(Year) %&gt;% summarise(tmin=round(mean(tmin),2),tmax=round(mean(tmax),2),rain=sum(rain)) -&gt; yrly dt(yrly) 3.13 Repeating the operation We can roll up all the data loading and cleaing steps above as a function which returns the data for the station as it is named on the web site. When the data on the site is updated to include recent months all that is required to update the data and all subsequent analysis based on it is to rerun the code. I noticed that some stations have more lines that need to be skipped in the header (e.g Southampton). f&lt;-function(nm,skip=7) { URL&lt;-sprintf(&quot;https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/%sdata.txt&quot;,nm) d&lt;-read.table(URL,skip=skip, skipNul = TRUE, fill=TRUE,flush=TRUE) names(d)&lt;- c(&quot;Year&quot;, &quot;Month&quot;, &quot;tmax&quot;, &quot;tmin&quot;, &quot;af&quot;,&quot;rain&quot;, &quot;sun&quot;) d$Year&lt;-clean(d$Year) d$tmin&lt;-clean(d$tmin) d$tmax&lt;-clean(d$tmax) d$af&lt;-clean(d$af) d$rain&lt;-clean(d$rain) d$sun&lt;-clean(d$sun) d$date&lt;-as.Date( paste(d$Year,d$Month , 15 , sep = &quot;/&quot; ) , format = &quot;%Y/%m/%d&quot; ) d$month&lt;-lubridate:::month(d$date,label=TRUE) d$station&lt;-nm d } ### I load each station by name as the raw data on the web site needs eyeballing first due to slight differences in format. d1&lt;-f(&quot;hurn&quot;) d2&lt;-f(&quot;yeovilton&quot;) d3&lt;-f(&quot;eastbourne&quot;) d4&lt;-f(&quot;southampton&quot;,skip=8) d5&lt;-f(&quot;camborne&quot;) d6&lt;-f(&quot;heathrow&quot;) d7&lt;-f(&quot;chivenor&quot;) d8&lt;-f(&quot;oxford&quot;) d9&lt;-f(&quot;eastbourne&quot;) ## Add a contrast d10&lt;-f(&quot;braemar&quot;,skip=8) d&lt;-rbind(d1,d2,d3,d4,d5,d6,d7,d8,d9,d10) write.csv(d,&quot;/home/aqm/data/met_office.csv&quot;) str(d) ## &#39;data.frame&#39;: 9501 obs. of 10 variables: ## $ Year : num 1957 1957 1957 1957 1957 ... ## $ Month : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ tmax : num 9.1 9.6 12.9 14.2 16.1 22.2 21.6 20.6 17.8 15.9 ... ## $ tmin : num 2.5 2.7 5.5 4.4 5.5 8.8 12.9 11.7 9.6 6.6 ... ## $ af : num 7 8 4 2 1 0 0 0 1 2 ... ## $ rain : num 73 100.5 61.3 5.5 43.7 ... ## $ sun : num NA NA NA NA NA NA NA NA NA NA ... ## $ date : Date, format: &quot;1957-01-15&quot; &quot;1957-02-15&quot; ... ## $ month : Ord.factor w/ 12 levels &quot;Jan&quot;&lt;&quot;Feb&quot;&lt;&quot;Mar&quot;&lt;..: 1 2 3 4 5 6 7 8 9 10 ... ## $ station: chr &quot;hurn&quot; &quot;hurn&quot; &quot;hurn&quot; &quot;hurn&quot; ... dt(d) write.csv(d,&quot;~/rstudio/aqm/inst/extdata/met_office.csv&quot;) 3.14 Get lattitude and Longitude This is a bit tricky but can be done. Don’t try to follow any of this, just use the results. library(stringr) f2&lt;-function(nm=&quot;hurn&quot;,line=2) { URL&lt;-sprintf(&quot;https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/%sdata.txt&quot;,nm) d&lt;-read_lines(URL) Lat&lt;-clean(str_match(d[line], &quot;Lat\\\\s*[+-]?([0-9]*[.])?[0-9]+&quot;)[,1]) Lon&lt;-clean(str_match(d[line], &quot;Lon\\\\s*[+-]?([0-9]*[.])?[0-9]+&quot;)[,1]) data.frame(station=nm,Lat=Lat,Lon=Lon) } d1&lt;-f2(&quot;hurn&quot;) d1&lt;-rbind(d1,f2(&quot;yeovilton&quot;)) d1&lt;-rbind(d1,f2(&quot;eastbourne&quot;)) d1&lt;-rbind(d1,f2(&quot;southampton&quot;,line=3)) d1&lt;-rbind(d1,f2(&quot;camborne&quot;)) d1&lt;-rbind(d1,f2(&quot;heathrow&quot;)) d1&lt;-rbind(d1,f2(&quot;chivenor&quot;)) d1&lt;-rbind(d1,f2(&quot;oxford&quot;)) d1&lt;-rbind(d1,f2(&quot;eastbourne&quot;)) d1&lt;-rbind(d1,f2(&quot;braemar&quot;,line=3)) d1 ## station Lat Lon ## 1 hurn 50.779 -1.835 ## 2 yeovilton 51.006 -2.641 ## 3 eastbourne 50.762 0.285 ## 4 southampton 50.898 -1.408 ## 5 camborne 50.218 -5.327 ## 6 heathrow 51.479 -0.449 ## 7 chivenor 51.089 -4.147 ## 8 oxford 51.761 -1.262 ## 9 eastbourne 50.762 0.285 ## 10 braemar 57.006 -3.396 write.csv(d1,&quot;~/rstudio/aqm/inst/extdata/met_office_coords.csv&quot;) library(mapview) library(sf) d1&lt;-st_as_sf(d1, coords = c(&quot;Lon&quot;, &quot;Lat&quot;), crs = 4326) mapview(d1) "],
["using-dplyr.html", "Chapter 4 Using dplyr 4.1 Loading the saved data 4.2 Using dplyr 4.3 Plotting 4.4 Another example 4.5 The %in% filter 4.6 Aggregating years to decades 4.7 Using dygraphs 4.8 Exercise", " Chapter 4 Using dplyr 4.1 Loading the saved data data(&quot;met_office&quot;) d&lt;-met_office 4.2 Using dplyr If you look at older tutorials on R you will find many ways of manipulating data using functions such as apply, tapply, lapply and subset. These methods are still valid and useful However these have mainly been superseded by dplyr. The dplyr approach is preferable as it leads to simpler code that is easier to read and explain. There are three very frequently used functions in dplyr. These are .. filter group_by summarise They are often linked together using the %&gt;% operator. Take a look at this chain and try to follow the logic. d %&gt;% filter(!is.na(tmin)) %&gt;% group_by(station,Year) %&gt;% summarise(n=n(),tmin=round(mean(tmin),2),tmax=round(mean(tmax),2),rain=sum(rain)) %&gt;% filter(n==12) -&gt; yrly What is going on? Well the first step is to remove any rows with no data for tmin. filter(!is.na(tmin)) The next step is to group the data up to the level which will be used for a summary group_by(station,Year). Notice that by grouping by station and Year we are going to use all the stations separately. Remember that the full data has a column labled month. So by dropping month from the grouping operation we will summarise by year. (What would we get if we used this group_by(Year)? Or this group_by(station,Month)?) The next step is to calculate the summarised table. summarise(n=n(),tmin=round(mean(tmin),2),tmax=round(mean(tmax),2),rain=sum(rain)) Things to notice here is that the n() summary gives the number of observations in the group. It is always worth including this, even if it is only used a check of your logic (if n were to be greater than 12 clearly something would be going wrong in this case). The next steps calculate the means. Notice that they are rounded to two decimal places. Rainfall is best summarised as the total monthly rainfall. Finally notice that the last operator is not %&gt;%. It is -&gt;. This assigns the result to a new data frame called yrly. We can then look at this. dt(yrly) 4.3 Plotting Filters can be used at any point in an analysis. If we pipe the data into a plot using %&gt;% we can preapply some filters to use parts of the data. For example, say we want to look at all the staions annual rainfall since 2000, excludig the Scottish station. yrly %&gt;% filter(Year&gt;=2000) %&gt;% filter(station != &quot;braemar&quot;) %&gt;% ggplot(aes(x=Year,y=rain,col=station)) + geom_line() +geom_point() 4.4 Another example To look at the monthly rainfall as boxplots for Hurn. d %&gt;% filter(station ==&quot;hurn&quot;) %&gt;% ggplot(aes(x=month,y=rain)) +geom_boxplot() 4.5 The %in% filter A trick that is well wrth learning is the use of the %in% operator to filter data. This can solve problems that are quite tricky to resolve in any other way. Say, for example, we want to look at only the winter months. The definition of “winter” may be specific to a study. It could mean the period during which a bird species is over wintering. The month column is a factor with named levels levels(d$month) ## [1] &quot;Jan&quot; &quot;Feb&quot; &quot;Mar&quot; &quot;Apr&quot; &quot;May&quot; &quot;Jun&quot; &quot;Jul&quot; &quot;Aug&quot; &quot;Sep&quot; &quot;Oct&quot; &quot;Nov&quot; &quot;Dec&quot; Let’s define winter as the months of January February, November and December. winter&lt;-levels(d$month)[c(1,2,11,12)] Now to filter out just the winter months we can simply use d %&gt;% filter(month %in% winter) %&gt;% dt() 4.6 Aggregating years to decades The cut function can be very useful when we want to cut a continuous variable into discrete sections. Say for example we wanted to find the rainiest decades. We can design a filter that only uses only the complete decades. The rain total is summed over ten years and divided by 10. decades&lt;-seq(1800,2020,10) d$decades&lt;-cut(d$Year,decades,dig.lab=4) d %&gt;% group_by(station,decades) %&gt;% filter(!is.na(rain)) %&gt;% summarise(n=n(),sum(rain)/10) %&gt;% filter(n==120) %&gt;% dt() ## Warning: Factor `decades` contains implicit NA, consider using ## `forcats::fct_explicit_na` 4.7 Using dygraphs The dygraphs package provide library(dygraphs) library(xts) d %&gt;% filter(station==&quot;oxford&quot;) -&gt; ox oxtmax&lt;-xts(x = ox$tmax, order.by = ox$date) oxtmax %&gt;% dygraph(group = &quot;temp&quot;) %&gt;% dyRangeSelector(dateWindow = c(&quot;1900-01-01&quot;, &quot;2020-12-30&quot;)) %&gt;% dyRoller(rollPeriod = 12*10) 4.8 Exercise Design an analysis to answer the following question. Do the spring temperatures in the south of England provide evidence of climate change? Think carefully about how you will summarise the data. Use dplyr to produce tables and ggplot to visualise the results. "],
["r-programming.html", "Chapter 5 R programming 5.1 Using base R to simulate data 5.2 A simple simulated data set 5.3 Simulating a regression 5.4 Exercises", " Chapter 5 R programming During the course of this unit you will develop the skills to be able to apply R code to the analysis of a range of data sets. Although crib sheets will be provided with model code for all the analytical techniques shown on the course, in order to manipulate data effectively and apply novel techniques it is important to begin to become more comfortable with basic R programming techniques and concepts. This week we will revise some of the fundamentals of R programming. Note that dplyr and the “tidyverse” set of functions now make programming large data sets much more agile and elegant than would be the case if we only used the original base R language. However many of the functions that you will find in packages are written using base R. In order to understand other people’s code you need some familiarity with the standard R language. 5.1 Using base R to simulate data If you understand data structures you can usually choose and run the right analysis. If you do not consider the basic properties of your data then not only will you not know which analysis to use, but you will probably not even be able to import the data into any software in order to run it! 5.1.1 Some very simple R commands If you are still completely new to R try running some simple commands by stepping through them in code chunks. When working in RStudio notice that the results are shown under the code chunk when you click on the green arrrow. If you are comfortable with basic R then skip this section. 1+1 ## [1] 2 Experiment some more. For example .. 5*10 ## [1] 50 12/4 ## [1] 3 3^2 ## [1] 9 There is a lot more maths functionality built into the R language that you can find out about as you go on. However to follow the primer you do not need to learn any more commands than are shown in this document. Note that when you type maths in the console the output is just printed to the screen. This is not stored anywhere in the computer’s memory. So we need to bring in the concept of data objects. A data object “holds” the numbers and can be manipulated by commands. This is how R manages to run analyses on your data. When you import a file containing data it will be stored into a data object. The simplest data object is a variable. If a variable only contains one number it is known formally as a scalar. A variable with many numbers is a vector. So let’s assign a single value to a variable to form a scalar. We do that in R using the &lt;- operator which is typed as a combination of the less than sign &lt; with a horizontal bar -. x&lt;-5 Nothing appears to have happened! However you now have your first data object, a scalar called x. This is stored in memory. So try this. x*2 ## [1] 10 Notice that this would not have worked if you had typed X*2 as R is case sensitive. So, how do we form a vector with multiple values? When you are analysing your own data the answer is that you usually won’t need to. You import all the values from a file. But if you wish to form a vector in the console you must use the concatenation operator “c”. So this gives the variable x a series of values. x&lt;-c(1,4,5,9,10,11,12,34,56,67,100,123,45) Now see what happens if you type x*2 ## [1] 2 8 10 18 20 22 24 68 112 134 200 246 90 You can carry out any sort of mathematical operation that involves x and all the values in the vector will be used. Notice that the results are just printed out to the console and lost. If you want to assign the results to a new variable you use the “&lt;-” operator again. This is a very common practice when analysing data. So, say you are intending to work with the natural logarithm of x you might write. logx&lt;-log(x) You can see that this has worked by writing the new variable name so that R prints out the contents to the console. logx ## [1] 0.000000 1.386294 1.609438 2.197225 2.302585 2.397895 2.484907 3.526361 ## [9] 4.025352 4.204693 4.605170 4.812184 3.806662 This time you can see more clearly the purpose of the indices in the output. The second line starts with the 12th number in the vector. You can find the names of the data objects that are held in memory by typing ls(). In Rstudio it is easier to look in the Environment tab in order to see the objects in working memory. ls() ## [1] &quot;logx&quot; &quot;x&quot; 5.1.2 Data structures Now we can begin looking at data structures. You can ask R to tell you about the structure of any object that it has in memory by typing str(). str(x) ## num [1:13] 1 4 5 9 10 11 12 34 56 67 ... str(logx) ## num [1:13] 0 1.39 1.61 2.2 2.3 ... So R has told us that both x and logx are numerical vectors. If you have a lot of data you probably do not want to look at all of it at once. How does this relate to choosing an analysis? We have seen that this particular variable contains numbers. However in statistical analysis we also use “variables”&quot; that don’t consist of numbers. They vary in another respect. If you look at any introductory statistical textbook you will see a chapter that defines types of variables in terms of interval, ordinal and scale variables and nominal and ordinal variables. 5.1.2.1 More vector functions As we have seen functions act on the whole vector at once. This applies also to functions that return only one number as a result. x&lt;-c(1,3,5,7,4,2,7) sum(x) ## [1] 29 mean(x) ## [1] 4.142857 length(x) ## [1] 7 var(x) ## [1] 5.47619 sd(x) ## [1] 2.340126 It is very important to realise that a vector with NA values can cause problems for these sorts of functions. You need to tell R explicitly to remove the NAs. If not the result itself will be an NA. For example. x&lt;-c(1,2,3,4,5,6) mean(x) ## [1] 3.5 x&lt;-c(1,2,3,4,5,NA) mean(x) ## [1] NA mean(x,na.rm=T) ## [1] 3 This is a very common pitfall for beginners. In general, if something does not work as you expect look for NAs! 5.1.3 Generating sequences of numbers in R One of the most useful features of R is the ease with which you can generate sequences of numbers and simulated data sets. To produce a sequence you can use the : syntax. x&lt;-0:100 0:100 ## [1] 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [19] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## [37] 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 ## [55] 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 ## [73] 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 ## [91] 90 91 92 93 94 95 96 97 98 99 100 x&lt;-30:10 x ## [1] 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 A longer but more flexible way of producing a sequence is with seq. For example to produce even numbers between 0 and 100. x&lt;-seq(0,100,by=2) x ## [1] 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 ## [20] 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 ## [39] 76 78 80 82 84 86 88 90 92 94 96 98 100 Say we know the length of the sequence we want but have not worked out the intervals. x&lt;-seq(0,100,length=23) x ## [1] 0.000000 4.545455 9.090909 13.636364 18.181818 22.727273 ## [7] 27.272727 31.818182 36.363636 40.909091 45.454545 50.000000 ## [13] 54.545455 59.090909 63.636364 68.181818 72.727273 77.272727 ## [19] 81.818182 86.363636 90.909091 95.454545 100.000000 5.1.3.1 Using rep to replicate a vector If we want ten copies of the same vector one after the other we can use. x&lt;-rep(c(1,4,9,23),times=10) x ## [1] 1 4 9 23 1 4 9 23 1 4 9 23 1 4 9 23 1 4 9 23 1 4 9 23 1 ## [26] 4 9 23 1 4 9 23 1 4 9 23 1 4 9 23 However we might want each number in the vector replicated ten times before moving to the next. In this case we use each instead of times. x&lt;-rep(c(1,4,9,23),each=10) x ## [1] 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 4 9 9 9 9 9 ## [26] 9 9 9 9 9 23 23 23 23 23 23 23 23 23 23 5.1.4 Logical vectors and subsetting One of the keys to using base R efficiently is the concept of logical vectors, indices and subsetting. However the concept does take a little effort to get used to. Let’s take it a step at a time. Say we have a vector x which we have setup like this. x&lt;-seq(-4,10,by=2) x&lt;-rep(x,times=3) x ## [1] -4 -2 0 2 4 6 8 10 -4 -2 0 2 4 6 8 10 -4 -2 0 2 4 6 8 10 Now we can ask a question that will be answered as true or false for each of the numbers. Is the element of x greater than zero? x&gt;0 ## [1] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE ## [13] TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE R replies with a vector stating whether the result is true or false. We can also ask which of the numbers is greater than zero. which(x&gt;0) ## [1] 4 5 6 7 8 12 13 14 15 16 20 21 22 23 24 Now R replies with the indices of the elements of the vector. Subsetting the vector involves the use of the square brackets {[} and {]}. If you include either a logical vector with TRUE and FALSE values or numeric indices within the square brackets R will subset the vector. Either way works. x[x&gt;0] ## [1] 2 4 6 8 10 2 4 6 8 10 2 4 6 8 10 x[which(x&gt;0)] ## [1] 2 4 6 8 10 2 4 6 8 10 2 4 6 8 10 When we move on to handling more than one variable at a time using data frames we will see that the same concept can be used to subset whole blocks of data.It is a very powerful and fundamentally simple way of manipulating data. A more complex example is given here. This takes every second member of x using a sequence of indices as a subset. x[seq(2,length(x),by=2)] ## [1] -2 2 6 10 -2 2 6 10 -2 2 6 10 Could you follow how this worked? Working outwards length(x) gives the number of elements in x. Lets call this n seq(0,length(x),by=2) gives the vector of indices 2,4,6,….n. A subset of x is found using the square brackets. With experience it is common to wrap up several steps in a single line. There is nothing wrong with explicitly writing n&lt;-length(x) ind&lt;-seq(2,n,by=2) x[ind] ## [1] -2 2 6 10 -2 2 6 10 -2 2 6 10 5.1.4.1 Sorting and ordering We can also sort vectors. There are two ways of doing this. The first is very direct, but I really do not recommend it. It uses the sort command with the argument decreasing=TRUE or FALSE sort(x,decreasing=T) ## [1] 10 10 10 8 8 8 6 6 6 4 4 4 2 2 2 0 0 0 -2 -2 -2 -4 -4 -4 That’s simple enough. However it is much better practice in the long run to use order. This needs a little explaining, again we will take it step by step. order(x,decreasing=T) ## [1] 8 16 24 7 15 23 6 14 22 5 13 21 4 12 20 3 11 19 2 10 18 1 9 17 Order has not given us the numbers in order! But of course it should not, as sort does that. Instead order has given us the the indices in order. Notice that if there are ties, as in this case, R respects the original order for the tied numbers. So how can we use order to sort the vector? If you have followed the logic of using indices to refer to elements of a vector you might have guessed. x[order(x,decreasing=T)] ## [1] 10 10 10 8 8 8 6 6 6 4 4 4 2 2 2 0 0 0 -2 -2 -2 -4 -4 -4 Although this involves more typing than simply writing sort, it is more powerful. The power comes from the way indices can be used with many variables at once. When we move on to see data frames this will be clearer. For the moment look at this simple example to see the logic. x&lt;-c(1,3,2,4) y&lt;-c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;) y[order(x,decreasing=F)] ## [1] &quot;A&quot; &quot;C&quot; &quot;B&quot; &quot;D&quot; 5.1.4.2 Ranks Finding ranks is a very common procedure in statistics. This is a slightly different problem. We need a way to deal with ties, and there is more than one solution to the problem. x&lt;-c(1,1,2,3,4,5,5) rank(x,ties=&quot;average&quot;) ## [1] 1.5 1.5 3.0 4.0 5.0 6.5 6.5 rank(x,ties=&quot;max&quot;) ## [1] 2 2 3 4 5 7 7 rank(x,ties=&quot;min&quot;) ## [1] 1 1 3 4 5 6 6 The last example coincides with the familiar ranks given in sports (joint gold medal followed by the bronze). Notice that there is no decreasing argument to ranks. The lowest numbers take the lowest ranks. If you really want to rank performance scores you must reverse them first by, say, subtracting all the scores from the maximum possible. x&lt;-c(1,1,2,3,4,5,5) rank(max(x)-x,ties=&quot;min&quot;) ## [1] 6 6 5 4 3 1 1 5.1.4.3 Replicating text When designing a format to hold the results from a planned experiment or field survey it can be very useful to generate replicated sequences of text for the factor levels or grouping variables. This is also very easy. x&lt;-rep(c(&quot;Control&quot;,&quot;Treatment1&quot;,&quot;Treatment2&quot;),each=10) x ## [1] &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; ## [6] &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; &quot;Control&quot; ## [11] &quot;Treatment1&quot; &quot;Treatment1&quot; &quot;Treatment1&quot; &quot;Treatment1&quot; &quot;Treatment1&quot; ## [16] &quot;Treatment1&quot; &quot;Treatment1&quot; &quot;Treatment1&quot; &quot;Treatment1&quot; &quot;Treatment1&quot; ## [21] &quot;Treatment2&quot; &quot;Treatment2&quot; &quot;Treatment2&quot; &quot;Treatment2&quot; &quot;Treatment2&quot; ## [26] &quot;Treatment2&quot; &quot;Treatment2&quot; &quot;Treatment2&quot; &quot;Treatment2&quot; &quot;Treatment2&quot; Or of course x&lt;-rep(c(&quot;Control&quot;,&quot;Treatment1&quot;,&quot;Treatment2&quot;),times=10) x ## [1] &quot;Control&quot; &quot;Treatment1&quot; &quot;Treatment2&quot; &quot;Control&quot; &quot;Treatment1&quot; ## [6] &quot;Treatment2&quot; &quot;Control&quot; &quot;Treatment1&quot; &quot;Treatment2&quot; &quot;Control&quot; ## [11] &quot;Treatment1&quot; &quot;Treatment2&quot; &quot;Control&quot; &quot;Treatment1&quot; &quot;Treatment2&quot; ## [16] &quot;Control&quot; &quot;Treatment1&quot; &quot;Treatment2&quot; &quot;Control&quot; &quot;Treatment1&quot; ## [21] &quot;Treatment2&quot; &quot;Control&quot; &quot;Treatment1&quot; &quot;Treatment2&quot; &quot;Control&quot; ## [26] &quot;Treatment1&quot; &quot;Treatment2&quot; &quot;Control&quot; &quot;Treatment1&quot; &quot;Treatment2&quot; 5.1.5 Simulating from known distributions R can simulate data from a very wide range of statistical distributions. For example, to obtain data from a normal distribution with known mean and standard deviation x&lt;-rnorm(100,mean=100,sd=5) The base R histogram is a quick way of visualising this. hist(x) Count data can be simulated using a poisson distribution x&lt;-rpois(100,lambda=2) hist(x) Right skewed data can be simulated using a log normal distribution. x&lt;-rlnorm(10000,2,0.5) hist(x) Bounded data can be simulated using a beta distribution hist(rbeta(100,5, 5)) 5.2 A simple simulated data set Let’s simulate a simple measurement based survey. Say we have decided to look at the difference between heights of male and female students at BU. Our simple survey will be based on a random sample of ten male and ten female students. Let’s make up some data that will have properties that are similar to the empirical data that we might obtain. We will first set up a categorical variable. gender&lt;-rep(c(&quot;Male&quot;,&quot;Female&quot;),each=10) gender ## [1] &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ## [9] &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ## [17] &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; If we ask R about the data structure it will tell us that we have a character vector. str(gender) ## chr [1:20] &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... In statistics categorical variables are referred to as factors. Factors are special character vectors with numbered levels. R automatically assumes that any column in a data file that contains non numeric values is a factor and converts the data to that form. In this case we need to tell R to turn the character vector into a factor. gender&lt;-as.factor(gender) str(gender) ## Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 2 ... Now R tells us that gender is a factor with two levels. There is no intrinsic order to the levels. R places them in alphabetical order by default. The important element to be aware of is that gender is a variable. It varies in a known and specific way (it has two levels), but it is a variable all the same. You cannot calculate means, medians, standard deviations or any similar summary statistics using a factor. However you can, and will, use them together with numerical variables in many types of analysis. Let’s produce a numerical variable to go alongside this. In the UK the mean height of men is around 176 cm and women 164 cm. So we could produce a vector with these values using this code. What we need to do is to replicate the “expected value” for each gender to be placed alongside the factor level. height&lt;-rep(c(176,164),each=10) height ## [1] 176 176 176 176 176 176 176 176 176 176 164 164 164 164 164 164 164 164 164 ## [20] 164 str(height) ## num [1:20] 176 176 176 176 176 176 176 176 176 176 ... So now we have another numerical variable. However if we really carried out a survey of people’s heights we would be absolutely amazed if we got data like this. Although the numbers represent an estimate of the expected value for each of the ten men and women we know from experience that people’s heights vary around this value. In fact they vary quite a lot. The standard deviation is around 6cm. So we need to add some variability. This demonstrates clearly how statistical procedures work. Data consists of two components. It has an underlying structure that we typically want to find out about. In this case there is a difference in heights which is related to gender. There is also random variability, somtimes called the stochastic component. We are usually less interested in this directly, but we need to be very careful to make justifiable assumptions about this variability in order to correctly analyse the data. Knowing this we can now make our variable more realistic by adding in some simulated values taken from a normal distribution with this standard deviation. set.seed(1) height&lt;-height+rnorm(20,sd=6) height ## [1] 172.2413 177.1019 170.9862 185.5717 177.9770 171.0772 178.9246 180.4299 ## [9] 179.4547 174.1677 173.0707 166.3391 160.2726 150.7118 170.7496 163.7304 ## [17] 163.9029 169.6630 168.9273 167.5634 We may want to round the values to one decimal place to make them equivalent to the sort of measurements we might make in practice. height&lt;-round(height,1) Now we have two variables that are held in R’s memory. We are assuming that they both form part of a simulated data set that we could have obtained if we had measured a stratified random sample of twenty students consisting of ten men and ten women. Let’s call the survey “hsurvey”&quot; and make what is known as a data frame to hold the results. hsurvey&lt;-data.frame(gender,height) str(hsurvey) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ gender: Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ height: num 172 177 171 186 178 ... So we have now made up data that has the same structure and similar properties to the results we would get from carrying out a simple survey of people’s heights. The basic concepts used to do this can be extended to more complex situations and used to help the design of experiments. A data frame is the basis of almost all statistical analysis. It consists of two or more columns that line up in such a way that the measurements or observations recorded have been taken from the same individual member of a sample. This is often equivalent to a single sheet in a spreadsheet. R tells us that we have one factor and one numerical variable in this case. We might have measured many other variables at the same time and produced a much wider set of data. These may have been either categorical or numerical. Providing that they all “line up”&quot; and correspond to the same individual we have our data frame. Many standard statistical techniques use two variables together. Later on in the course you will see how we can analyse data consisting of more than two variables using multivariate analysis. The complete data frame is shown below. library(dplyr) library(aqm) dt(hsurvey) If you want to remove all the other variables you set up from memory and leave just the data frame you could type. remove(x,logx,height,gender) ls() ## [1] &quot;hsurvey&quot; &quot;ind&quot; &quot;n&quot; &quot;y&quot; Now if we ask R to print height or gender it will not find them. They have been placed within the data frame. height ## Error in eval(expr, envir, enclos): object &#39;height&#39; not found gender ## Error in eval(expr, envir, enclos): object &#39;gender&#39; not found We can refer to the variables by writing out the full name of the data frame and the variable we want, separated by “$” hsurvey$height ## [1] 172.2 177.1 171.0 185.6 178.0 171.1 178.9 180.4 179.5 174.2 173.1 166.3 ## [13] 160.3 150.7 170.7 163.7 163.9 169.7 168.9 167.6 hsurvey$gender ## [1] Male Male Male Male Male Male Male Male Male Male ## [11] Female Female Female Female Female Female Female Female Female Female ## Levels: Female Male 5.2.1 Saving and loading data frames Now that we have made up some data we might want to save it in the format that we will eventually use to capture our real data. The simplest, most portable data format is a CSV (Comma Separated Variable) file. Such a file can be easily read by all software. First we must find a place to store the data. The standard practice is to make a separate data directory (folder) for each analysis that you carry out. You then set this as the working directory using the menu options in the R console. Once you have set your working directory (which could be on a USB stick for portability) you can save the data that is held in memory using an R command. write.csv(hsurvey,file=&quot;hsurvey.csv&quot;,row.names=FALSE) The command has three elements. The first is the name of the data frame that you wish to save. This is not quoted. The second is the name of the file into which you are going to save the data. The third tells R not to add row names to the file (these are not usually necessary). Data frames are the standard input to all statistical analysis and are of a consistent form. Many different sorts of data summaries and tables can be generated from a data frame quickly by statistical software. You can check the working directory using this command. getwd() ## [1] &quot;/home/rstudio/webpages/books/AQM_book&quot; To see a list of files in the directory type dir() dir() ##[1] hsurvey.csv If we remove the dataframe we formed we will be left with nothing in R’s memory remove(hsurvey) ls() The easy way to remove all files from memory is to use the environment tab in RStudio. To load the data back from the file type hsurvey&lt;-read.csv(&quot;hsurvey.csv&quot;) str(hsurvey) 5.2.2 Histogram of simulated data library(ggplot2) g0&lt;-ggplot(hsurvey,aes(x=height)) g0+geom_histogram(fill=&quot;grey&quot;,colour=&quot;black&quot;,binwidth=3)+facet_wrap(~gender,nrow=2) So we have simulated some data, saved it, and reloaded the data into R. We have then looked at it as a histogram. 5.2.3 Boxplot using ggplot bplot&lt;-ggplot(hsurvey,aes(x=gender,y=height)) bplot +geom_boxplot() 5.2.4 Confidence interval plot bplot + stat_summary(fun.y=mean,geom=&quot;point&quot;) + stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) 5.2.5 Summarising using dplyr hsurvey %&gt;% group_by(gender) %&gt;% summarise(n=n(),mean=round(mean(height),2),sd=round(sd(height),2)) %&gt;% dt() 5.2.6 Statistical test The simplest possible statistical test we could use on these data would be an unpaired t-test. t.test(hsurvey$height~hsurvey$gender) ## ## Welch Two Sample t-test ## ## data: hsurvey$height by hsurvey$gender ## t = -4.4996, df = 16.471, p-value = 0.00034 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -16.62611 -5.99389 ## sample estimates: ## mean in group Female mean in group Male ## 165.49 176.80 5.3 Simulating a regression It is very easy to simulate data with the structure of a regression equation. \\(y=a+bx+\\epsilon\\) where \\(\\epsilon=N(o,\\sigma^{2})\\) x&lt;-1:10 a&lt;-5 b&lt;-2 sig&lt;-4 y&lt;-a+b*x+rnorm(length(x),sd=sig) d&lt;-data.frame(x,y) ggplot(d,aes(x=x,y=y))+ geom_point() +geom_smooth(method=&quot;lm&quot;) 5.3.1 Simulating Likert responses I have added a simple function to the aqm package that will simulate some Likert scale data. q1&lt;-aqm::rand_likert(n=100,p=c(1,1,1,2,4)) q2&lt;-aqm::rand_likert(n=100,p=c(4,2,1,1,1)) d&lt;-data.frame(id=1:100,q1,q2) ## Note the wide format ## Make a data frame df&lt;-tidyr::pivot_longer(d, cols=-id,names_to=&quot;question&quot;,values_to = &quot;response&quot;) ## Long format str(d$q1) ## Ord.factor w/ 5 levels &quot;Strongly disagree&quot;&lt;..: 1 5 4 5 4 5 4 3 5 2 ... df %&gt;% group_by(question, response) %&gt;% summarise (n=n()) ## # A tibble: 10 x 3 ## # Groups: question [2] ## question response n ## &lt;chr&gt; &lt;ord&gt; &lt;int&gt; ## 1 q1 Strongly disagree 9 ## 2 q1 Disagree 8 ## 3 q1 Neutral 11 ## 4 q1 Agree 31 ## 5 q1 Strongly agree 41 ## 6 q2 Strongly disagree 44 ## 7 q2 Disagree 22 ## 8 q2 Neutral 10 ## 9 q2 Agree 14 ## 10 q2 Strongly agree 10 5.3.2 Simulating spatially explicit data library(giscourse) library(sf) library(mapview) conn&lt;-connect() arne&lt;-sssi(dist=1000) points&lt;-st_sample(arne,100) mapview(arne) + mapview(points) disconnect() ## [1] TRUE 5.4 Exercises In order to practice putting together these commands in order to simulate some data try these exercises A researcher is interested in looking at the differences in diameters of trees in three different woodland sites in the New Forest. At each site there are several different species. In order to simplify the task we will consider only two types of trees … conifers and broadleaves. We will also simplify the exercise by assuming that the same number of trees (50) are sampled in each woodland. Set up a dataframe with three columns. One column represents the site. The second represents the type of tree (i.e. conifer or broadleaf). The third represents the diameters. So there will be 150 observations (rows) in all. Try to produce data in which there is a difference in mean diameter that is affected by the site from which the measurements are taken and the type of tree being measured. You can assume that the random variation in diameters is normally distributed and that measurements are taken to the nearest cm. Simulate a questionaire in which participants responses differ by gender. "],
["grammar-of-graphics-plots.html", "Chapter 6 Grammar of graphics plots 6.1 Histograms 6.2 Aesthetics 6.3 Default histogram 6.4 Facet wrapping 6.5 Density plots. 6.6 Adding grouping aesthetics 6.7 Boxplots 6.8 Conditioning on two variables 6.9 Confidence interval plots 6.10 Dynamite plots 6.11 Inference on medians 6.12 Scatterplots 6.13 Adding a regression line 6.14 Grouping and conditioning 6.15 Curvilinear relationships 6.16 Generalised linear models 6.17 Binomial data", " Chapter 6 Grammar of graphics plots Grammar of Graphics plots (ggplots) were designed by Hadley Wickam, who also programmed dplyr. The two packages share the same aproach to high level declarative programming. The syntax differs from base R graphics syntax in various ways and can take some time to get used to. However ggplots provides an extremely elegant framework for building really nice looking figures with comparatively few lines of code. Like the older lattice plots, ggplots are quite prescriptive and will make a lot of the decisions for you, although most of the default settings can be changed. A very useful resource is provided by the R cookbook pages. http://www.cookbook-r.com/Graphs/ A detailed tutorial of gggplots is provided in chapter 3 of R for data science https://r4ds.had.co.nz/data-visualisation.html Some additional ideas are available here http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html A flip book approach is taken here. https://evamaerey.github.io/ggplot_flipbook/ggplot_flipbook_xaringan.html#23 6.1 Histograms Typical histograms use only one variable at a time, although they may be “conditioned” by some grouping variable. The aim of a histogram is to show the distribution of the variable clearly. Let’s try ggplot histograms in ggplot2 using some simple data on mussel shell length at six sites. library(ggplot2) library(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-24. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. # devtools::install_github(&quot;dgolicher/aqm&quot;) To install aqm package If not working on the server library(aqm) ## ## Attaching package: &#39;aqm&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## dt data(mussels) d&lt;-mussels str(d) ## &#39;data.frame&#39;: 113 obs. of 3 variables: ## $ Lshell : num 122.1 100.1 100.7 102.3 94.9 ... ## $ BTVolume: int 39 21 23 22 20 22 21 18 21 15 ... ## $ Site : Factor w/ 6 levels &quot;Site_1&quot;,&quot;Site_2&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... 6.2 Aesthetics The first step when building a ggplot is to decide how the data will be mapped onto the elements that make up the plot. The term for this in ggplot speak is “aesthetics”- Personally I find the term rather odd and potentially misleading. I would instinctively assume that aesthetics refers to the colour scheme or other visual aspect of the final plot. In fact the aesthetics are the first thing to decide on, rather than the last. The easiest way to build a plot is by first forming an invisible object which represents the mapping of data onto the page. The way these data are presented can then be changed. The only aesthetics (mappings) that you need to know about for basic usage are x,y, colour, fill and group. The x and y mappings coincide with axes, so are simple enough. Remember that a histogram maps onto the x axis. The y axis shows either frequency or density so is not mapped directly as a variable in the data. So to produce a histogram we need to provide ggplot2 with the name of the variable we are going to use. We do that like this. g0 &lt;- ggplot(data=d,aes(x=Lshell)) 6.3 Default histogram Once the mapping is established producing a histogram, or any other figure, involves deciding on the geometry used to depict the aesthetics. The default is very simple. g0 + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. There are several things to notice here. One is that the default theme has a grey background and looks rather like some of the figures in Excel. For some purposes this can be useful. However you may prefer a more traditional black and white theme. This is easy to change. You are also warned that the default binwidth may not be ideal. My own default settings for a histogram would therefore look more like this. g1&lt;-g0 +geom_histogram(fill=&quot;grey&quot;,colour=&quot;black&quot;,binwidth=10) + theme_bw() g1 This should be self explanatory. The colour refers to the lines. It is usually a good idea to set the binwidth manually anyway. Notice that this time I have assigned the results to another object. We can then work with this to produce conditional histograms. You can set the theme to black and white for all subsequent plots with a command. theme_set(theme_bw()) 6.4 Facet wrapping Conditioning the data on one grouping variable is very simple using a facet_wrap. Facets are the term used in ggplots for the panels in a lattice plot. There are two facet functions. Facet_wrap simply wraps a one dimensional set of panels into what should be a convenient number of columns and rows. You can set the number of columns and rows if the results are not as you want. g_sites&lt;-g1+facet_wrap(~Site) g_sites 6.5 Density plots. Density plots are produced in similar manner to histograms. g1&lt;-g0 +geom_density(fill=&quot;grey&quot;,colour=&quot;black&quot;) g1 g_sites&lt;-g1+facet_wrap(~Site) g_sites 6.6 Adding grouping aesthetics Adding a grouping aesthetic allow subgroups to be plotted on the same figure. g_group&lt;-ggplot(d, aes(Lshell, group=Site)) + geom_density() g_group Colour and fill are also grouping aesthetics. So a nicer way of showing this would be use them instead. g_group&lt;-ggplot(d, aes(Lshell, colour = Site,fill=Site)) + geom_density(alpha = 0.2) g_group 6.7 Boxplots A grouped boxplot uses the grouping variable on the x axis. So we need to change the aesthetic mapping to reflect this. library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_box&lt;-g0 + geom_boxplot(fill=&quot;grey&quot;,colour=&quot;black&quot;)+theme_bw() ggplotly(g_box) You should be able to work out that sets of boxplots could be conditioned on a third variable using faceting. 6.8 Conditioning on two variables Species&lt;-as.factor(rep(c(&quot;Sp_1&quot;,&quot;Sp_2&quot;),each=20)) Site&lt;-rep(c(&quot;Site_1&quot;,&quot;Site_2&quot;),times=20) resp&lt;-rnorm(40,10,4) d2&lt;-data.frame(Species,resp,Site) g_box&lt;-ggplot(d2,aes(y=resp,x=Species))+geom_boxplot(fill=&quot;grey&quot;,colour=&quot;black&quot;) g_box g_box+facet_wrap(~Site) 6.9 Confidence interval plots One important rule that you should try to follow when presenting data and carrying out any statistical test is to show confidence intervals for key parameters. Remember that boxplots show the actual data. Parameters extracted from the data are means when the data are grouped by a factor. When two or more numerical variables are combined the parameters refer to the statistical model, as in the case of regression. Grammar of graphics provides a convenient way of adding statistical summaries to the figures. We can show the position of the mean mussel shell length for each site simply by asking to plot the mean for each y like this. g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_mean&lt;-g0+stat_summary(fun.y=mean,geom=&quot;point&quot;) g_mean This is not very useful. However you can easily add confidence intervals g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) If you want more robust confidence intervals with no assumption of normality of the residulas then you can use bootstrapping. In this case the result should be just about identical, as the errors are approximately normally distributed. g_mean+stat_summary(fun.data=mean_cl_boot,geom=&quot;errorbar&quot;) 6.10 Dynamite plots The traditional “dynamite” plots with a confidence interval over a bar can be formed in the same way. g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_mean&lt;-g0+stat_summary(fun.y=mean,geom=&quot;bar&quot;) g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) Most statisticians prefer that the means are shown as points rather than bars. You may want to look at the discussion on this provided by Ben Bolker. http://emdbolker.wikidot.com/blog:dynamite 6.11 Inference on medians One way to infer differences between medians is to plot boxplots with notches. g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_box&lt;-g0 + geom_boxplot(fill=&quot;grey&quot;,colour=&quot;black&quot;, notch=TRUE)+theme_bw() g_box # Function included in aqm package # median_cl_boot &lt;- function(x, conf = 0.95) { # lconf &lt;- (1 - conf)/2 # uconf &lt;- 1 - lconf # require(boot) # bmedian &lt;- function(x, ind) median(x[ind]) # bt &lt;- boot(x, bmedian, 1000) # bb &lt;- boot.ci(bt, type = &quot;perc&quot;) # data.frame(y = median(x), ymin = quantile(bt$t, lconf), ymax = quantile(bt$t, # uconf)) # } g0+ stat_summary(fun.data = aqm::median_cl_boot, geom = &quot;errorbar&quot;) + stat_summary(fun.y = median, geom = &quot;point&quot;) 6.12 Scatterplots Scatterplots can be built up in a similar manner. We first need to define the aesthetics. In this case there are clearly a and y coordinates that need to be mapped to the names of the variables. g0 &lt;- ggplot(d,aes(x=Lshell,y=BTVolume)) g0+geom_point() 6.13 Adding a regression line It is very easy to add a regression line with confidence intervals to the plot. g0+geom_point()+geom_smooth(method = &quot;lm&quot;, se = TRUE) Although the syntax reads “se=TRUE” this refers to 95% confidence intervals. 6.14 Grouping and conditioning There are various ways of plotting regressions for each site. We could define a colour aesthetic that will automatically group the data and then plot all the lines as one figure. g0 &lt;- ggplot(d,aes(x=Lshell,y=BTVolume,colour=Site)) g1&lt;-g0+geom_point()+geom_smooth(method = &quot;lm&quot;, se = TRUE) g2&lt;-g1+geom_point(aes(col=factor(Site))) g2 This can be split into panels using facet wrapping. g2+facet_wrap(~Site) 6.15 Curvilinear relationships Many models involving two variables can be visualised using ggplot. d&lt;-read.csv(system.file(&quot;extdata&quot;, &quot;marineinverts.csv&quot;, package = &quot;aqm&quot;)) str(d) ## &#39;data.frame&#39;: 45 obs. of 4 variables: ## $ richness: int 0 2 8 13 17 10 10 9 19 8 ... ## $ grain : num 450 370 192 194 197 ... ## $ height : num 2.255 0.865 1.19 -1.336 -1.334 ... ## $ salinity: num 27.1 27.1 29.6 29.4 29.6 29.4 29.4 29.6 29.6 29.6 ... g0&lt;-ggplot(d,aes(x=grain,y=richness)) g1&lt;-g0+geom_point()+geom_smooth(method=&quot;lm&quot;, se=TRUE) g1 g2&lt;-g0+geom_point()+geom_smooth(method=&quot;lm&quot;,formula=y~x+I(x^2), se=TRUE) g2 g3&lt;-g0+geom_point()+geom_smooth(method=&quot;loess&quot;, se=TRUE) g3 You can also use a gam directly. g4&lt;-g0+geom_point()+stat_smooth(method = &quot;gam&quot;, formula = y ~ s(x)) g4 The plots can be arranged on a single page using the multiplot function taken from a cookbook for R. multiplot(g1,g2,g3,g4,cols=2) 6.16 Generalised linear models Ggplots conveniently show the results of prediction from a generalised linear model on the response scale. glm1&lt;-g0+geom_point()+geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;poisson&quot;), se=TRUE) +ggtitle(&quot;Poisson&quot;) glm1 glm2&lt;-g0+geom_point()+geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;quasipoisson&quot;), se=TRUE) + ggtitle(&quot;Quasipoisson&quot;) glm2 library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select glm3&lt;-g0+geom_point()+geom_smooth(method=&quot;glm.nb&quot;, se=TRUE) +ggtitle(&quot;Negative binomial&quot;) glm3 multiplot(glm1,glm2,glm3,cols=2) 6.17 Binomial data The same approach can be taken to binomial data. ragworm&lt;-read.csv(system.file(&quot;extdata&quot;, &quot;ragworm_test3.csv&quot;, package = &quot;aqm&quot;)) str(ragworm) ## &#39;data.frame&#39;: 100 obs. of 2 variables: ## $ presence: int 1 1 0 0 1 1 1 0 1 1 ... ## $ salinity: num 2.55 2.21 3.39 2.96 1.88 ... g0 &lt;- ggplot(ragworm,aes(x=salinity,y=presence)) g1&lt;-g0+geom_point()+stat_smooth(method=&quot;glm&quot;,formula = y~ x, method.args=list(family=&quot;binomial&quot;))+ggtitle(&quot;Linear&quot;) g2&lt;-g0+geom_point()+stat_smooth(method=&quot;glm&quot;,formula = y~ x+I(x^2), method.args=list(family=&quot;binomial&quot;))+ggtitle(&quot;Polynomial&quot;) g3&lt;-g0+geom_point()+stat_smooth(method = &quot;gam&quot;, formula = y ~ s(x), method.args=list(family=&quot;binomial&quot;))+ggtitle(&quot;GAM&quot;) multiplot(g1,g2,g3,cols=2) "],
["introduction-to-statistical-modelling.html", "Chapter 7 Introduction to statistical modelling 7.1 What is a statistical model? 7.2 The general linear model 7.3 Regression 7.4 Using ggplot2 for confidence intervals. 7.5 Exercises 7.6 Some “data wrangling” 7.7 One way to run multiple analyses", " Chapter 7 Introduction to statistical modelling In most cases the relationship between a traditional statistical test and the model is not explicit, although one way anova is based on an underlying statistical model. Statistical modelling can allow you to extract more information from your data. Most contemporary papers in ecology use models of some kind. Even if the nature of the data you collect limits your options, it is very important to learn to fit and interpret statistical models in order to follow the literature. 7.1 What is a statistical model? To some extent statistical modelling is easier than other forms of ecological modelling. Building process based models requires a great deal of understanding of a system. Statistical models are built from the data themselves. Providing you do have some data to work with you can let the combination of data and prebuilt algorithms find a model. However there are many issues that make statistical modelling challenging. A statistical model is a formal mathematical representation of the “sample space”“, in other words the population from which measurements could have been drawn. It has two components. An underlying ``deterministic’’ component, that usually represents a process of interest A stochastic component representing “unexplained” variability So, a statistical model effectively partitions variability into two parts. One part represents some form of potentially interesting relationship between variables. The other part is just “random noise”. Because statistics is all about variability, the “noise” component is actually very important. Variability must be looked at in some detail on order to decide on the right model. Many of the challenges involved in choosing between statistical models involves finding a way to explain the “unexplained” variability. 7.1.1 Uses of models The literature on statistical modelling frequently uses the terms “explanation”&quot; and “prediction”&quot; to describe the way models are used. Although the same model can have both roles, it is worth thinking about the difference between them before fitting and interpreting any models. 7.1.1.1 Prediction Models can be used to predict the values for some variable when we are given information regarding some other variable upon which it depends. An example is a calibration curve used in chemistry. We know that conductivity and salinity are directly related. So if we measure the conductivity of liquids with known salinities we can fit a line. We can then use the resulting model to predict salinity at any point between the two extremes that we used when finding the calibration curve. Notice that we cannot easily extrapolate beyond the range of data we have. We will see how the same concept applies to the models we use in quantiative ecology. 7.1.1.2 Explanation The idea that the variability in the data can be “explained” by some variable comes from the terminology that was used when interpretating experimental data. Experiments usually involve a manipulation and a control of some description. If values for some response differ between control and intervention then it is reasonable to assume that the difference is “explained” by the intervention. If you wish to obtain data that are simple to analyse and interpret you should always design an experiment. However, experiments can be costly. Ecological systems are often slow to respond to interventions, making experiments impossible within the time scale of a master’s dissertation. We are often interested in systems that cannot be easily modified anyway on ethical or practical grounds. Thus in ecology we often have to interpret associations between variables as evidence of process that “explain” a relationship. Correlation is not causation. Although patterns of association can provide insight into causality they are not enough to establish it. So, when you read the words “variance explained” look carefully at the sort of data that are being analysed. In an ecological setting this may only suggest close association between variables, not a true explanation in the everyday sense of the word. 7.2 The general linear model General linear models lie behind a large number of techniques. These have different names depending on the type of data used to explain or predict the variability in a numerical response variable. Regression (Numerical variable) Analysis of variance (One or more categorical variables) Analysis of covariance (Categorical variable plus numerical variable) Multiple regression (Multiple numerical variables) Although these analyses are given different names and they appear in different parts of the menu in a program such as SPSS, they are all based on a similar mathematical approach to model fitting. In R the same model syntax is used in all cases. The steps needed to build any linear model are… Look at the data carefully without fitting any model. Try different ways of plotting the data. Look for patterns in the data that suggest that they are suitable for modelling. Fit a model: The standard R syntax for a simple linear regression model is mod&lt;-lm(y~x) However model formulae may be much more complex. Look at whether the terms that have been entered in the model are significant: The simplest R syntax is Again, this part of the process can be become much more involved in the case of models with many terms. Summarise the model in order to understand the structure of the model. This can be achieved with* summary(mod) Run diagnostics to check that assumptions are adequately met. This involves a range of techniques including statistical tests and graphical diagnoses. This step is extremely important and must be addressed carefully in order to ensure that the results of the exercise are reliable. 7.3 Regression 7.3.1 Theory The regression equation is .. \\(y=a+bx+\\epsilon\\) where \\(\\epsilon=N(o,\\sigma^{2})\\) In other words it is a straight line with a as the intercept, b as the slope, with the assumption of normally distributed errors (variability around the line) The diagram below taken from Zuur et al (2007) illustrates the basic idea. In theory, if we had an infinite number of observations around each point in a fitted model, the variability would form a perfect normal distribution. The shape and width of the normal curves would be constant along the length of the model. These normal curves form the stochastic part of the model. The strait line is the deterministic part. This represents the relationship we are usually most interested in. The line is defines by its intercept with the axis and its slope. knitr::include_graphics(&quot;figs/regression1.png&quot;) For any observed value of y there will be a fitted value (or predicted value) that falls along the line. The difference between the fitted value and the observed value is known as the residual. knitr::include_graphics(&quot;figs/regression.png&quot;) In reality we do not have infinite observed values at each point. However if we collected all the residuals together we should get a normal distribution. 7.3.2 Example The example is taken from Crawley’s R book. It is very simplified and idealised, but serves to explain the concepts. Crawley’s own ecological research involves herbivory. Here he presents an example in which the growth of insect larvae on leaves with different concentrations of tannin has been measured. Tannin concentrations slow insect growth (Crawley unfortunately does not provide the units in which this is measured). The question is, how much is growth inhibited by an increase of 1% in tannin concentration? The first step after loading the data is to produce a scatterplot. larvae&lt;-read.csv(&quot;/home/aqm/data/Examples/larvae.csv&quot;) names(larvae) ## [1] &quot;growth&quot; &quot;tannin&quot; attach(larvae) plot(growth~tannin) The scatterplot is produced using the syntax that we will use in the model. Growth is a function of (~) tannin. We now fit a model and assign the results to an object in R. We can call this “mod”. This contains all the information about the fitted model. mod&lt;-lm(growth~tannin) We can now look at the properties of the model. If we ask for an “anova”. R will produce the following output. anova(mod) ## Analysis of Variance Table ## ## Response: growth ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## tannin 1 88.817 88.817 30.974 0.0008461 *** ## Residuals 7 20.072 2.867 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This is similar to the table produced for an ANOVA involving categegorical variables (factors). It can be interpreted in a similar way. Just as for an ANOVA we have an F ratio that represents the amount of variability that falls along the regression line divided by the residual variation. The numerator degrees of freedom for a simple regression is always one. The numerator degrees of freedom is n-2 as we have estimated two parameters, the slope and the intercept. Thus we have found a very significant effect of tannin concentration on growth F(1, 7) =31, p &lt;0.001. You should report the R² values along with the p-value. A correlation test would have shown the same significant relationship, but without as much detail. cor.test(growth,tannin) ## ## Pearson&#39;s product-moment correlation ## ## data: growth and tannin ## t = -5.5654, df = 7, p-value = 0.0008461 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9796643 -0.5972422 ## sample estimates: ## cor ## -0.9031408 The additional information that we have with a regression concerns the two elements in the regression equation. The slope and the intercept. summary(mod) ## ## Call: ## lm(formula = growth ~ tannin) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4556 -0.8889 -0.2389 0.9778 2.8944 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.7556 1.0408 11.295 9.54e-06 *** ## tannin -1.2167 0.2186 -5.565 0.000846 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.693 on 7 degrees of freedom ## Multiple R-squared: 0.8157, Adjusted R-squared: 0.7893 ## F-statistic: 30.97 on 1 and 7 DF, p-value: 0.0008461 Remember the equation we used. \\(y=a+bx+\\epsilon\\) The intercept (a) is 11.76 The slope (b) is -1.22 Which means that growth (whatever this was measured in by Crawley) is reduced by 1.22 for each increase of 1% in tannin concentration. 7.3.3 Confidence intervals There is an issue with this way of summarising the results. If we only report the coefficients as given we have ignored the fact that there was unexplained variation in the model. This variation produces uncertainty regarding the true values of the parameters. The greater the unexplained variation (scatter or noise) around the line, the less confident we can be regarding their values. The statistical mechanism used in the calculations (that is not particularly complex, but can safely be taken on trust) allows us find confidence intervals for the parameters. If the confidence intervals do not include zero then the parameters are significant. There are only a few cases where this is very meaningful for the intercept. We are usually more interested in the slope. confint(mod) ## 2.5 % 97.5 % ## (Intercept) 9.294457 14.2166544 ## tannin -1.733601 -0.6997325 So now we can go a bit further. Instead of giving a point estimate for the effect of tannin on growth we can state that the 95% confidence interval for the effect of a 1% increase of tannin on growth lies between -1.73 and -0.7 7.3.4 Prediction The equation can be used to predict the most likely value of y given a value of x. If we just ask R to “predict” we get the fitted values for the values of the explanatory variable that we used when fitting the model. predict(mod) ## 1 2 3 4 5 6 7 8 ## 11.755556 10.538889 9.322222 8.105556 6.888889 5.672222 4.455556 3.238889 ## 9 ## 2.022222 So we can plot out the data again and show the predicted values as red points and draw the regression line. plot(growth~tannin) points(tannin,predict(mod),pch=21,bg=2) lines(tannin,predict(mod)) 7.3.4.1 Prediction with confidence intervals As we have seen, a statistical model takes into account uncertainty that arises as a result of variability in the data. So we should not simple look at the line of best fit as summing up a regression. We should add some indication of our confidence in the result to the figure. To achieve this nicely in R requires a couple of extra steps. After plotting the data with plot(growth~tannin) we set up a sequence of 100 x values that lie between the minimum and the maximum. Now if we pass these to the predict function in R and ask for confidence intervals (that by default are 95%) we get the figure below. plot(growth~tannin) x&lt;-seq(min(tannin),max(tannin),length=100) matlines(x,predict(mod,list(tannin=x),interval=&quot;confidence&quot;)) The confidence bands refer to the fitted model. They show uncertainty regarding the regression line and are calculated from the standard error. 7.4 Using ggplot2 for confidence intervals. library(ggplot2) g0&lt;-ggplot(data=larvae, aes(x=tannin,y=growth)) g1&lt;-g0+geom_point() g1+geom_smooth(method=&quot;lm&quot;) 7.4.1 Prediction intervals There is another way to look at uncertainty. If we know the value for tannin, where would we expect a single measured value for growth to lie. Notice that this is different. The confidence bands show where the mean value migh lie if we measured growth many times. But in this case we want to know where we might expect a data point to fall. This is a much broader interval, and is based on the idea of theoretical normal curves falling around the regression line with a standard deviation estimated from the data. We cut off the tails of these curves. plot(growth~tannin) x&lt;-seq(min(tannin),max(tannin),length=1000) matlines(x,predict(mod,list(tannin=x),interval=&quot;prediction&quot;)) There is a demo to ilustrate this here. https://sarid.shinyapps.io/intervals_demo/ ### Diagnostics It is not enough to simply fit a regression line, or any other model. We have to justify our choice of model and convince those who might use it that the assumptions have been adequately met. The question of how close the data are to meeting the assumptions requires model diagnostics. The basic assumptions for regression Normally distributed errors Identically distributed errors over the range (homogeneity of variance) No undue influence of points with high leverage An underlying linear relationship Independent errors In this rather artificial example all the assumptions are met. However real ecological data is rarely as simple as this. In order to justify the use of a regression model you must be able to show that the assumptions are not seriously violated. We will come on to what “seriously”&quot; means later. 7.4.1.1 Normality It is important to remember that the assumption of normality applies to the residuals after a model has been fitted. You do not test the for normality of the variable itself, as the relationship that you are modelling influences the distribution of the variable. You can look at the distribution of the residuals by plotting a histogram. hist(residuals(mod),col=&quot;grey&quot;) This looks good enough. A slightly more sophisticated way of spotting deviations from normality is to use a qqplot. If we ask R to plot a fitted model, we actually get a set of diagnostic plots. There are six of these. By default R will produce four of them. The qqplot is the second in the series. It is used as a visual check of the normality assumption. If the assumption is met the points should fall approximately along the predicted straight line. plot(mod,which=2) Qqplots often have a sigmoid shape. This occurs when some of the extreme values fall further along the tail of the normal distribution than expected. Providing this effect is slight it is not a problem. However deviations from the line away from the extremes does show a major departure from the assumption. Interpretation of QQplots requires some experience, but they can be very useful. If you have doubts about your ability to interpret the plot you could try using a statistical test of normality. shapiro.test(residuals(mod)) ## ## Shapiro-Wilk normality test ## ## data: residuals(mod) ## W = 0.98794, p-value = 0.9926 The problem with this test is that it becomes more sensitive as the number of observations increase. Thus you are more likely to get a p-value below 0.05 if you have a lot of data. However violations of normality become much less serious as the number of observations increase. So it can be safe to ignore a significant test result if the QQplot and histogram do not suggest major problems providing you have more than around 30 observations. If the histogram shows clear skew then you should think about a data transform, weighted regression or using a generalised linear model. More about this later. 7.4.1.2 Homogeneity of variance The assumption here is that the distance a point is likely to fall from the fitted line does not change along the x values. This is very often violated. For example if the values represent counts of individuals it is constrained to not fall below zero and will tend to be some function of the expected value. This results in residuals that follow a poisson distribution or more likely, a negative binomial. The characteristic of both these distributions is that the scatter increases as the expected value increases. The following lines produce some data with this characteristic in order to illustrate the pattern. set.seed(5) library(MASS) x&lt;-sample(0:10,20,rep=T) y&lt;-rnegbin(20,mu=x,theta=2) plot(y~x) mod.negbin&lt;-lm(y~x) You should be able to spot a “fan” effect. The third diagnostic plot in the series is used to see this effect more clearly. This plot shows the square root of the standardised residuals plotted against the fitted values. Because the square root is used all the values are positive. If there is an increasing (or decreasing) trend in the absolute size of the residuals it should show up on the plot. plot(mod.negbin,which=3) The red line is meant to guide the eye, but you should look carefully at the pattern rather than the line itself. This is the pattern for the original model, that does not have a problem with heterogeneity of variance. plot(mod,which=3,main=&quot;Growth model&quot;) If you are having difficulty interpreting the results you could try a test for the significance of the trend using a correlation test between the variables shown on the diagonistic plot. cor.test(sqrt(stdres(mod.negbin)),predict(mod.negbin)) ## Warning in sqrt(stdres(mod.negbin)): NaNs produced ## ## Pearson&#39;s product-moment correlation ## ## data: sqrt(stdres(mod.negbin)) and predict(mod.negbin) ## t = 4.8093, df = 7, p-value = 0.001945 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5071470 0.9737071 ## sample estimates: ## cor ## 0.8761687 cor.test(sqrt(stdres(mod)),predict(mod)) ## Warning in sqrt(stdres(mod)): NaNs produced ## ## Pearson&#39;s product-moment correlation ## ## data: sqrt(stdres(mod)) and predict(mod) ## t = -0.50019, df = 2, p-value = 0.6666 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.9803574 0.9236405 ## sample estimates: ## cor ## -0.3334484 The same sort of caveats apply to the literal interpretation of this test as a decision rule as the normality test. Although most violations of homogeneity of variance have to be taken seriously, large data sets may show statistically significant, but inconsequential violations. 7.4.1.3 Leverage In the artificial example provided by Crawley all the points are equally spaced along the x axis. This is the “classic” form for a regression and prevents problems with leverage. However in ecology we often have to analyse data that does not have this property. Some of the x values on a scatterplot may fall a long way from the centre of the distribution. Such points potentially could have an excessive influence on the form of the fitted model. The influence a point has on the regression is a function of leverage and distance from a line fitted using all the other points. knitr::include_graphics(&quot;figs/leverage.png&quot;) To illustrate this let’s add a point to the data at a high tannin density and assume that zero growth was measured at this value. How much does this new point influence the model? tannin&lt;-c(tannin,15) growth&lt;-c(growth,0) We can plot the new data and show the effect of this one point on the regression line. It is shown in red. new.mod&lt;-lm(growth~tannin) plot(growth~tannin) lines(tannin,predict(mod,list(tannin))) lines(tannin,predict(new.mod,list(tannin)),col=2) This data point has shifted the regression quite a lot due to a combination of its distance from the others (which gives it high leverage) and the fact that it lies a long way from the fitted line (high residual deviation). These two effects are captured by a quantity known as Cook’s distance. The diagnostic plot shows leverage on the x axis and standardised residuals on the y axis. Large residuals with low leverage do not affect the model, so the safe area with low values for Cook’s distance (below 1) forms a funnel shape. Points falling outside this “safe area” can be flagged up. There are no problem points in the original model. plot(mod,which=5) However the point that we added with both a high leverage and a high deviation from the original model shows up clearly when we diagnose the model with this point included. plot(new.mod,which=5) Another way of spotting this influential points is by looking at Cook’s distance directly. Values over 1 are typically considered to be extreme. plot(new.mod,which=4) 7.4.2 Lack of independence The value and sign of the residual deviation should not be related in any way to that of the previous point. If residual values form “clusters” on one side of the line or another it is a sign of lack of independence. There are two causes of this. The most serious is intrinsic temporal or spatial autocorrelation. This is discussed in the next section. A less serious matter is that the shape of the model is not appropriate for the data. We can see this in the last example. While the strait line might have been OK for the range of data originally given by Crawley, a straight line is a poor model when an additional data point is added at a high tannin level. At some point tannin stops all growth, so the underlying model must be asymptotic. Thus the model with the extra point has correlation in the raw residuals. High tannin values have positive residual deviation as a result of the poor fit of the model. This can be seen by using the first diagnostic plot that R produces. plot(new.mod, which=1) A common way of testing for lack of independence is the Durbin Watson test for serial autocorrelation. library(lmtest) ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric dwtest(growth~tannin) ## ## Durbin-Watson test ## ## data: growth ~ tannin ## DW = 2.0168, p-value = 0.3679 ## alternative hypothesis: true autocorrelation is greater than 0 7.4.2.1 Putting it all together If you just ask R to plot a model you get all four plots. With some practice these should help you spot problems that need attention. Notice that to get four plots on one page you use par(mfcol=c(2,2)). par(mfcol=c(2,2)) plot(mod) 7.4.3 Violations of assumptions What do you do if your diagnostic analysis shows that there is a problem with the model? The first thing to realise is that all models are approximations of reality. As the statistician G.E. Box famously stated “All models are wrong … but some are useful.” If journal editors and referees only admitted papers which used the “correct” model ecological publishing would stop in its tracks. The important point to be aware of is that not all violations of the assumptions are equally serious. Almost all of them can be handled by more advanced modelling, but sometimes it is simply enough to point out that a minor violation was noticed, but it was not considered serious enough to prevent further analysis, together with a clearly stated justification. Normality of the residuals: This is considered to be the least important assumption of the model. Slight deviations from normality will not usually affect the conclusions drawn. Furthermore some influential authors, eg Sokal and Rolf, state that the central limit theorem means that the assumption can safely be ignored for large (n&gt;30) samples. However, be careful with this, as the assumption is that the residuals are non-normal, but homogeneous. This is unlikely to occcur. Fixed values of the independent variable: This assumption is in fact nearly always violated in ecological studies. We never measure anything without error. The main point to be aware of is that it is error relative to the range that causes an issue. So, if we measure a variable such as tree height with an accuracy of 10cm and the range of values falls between 5m and 20m there is no problem. However if the range of heights were to be only between 10m and 11m an issue may arise. You should always aim to have a large range of values for the explanatory variable with respect to measurement errors. Homogeneity of variance: Violations of this assumption can be quite serious. However you should be aware that fixing the problem using techniques such as weighted regression will not affect the slope of the regression line. It will affect the p-values (making them larger) and confidence intervals (making them wider). If the p-value from the uncorrected model is very small (p&lt;0.001) then a correction for heterogeneity of variance is unlikely to result in a loss of significance. If the p-value is close to the conventional cut off (p&lt;0.05) then the correction will almost certainly change your conclusions. In some cases the apparent heterogenity is the result of outliers that need removal. Doing this will of course change the slope. Incorrect model form: A regression is a straight line. In many ecological situations the true responses take the form of a curve of some kind. Asymptotes are very commonly predicted both from theory and “common sense”. For example, adding more nitrogen fertilizer beyond a certain point will not produce any more growth. The biggest problem for regression in these circumstances is not that the model does not fit. It may approximate quite well to one part of the curve. The issue is that the functional form is misleading and does not represent the underlying process. Influential outliers: We have seen that an outlier does not necessarily influence the regression unless it also has high leverage. You have to think carefully before simply removing these points. If points at the extreme ends of the x axis have high residual values it may be that the model is badly formed. We have seen the potential for this in the example above. In this case you may want to restrict a linear regression to the linear part of the relationship and remove data at the ends. Flexible models such as GAMs which we will see later can deal with the issue. You may also use a data transformation to pull the extreme values in. It may well turn out that the issue simply is due to mistaken measurements, in which case the extreme values are rejected after checking. There are also other methods to formally handle the problem such as robust regression. Independence of the errors: This is the big issue. Violations of this assumption are always serious. The main problem is that if observations are not independent the model is claiming many more degrees of freedom (replication) than is justified. This means that the model cannot be generalised. The issue affects many, if not all, ecological studies to some extent. Measurements are rarely truly independent from each other in space and time. The issue affects the interpetation of the p-value and confidence intervals of the model. While the model may still be suitable for prediction within its sample space it will not generalise beyond it. In the next class we will look at an example in detail that may help to clarify this. 7.5 Exercises Climate data When analysing time series it can be very important to take into account serial autocorrelation. However if serial autocorrelation is not particularly strong simple regression analysis can provide reliable insight regarding trends. The following data are taken from the UK Meterological office. They represent total precipitation and mean monthly temperatures averaged over climate stations in England. Is there any evidence of trends? Temp&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/Temp.csv&quot;) Prec&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/Prec.csv&quot;) 7.6 Some “data wrangling” There are now many different ways in R to reshape data frames into consistent formats. The new “tidyr” and dplyr are becoming increasingly popular. A simple way of stacking many columns into two (variable and value) is provided by the reshape package. The melt function takes the original data frame and arguments defining “id” variables and “measurement” variables. library(reshape2) Temp2&lt;-melt(Temp[,3:15],id=&quot;Year&quot;) str(Temp2) ## &#39;data.frame&#39;: 1236 obs. of 3 variables: ## $ Year : int 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 ... ## $ variable: Factor w/ 12 levels &quot;JAN&quot;,&quot;FEB&quot;,&quot;MAR&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ value : num 3.2 3.4 3.3 3.9 2.9 3.5 6.9 1.2 3.3 2.5 ... head(Temp2) ## Year variable value ## 1 1910 JAN 3.2 ## 2 1911 JAN 3.4 ## 3 1912 JAN 3.3 ## 4 1913 JAN 3.9 ## 5 1914 JAN 2.9 ## 6 1915 JAN 3.5 All the months can now be plotted on a single figure using ggplot2. g0&lt;-ggplot(Temp2,aes(x=Year,y=value)) g0+geom_point()+geom_smooth(method=&quot;lm&quot;)+facet_wrap(&quot;variable&quot;) 7.7 One way to run multiple analyses Once the data are in long format it is fairly easy to loop through the factor levels and run a separate analysis for each month. Note that the value of the factor is used to subset the data. Then within the loop the same code is run multiple times. To include output in a knitted document you need to explicitly tell R to print the results. for(i in levels(Temp2$variable)) { d&lt;-subset(Temp2,Temp2$variable==i) mod&lt;-lm(data=d,value~Year) print(i) print(anova(mod)) print(summary(mod)) } ## [1] &quot;JAN&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 4.071 4.0706 1.3876 0.2416 ## Residuals 101 296.298 2.9336 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9279 -0.8244 0.1514 1.2828 3.5863 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.497250 11.132331 -0.853 0.396 ## Year 0.006686 0.005676 1.178 0.242 ## ## Residual standard error: 1.713 on 101 degrees of freedom ## Multiple R-squared: 0.01355, Adjusted R-squared: 0.003785 ## F-statistic: 1.388 on 1 and 101 DF, p-value: 0.2416 ## ## [1] &quot;FEB&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 2.62 2.6165 0.7436 0.3906 ## Residuals 101 355.40 3.5188 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7250 -0.9115 0.1946 1.3608 3.1445 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.712280 12.192223 -0.551 0.583 ## Year 0.005361 0.006217 0.862 0.391 ## ## Residual standard error: 1.876 on 101 degrees of freedom ## Multiple R-squared: 0.007308, Adjusted R-squared: -0.00252 ## F-statistic: 0.7436 on 1 and 101 DF, p-value: 0.3906 ## ## [1] &quot;MAR&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 15.482 15.482 7.964 0.005747 ** ## Residuals 101 196.345 1.944 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3082 -0.9820 0.1701 0.9679 3.4048 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.975905 9.062175 -2.204 0.02977 * ## Year 0.013040 0.004621 2.822 0.00575 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.394 on 101 degrees of freedom ## Multiple R-squared: 0.07309, Adjusted R-squared: 0.06391 ## F-statistic: 7.964 on 1 and 101 DF, p-value: 0.005747 ## ## [1] &quot;APR&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 11.266 11.2657 9.023 0.003362 ** ## Residuals 101 126.104 1.2486 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5791 -0.6621 -0.0448 0.6434 3.2429 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.011848 7.262512 -1.929 0.05650 . ## Year 0.011123 0.003703 3.004 0.00336 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.117 on 101 degrees of freedom ## Multiple R-squared: 0.08201, Adjusted R-squared: 0.07292 ## F-statistic: 9.023 on 1 and 101 DF, p-value: 0.003362 ## ## [1] &quot;MAY&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 1.797 1.7970 1.7804 0.1851 ## Residuals 101 101.944 1.0093 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.42442 -0.62451 -0.02886 0.67880 1.89335 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.257157 6.529832 0.346 0.730 ## Year 0.004443 0.003329 1.334 0.185 ## ## Residual standard error: 1.005 on 101 degrees of freedom ## Multiple R-squared: 0.01732, Adjusted R-squared: 0.007593 ## F-statistic: 1.78 on 1 and 101 DF, p-value: 0.1851 ## ## [1] &quot;JUN&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 3.466 3.4664 3.7702 0.05496 . ## Residuals 101 92.860 0.9194 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.50768 -0.57619 0.01828 0.61019 2.46764 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.740239 6.232143 0.279 0.781 ## Year 0.006170 0.003178 1.942 0.055 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9589 on 101 degrees of freedom ## Multiple R-squared: 0.03599, Adjusted R-squared: 0.02644 ## F-statistic: 3.77 on 1 and 101 DF, p-value: 0.05496 ## ## [1] &quot;JUL&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 8.144 8.1436 6.5637 0.01188 * ## Residuals 101 125.311 1.2407 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.25628 -0.83798 -0.07239 0.64977 3.15598 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.827188 7.239644 -0.391 0.6970 ## Year 0.009457 0.003691 2.562 0.0119 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.114 on 101 degrees of freedom ## Multiple R-squared: 0.06102, Adjusted R-squared: 0.05172 ## F-statistic: 6.564 on 1 and 101 DF, p-value: 0.01188 ## ## [1] &quot;AUG&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 11.613 11.6132 9.2353 0.003023 ** ## Residuals 101 127.005 1.2575 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.56001 -0.78660 -0.03558 0.62243 2.98701 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.568975 7.288419 -0.901 0.36958 ## Year 0.011294 0.003716 3.039 0.00302 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.121 on 101 degrees of freedom ## Multiple R-squared: 0.08378, Adjusted R-squared: 0.07471 ## F-statistic: 9.235 on 1 and 101 DF, p-value: 0.003023 ## ## [1] &quot;SEP&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 9.92 9.9203 9.6765 0.002426 ** ## Residuals 101 103.54 1.0252 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5740 -0.6080 0.1034 0.6420 2.7573 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.100940 6.580889 -1.079 0.28315 ## Year 0.010438 0.003355 3.111 0.00243 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.013 on 101 degrees of freedom ## Multiple R-squared: 0.08743, Adjusted R-squared: 0.0784 ## F-statistic: 9.677 on 1 and 101 DF, p-value: 0.002426 ## ## [1] &quot;OCT&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 16.922 16.9225 11.95 0.0008011 *** ## Residuals 101 143.030 1.4161 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.05175 -0.65856 0.02091 0.60267 3.11619 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -16.704930 7.734564 -2.160 0.033156 * ## Year 0.013633 0.003944 3.457 0.000801 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.19 on 101 degrees of freedom ## Multiple R-squared: 0.1058, Adjusted R-squared: 0.09694 ## F-statistic: 11.95 on 1 and 101 DF, p-value: 0.0008011 ## ## [1] &quot;NOV&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 22.704 22.7043 15.115 0.0001812 *** ## Residuals 101 151.714 1.5021 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.96830 -0.60514 0.06336 0.76071 2.77387 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -24.676785 7.965901 -3.098 0.002525 ** ## Year 0.015791 0.004062 3.888 0.000181 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.226 on 101 degrees of freedom ## Multiple R-squared: 0.1302, Adjusted R-squared: 0.1216 ## F-statistic: 15.11 on 1 and 101 DF, p-value: 0.0001812 ## ## [1] &quot;DEC&quot; ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Year 1 0.02 0.01993 0.0084 0.927 ## Residuals 101 238.63 2.36263 ## ## Call: ## lm(formula = value ~ Year, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.7346 -0.9670 0.3108 0.9989 3.2010 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.3941682 9.9903576 0.340 0.735 ## Year 0.0004679 0.0050939 0.092 0.927 ## ## Residual standard error: 1.537 on 101 degrees of freedom ## Multiple R-squared: 8.352e-05, Adjusted R-squared: -0.009817 ## F-statistic: 0.008436 on 1 and 101 DF, p-value: 0.927 This is “quick and dirty” as no diagnostics have been run for each regresson. In order to fully justify any interesting results you would need to do more work than this. In reality a correct analysis of time series data should include an investigation of serial autocorrelation, amongst other elements. We’ll be looking at this later in the course. 2.. Recall that in the primer we looked at the relationship between measurements on mussel shell length and body tissue volume in ml. Load the data and analyse them using linear regression. mussels&lt;-Prec&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/mussels.csv&quot;) attach(mussels) library(ggplot2) theme_set(theme_bw()) library(multcomp) "],
["some-theory-on-the-general-linear-model.html", "Chapter 8 Some theory on the general linear model 8.1 Calculating the sum of squares 8.2 Where does R squared (coefficient of determination) come from? 8.3 Model assumptions 8.4 Some practice using linear models 8.5 One way Anova 8.6 Statistical inference 8.7 Fitting a linear model 8.8 Diagnostics 8.9 Treatment contrasts using summary 8.10 Changing the reference leval 8.11 Multiple comparisons 8.12 Scale location plot 8.13 Example of heterogeniety 8.14 Exercises", " Chapter 8 Some theory on the general linear model Mathematically the equation for a one way anova is equivalent to this too. If you think about a situation in which instead of a single value for x being multipled by a single coefficient we have a matrix of values representing which level of a factor is being taken into account when calculating a value of y we have an equation with a parameter for each factor level. 8.1 Calculating the sum of squares In both cases the formula that is dropped in to the call to fit a linear model in R is similar. The difference is that one way ANOVA uses a factor to predict fitted values and calculate residuals whereas regression uses a numerical variable. In order to demonstrate the fundamental similarity between the two models we will set up some simulated data and then calculate the sum of squares from first principles. 8.1.0.1 ANOVA Let’s make up a predictor variable. This is a factor with three levels, call them A, B and C. Now, let’s assume that the fitted values (in other words the deterministic pattern of response to the three levels of this factor) are mean values of 10, 15 and 20. These are the expected responses if there were no variability apart from that between groups. set.seed(1) predictor&lt;-as.factor(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),each=10)) fitted&lt;-rep(c(10,12,20),each=10) Of course, there is going to be variability within each group. So let’s add this in. We will assume that the variability can be modelled as a normal distribution with mean zero and standard deviation of 5. So the results we actually get are derived by adding these two components together. residuals&lt;-rnorm(30,mean=0,sd=5) results&lt;-fitted+residuals d&lt;-data.frame(predictor,results) boxplot(results~predictor,data=d) Of course, because this was a simulation experiment the actual means will be slightly different to the invested values. We can set up a data frame with true fitted and residual values like this. mod&lt;-lm(results~predictor,data=d) d&lt;-data.frame(predictor,results,fitted=fitted(mod),residuals=residuals(mod)) head(d) ## predictor results fitted residuals ## 1 A 6.867731 10.66101 -3.7932830 ## 2 A 10.918217 10.66101 0.2572027 ## 3 A 5.821857 10.66101 -4.8391570 ## 4 A 17.976404 10.66101 7.3153901 ## 5 A 11.647539 10.66101 0.9865250 ## 6 A 5.897658 10.66101 -4.7633558 8.1.0.2 The numerator sum of squares OK, so where do the numbers in the Anova table come from? The first step is to understand that the numerator sum of squares represents all the deterministic variability in the system. This is the variability attributable to differences between groups. The sum of squares is the sum of the squared deviations around the mean. But, which mean? In this case it is the overall mean value for the respnse. So, look at the boxplot again, but this time remove the variability and just plot the means. We can show the difference for each mean from the grand mean using arrows. boxplot(fitted~predictor,data=d) abline(h=mean(results),lwd=3,col=2) arrows(1,10.66,1,mean(results),code=3) arrows(2,13.24,2,mean(results),code=3) arrows(3,19.33,3,mean(results),code=3) OK, so for each mean there are 10 fitted values. The sum of squares is simply nsqrs&lt;-(d$fitted-mean(results))^2 nsumsqrs&lt;-sum(nsqrs) 8.1.0.3 Denominator sum of squares The denominator sum of squares in an Anova table represents all the variability that can be attributed to the stochastic component of the model. In other words, the residual variablity. We produced that when simulating the data. The values are simply the residuals once the means for each group have been subtracted. g0&lt;-ggplot(d,aes(x=predictor,y=results)) g0+geom_point(pch=21,bg=2)+stat_summary(fun.y=mean,geom=&quot;point&quot;,size=4) So each residual is the distance between the red points and the large black point representing the mean for that group. dsqrs&lt;-d$residuals^2 dsumsqrs&lt;-sum(dsqrs) mod&lt;-lm(results~predictor) anova(mod) ## Analysis of Variance Table ## ## Response: results ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## predictor 2 396.36 198.18 8.9192 0.001062 ** ## Residuals 27 599.93 22.22 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 nsumsqrs ## [1] 396.3639 dsumsqrs ## [1] 599.9315 The degrees of freedom for the munerator sum of squares are the number of free parameters in the model. This is one less than the number of groups because we need to calculate the grand mean from the data, and thus use up degree of freedom. In order to calculate the residuals we need to calculate three means in this case, so the denominator degree of freedom is the total sample size minus three. 8.1.1 Regression The set up for regression is, in effect, identical, apart from the fact that the fitted values are continuous rather than group means. So if we invent some data x&lt;-11:40 y&lt;-10+x*2+rnorm(30,0,10) plot(y~x) mod&lt;-lm(y~x) d&lt;-data.frame(x,y,fitted=fitted(mod),residuals=residuals(mod)) head(d) ## x y fitted residuals ## 1 11 45.58680 32.84861 12.738183 ## 2 12 32.97212 34.88166 -1.909533 ## 3 13 39.87672 36.91470 2.962016 ## 4 14 37.46195 38.94774 -1.485794 ## 5 15 26.22940 40.98079 -14.751383 ## 6 16 37.85005 43.01383 -5.163776 plot(y~x) points(x,fitted(mod),pch=21,bg=2) abline(h=mean(y),lwd=4,col=3) arrows(x,fitted(mod),x,mean(y),code=3) nsqrs&lt;-(d$fitted-mean(y))^2 nsumsqrs&lt;-sum(nsqrs) 8.1.2 Residuals plot(y~x) points(x,fitted(mod),pch=21,bg=2) arrows(x,fitted(mod),x,y,code=2) dsqrs&lt;-d$residuals^2 dsumsqrs&lt;-sum(dsqrs) mod&lt;-lm(y~x) anova(mod) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 9289.5 9289.5 141.99 1.759e-12 *** ## Residuals 28 1831.9 65.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 nsumsqrs ## [1] 9289.517 dsumsqrs ## [1] 1831.902 8.2 Where does R squared (coefficient of determination) come from? The “explained variability” in the data is often reported using R squared. High values suggest that a large proportion of the variability is “explained” by the model, whether the model is a regression or an ANOVA. Where does that fit in? Well the total sum of squares is the sum of all the squared distances that we have calculated. If we divide the sum of squares attributable to the model by the total we get the proportion of the variability attributable to the model. totsumsqrs&lt;-nsumsqrs+dsumsqrs nsumsqrs/totsumsqrs ## [1] 0.8352817 We can see that this is the same as R provides by asking for a summary. summary(mod) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.751 -5.120 -1.579 5.365 18.129 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.4851 4.5945 2.282 0.0303 * ## x 2.0330 0.1706 11.916 1.76e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.089 on 28 degrees of freedom ## Multiple R-squared: 0.8353, Adjusted R-squared: 0.8294 ## F-statistic: 142 on 1 and 28 DF, p-value: 1.759e-12 8.3 Model assumptions Assumptions of any linear model are the same. Normally distributed errors Identically distributed errors over the range (homogeneity of variance) Independent errors In the case of regression we can also state that it is assumed that there is No undue influence of points with high leverage (regression) An underlying linear relationship (regression) If these assumptions are not met then the p-values and R squared calculations based on the sums of squares may potentially be misleading. So testing assumptions critically is an important part of using and interpreting linear models. The best diagnostic tests involve looking carefully at the data rather than running an automated decision tool. Minor violations may be acceptable in some circumstances. 8.4 Some practice using linear models The best way to become comfortable fitting models in R is to fit them several times until the procedure becomes routine. Let’s look at some classic data on the morphological characteristics of iris flowers. This is a useful data set to get to know, as it is very frequently used in the R literature to illustrate a wide range on more advanced techniques, including machine learning. We can load the data set into memory with the data command. For consistency I’ll then call it d. data(&quot;iris&quot;) d&lt;-iris DT:::datatable(d) So there are three (closely related) species with measurements made on petals and sepals. 8.5 One way Anova So, a reminder. The first step in any analysis is to look at the data. So if we want to look at differences in sepal width between species we can plot the data as boxplots. g0&lt;-ggplot(d,aes(x=Species,y=Sepal.Width)) g0 +geom_boxplot() 8.5.1 Diagnostics The boxplots allow basic diagnostic tests for one way anova. Using them we can test for Normally distributed errors Identically distributed errors over the range (homogeneity of variance) Look carefully at the boxplots. Do they look approximately symetrical? Is there a clear pattern of outliers either above or below the box? If the answers are yes and no, then the data may be approximately normally distributed. Look at the width of the boxes for each group. Are they similar? If the answer is yes then heterogenity of variance is not likely to be a major problem. 8.6 Statistical inference The next step is to produce confidence intervals. g0 +stat_summary(fun.y=mean,geom=&quot;point&quot;) + stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) Remember that confidence intervals are a function of sample size. So if there are differences in the number of measurements in each group this may result in some showing narrower confidence intervals even if the underlying variances are similar. 8.7 Fitting a linear model There are two commonly forms of syntax for fitting a one way anova in R. We can use either oav, or lm. The aov syntax is actually just a wrapper to lm. Lm stands for linear model. The syntax is identical to that used for regression, but the second variable in the formula is a factor. mod&lt;-lm(data=d,Sepal.Width~Species) anova(mod) ## Analysis of Variance Table ## ## Response: Sepal.Width ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.345 5.6725 49.16 &lt; 2.2e-16 *** ## Residuals 147 16.962 0.1154 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You should be able to interpret all the elements in the output now. 8.8 Diagnostics A qqplot can be the most useful of the diagnistic plots for anova. This will help in deciding if the residuals are normally distributed. Hetereogeneity of variance is best spotted using the boxplots. plot(mod,which=2) If the points more or less fall along the diagonal this can be taken as a good indication that the assumption of normality is met. Minor deviations from normality are rarely important, and taken overall the assumption of “exact” normality of residuals is not the most important. Minor deviations will not invalidate the model. You should also be very careful before talking about an “invalid” model if there are minor violations. The underlying pattern may be quite clearly shown through almost any reasonable analtsis. Finding the “best” model involves refining the way in which p-values and confidence intervals are calculated to ensure that they are justifiable. If p-values are extremely small, then refinement may be unlikely to change the overall significance of the result. If p-values are close to the traditional cut off point (0.05) then the significance of the result may be questionable, and changes in the way a model is fitted to the data may change the conclusions. 8.9 Treatment contrasts using summary If we ask for a summary of a one way anova the default in R is to provide a table with treatment contrasts. Treatment constrasts take the first level of the factor as the reference level. summary(mod) ## ## Call: ## lm(formula = Sepal.Width ~ Species, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.128 -0.228 0.026 0.226 0.972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.42800 0.04804 71.359 &lt; 2e-16 *** ## Speciesversicolor -0.65800 0.06794 -9.685 &lt; 2e-16 *** ## Speciesvirginica -0.45400 0.06794 -6.683 4.54e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3397 on 147 degrees of freedom ## Multiple R-squared: 0.4008, Adjusted R-squared: 0.3926 ## F-statistic: 49.16 on 2 and 147 DF, p-value: &lt; 2.2e-16 8.10 Changing the reference leval If the anova represented an experiment with some control group, this would be the natural choice as the reference level. We can set alternative reference levels for the summary table. contrasts(d$Species)&lt;-contr.treatment(levels(d$Species),base=3) mod&lt;-lm(data=d,Sepal.Width~Species) summary(mod) ## ## Call: ## lm(formula = Sepal.Width ~ Species, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.128 -0.228 0.026 0.226 0.972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.97400 0.04804 61.908 &lt; 2e-16 *** ## Speciessetosa 0.45400 0.06794 6.683 4.54e-10 *** ## Speciesversicolor -0.20400 0.06794 -3.003 0.00315 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3397 on 147 degrees of freedom ## Multiple R-squared: 0.4008, Adjusted R-squared: 0.3926 ## F-statistic: 49.16 on 2 and 147 DF, p-value: &lt; 2.2e-16 8.11 Multiple comparisons In an observational study we will often want to look at the differences between pairs of groups. The “General Linear Hypothesis” from the multcomp package is useful for this. library(multcomp) #plot(glht(mod, linfct = mcp(Species = &quot;Tukey&quot;))) ## Have to adjust the margins to show this clearly summary(glht(mod, linfct = mcp(Species= &quot;Tukey&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: lm(formula = Sepal.Width ~ Species, data = d) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## versicolor - setosa == 0 -0.65800 0.06794 -9.685 &lt; 1e-04 *** ## virginica - setosa == 0 -0.45400 0.06794 -6.683 &lt; 1e-04 *** ## virginica - versicolor == 0 0.20400 0.06794 3.003 0.00876 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) 8.12 Scale location plot This can be useful for spotting heterogeneity of variance. If heterogeniety of variance is an issue you are likely to see a trend towards larger values on the y axis as the fitted value increases. plot(mod,which=3) 8.13 Example of heterogeniety set.seed(5) library(MASS) x&lt;-sample(0:10,20,rep=T) y&lt;-rnegbin(20,mu=x,theta=2) plot(y~x) mod.negbin&lt;-lm(y~x) plot(mod.negbin,which=3) 8.14 Exercises Try fitting regressions to different variables within the iris data set. The statistician Francis Anscombe invented four data sets in order to demonstrate some of the pitfalls involved in running regression analysis blind (without either looking at the data or carrying out diagnostics).The four data sets are provided in R for illustration. data(anscombe) str(anscombe) ## &#39;data.frame&#39;: 11 obs. of 8 variables: ## $ x1: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x2: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x3: num 10 8 13 9 11 14 6 4 12 7 ... ## $ x4: num 8 8 8 8 8 8 8 19 8 8 ... ## $ y1: num 8.04 6.95 7.58 8.81 8.33 ... ## $ y2: num 9.14 8.14 8.74 8.77 9.26 8.1 6.13 3.1 9.13 7.26 ... ## $ y3: num 7.46 6.77 12.74 7.11 7.81 ... ## $ y4: num 6.58 5.76 7.71 8.84 8.47 7.04 5.25 12.5 5.56 7.91 ... d&lt;-anscombe We can fit models to x1,y1, x2,y2 etc mod1&lt;-lm(data=d,y1~x1) mod2&lt;-lm(data=d,y2~x2) mod3&lt;-lm(data=d,y3~x3) mod4&lt;-lm(data=d,y4~x4) The summaries of the models look very similar. summary(mod1) ## ## Call: ## lm(formula = y1 ~ x1, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 summary(mod2) ## ## Call: ## lm(formula = y2 ~ x2, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9009 -0.7609 0.1291 0.9491 1.2691 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.001 1.125 2.667 0.02576 * ## x2 0.500 0.118 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6662, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002179 summary(mod3) ## ## Call: ## lm(formula = y3 ~ x3, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1586 -0.6146 -0.2303 0.1540 3.2411 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0025 1.1245 2.670 0.02562 * ## x3 0.4997 0.1179 4.239 0.00218 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6292 ## F-statistic: 17.97 on 1 and 9 DF, p-value: 0.002176 summary(mod4) ## ## Call: ## lm(formula = y4 ~ x4, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.751 -0.831 0.000 0.809 1.839 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0017 1.1239 2.671 0.02559 * ## x4 0.4999 0.1178 4.243 0.00216 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.236 on 9 degrees of freedom ## Multiple R-squared: 0.6667, Adjusted R-squared: 0.6297 ## F-statistic: 18 on 1 and 9 DF, p-value: 0.002165 However only one of these data sets is really suitable for linear regression. Run the appropriate diagnostics to find out which. Geek of the week data For fun I have downloaded spotify detials on tracks from three bands. library(spotifyr) id &lt;- &quot;df4b4ced508a4ac39ea5357c3ed2d477&quot; secret&lt;-&quot;48e05067ee3e47a4b9fcedea97ffa5ae&quot; Sys.setenv(SPOTIFY_CLIENT_ID = id) Sys.setenv(SPOTIFY_CLIENT_SECRET = secret) library(spotifyr) d1 &lt;- data.frame(artist=&quot;Oasis&quot;,get_artist_audio_features(&#39;Oasis&#39;)) d2 &lt;- data.frame(artist=&quot;Blur&quot;,get_artist_audio_features(&#39;blur&#39;)) d3 &lt;- data.frame(artist=&quot;Arctic Monkeys&quot;,get_artist_audio_features(&#39;Arctic Monkeys&#39;)) d&lt;-rbind(d1,d2,d3) d$track_name&lt;-gsub(&quot;Remastered&quot;,&quot;&quot;,d$track_name) d$album_name&lt;-gsub(&quot;(Remastered)&quot;,&quot;&quot;,d$album_name) library(dplyr) d&lt;-d[,c(1,3,8,10:21,24)] write.csv(d,&quot;/home/aqm/data/spotify.csv&quot;) These data are rather like the Iris data in general format. d&lt;-read.csv(&quot;/home/aqm/data/spotify.csv&quot;) str(d) ## &#39;data.frame&#39;: 381 obs. of 17 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ artist : Factor w/ 3 levels &quot;Arctic Monkeys&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ album_name : Factor w/ 27 levels &quot;13&quot;,&quot;All The People... Blur Live At Hyde Park 02/07/2009&quot;,..: 5 5 5 5 5 5 5 5 5 5 ... ## $ track_name : Factor w/ 376 levels &quot;(Get Off Your) High Horse Lady&quot;,..: 82 204 190 279 154 310 108 94 33 15 ... ## $ danceability : num 0.312 0.367 0.327 0.214 0.355 0.389 0.399 0.426 0.486 0.366 ... ## $ energy : num 0.886 0.988 0.895 0.828 0.978 0.905 0.729 0.838 0.913 0.831 ... ## $ key : Factor w/ 12 levels &quot;A&quot;,&quot;A#&quot;,&quot;B&quot;,&quot;C&quot;,..: 3 9 4 11 6 11 11 4 1 11 ... ## $ loudness : num -3.24 -1.07 -3.06 -2.66 -2.33 ... ## $ mode : Factor w/ 2 levels &quot;major&quot;,&quot;minor&quot;: 2 1 1 1 1 1 1 1 1 1 ... ## $ speechiness : num 0.0427 0.0706 0.0476 0.0383 0.0596 0.0352 0.0333 0.0362 0.0394 0.0429 ... ## $ acousticness : num 7.32e-05 4.25e-03 1.34e-01 1.42e-02 6.57e-04 1.27e-01 1.26e-02 5.32e-02 1.22e-02 5.72e-03 ... ## $ instrumentalness: num 1.61e-04 7.58e-06 2.06e-03 1.61e-06 4.95e-03 0.00 3.24e-04 3.02e-06 4.98e-01 1.62e-03 ... ## $ liveness : num 0.0885 0.0486 0.385 0.161 0.448 0.382 0.368 0.0829 0.446 0.484 ... ## $ valence : num 0.251 0.327 0.295 0.388 0.19 0.315 0.181 0.182 0.308 0.339 ... ## $ tempo : num 80 135 148 170 137 ... ## $ duration_ms : int 462227 302600 439373 356600 262427 347400 412293 288600 313200 560000 ... ## $ track_popularity: int 0 0 0 0 0 0 0 0 0 0 ... Which artist’s songs have the most energy? Is it sensible to test this statistically? "],
["one-way-anova-1.html", "Chapter 9 One way ANOVA 9.1 Introduction 9.2 Alternative to the one way test 9.3 Power analysis 9.4 Bayesian methods 9.5 The mussels data set 9.6 Treatment contrasts 9.7 Multiple comparisons 9.8 Sum to zero contrasts 9.9 Bayesian credible inference 9.10 Independent variances model 9.11 Random effects model 9.12 Extensions and conclusion 9.13 References", " Chapter 9 One way ANOVA 9.1 Introduction This is a reminder of the material covered previously, placed in context of linear models. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/mussels.csv&quot;) 9.1.1 Multiple comparisons A t-test involves comparisons between two means. You can use R to find out how many combinations of pairs of sites there would be using binomial theory. levels(d$Site) ## [1] &quot;Site_1&quot; &quot;Site_2&quot; &quot;Site_3&quot; &quot;Site_4&quot; &quot;Site_5&quot; &quot;Site_6&quot; There are 6 levels. Choosing combinations of two from six. choose(6,2) ## [1] 15 Running 15 separate t-tests would be quite a lot of work. So in this sort of case we often ask a more general question first. We test whether there is any significant differences between any of the sites. 9.1.2 Visualising between group variation using boxplots A good first step is to look at the variation in the data for each site using boxplots. If we use the plotly library in R we obtain dynamic boxplots. library(ggplot2) library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout theme_set(theme_bw()) g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_box&lt;-g0+ geom_boxplot() ggplotly(g_box) 9.1.3 Boxplot statistics Hovering over the boxplots shows some key statistic for each group. The central line in the boxplot is the median. The box holds the interquartile range (i.e. 50% of the observations fall within it). The whiskers extend out to the what is, informally speaking, either the upper or lower limits to the distribution or the upper or lower limits expected if the data are normally distributed. Thus boxplots are useful as diagnostic tools to identify outliers or skewed distributions. The problem with comparing data using boxplots is that they show all the variability in the data, so there is often a large overlap between the boxes. 9.1.4 Plotting confidence intervals for each group The standard error for the mean and the confidence intervals for the mean that are calculated from it are a function of sample size. So we may want to plot the confidence intervals for the mean in order to spot differences more clearly. g_mean&lt;-g0+stat_summary(fun.y=mean,geom=&quot;point&quot;) g_mean&lt;-g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) g_mean If there is clear space between confidence intervals a significance test will be significant. If confidence intervals overlap slightly the test may, or may not be significant depending on the extent of the overlap. If the overlap is large the tests will never be significant. Combining the two. This looks a bit messy visually, but helps to show the relationship between boxplots and confidence intervals. g_mean&lt;-g_box+stat_summary(fun.y=mean,geom=&quot;point&quot;,col=&quot;red&quot;) g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,col=&quot;red&quot;) 9.1.5 Fitting a model We can test a simple hypothesis using a technique called one way ANOVA. Could the variation in means between sites simply be due to chance? To do that we compare the variability in means to the overall variability in the data. Crawley’s R book provides an in depth explanation of the logic behind analysis of variance in chapter 9. I will not repeat all the details today. Next week we will look at where the sum of squares come from in linear models in the case of both anova and regression. In R to fit the analysis of variance as a model we can write aov(Lshell~Site) and then ask for a summary. mod&lt;-aov(data=d,Lshell~Site) summary(mod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Site 5 5525 1105 6.173 4.58e-05 *** ## Residuals 107 19153 179 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The result suggests that there is significant variation in mean shell length between sites, providing the assumptions of the test hold. 9.1.6 The F ratio and degrees of freedom The key to understanding analysis of variance is to understand the F ratio. The calculations produce a sum of squares and a mean square that are attributed to two sources of variation in the data. For this reason we often talk about partitioning the variance. The Site component is the amount of variation attributable to variations in the mean shell lengths between sites. The residual variation is the amount of variability around these mean values. You can see that the mean square for the Site term in the model is much larger than the mean square for the Residuals term. In fact it is just over 6 times larger. We know that because the table includes the F value, which is the ratio of the two mean squares. The table also contains information regarding the number of groups and the amount of replication. These are the degrees of freedom. The degrees of freedom for Site is n-1. There are six sites so there are five degrees of freedom. The degrees of freedom for the residuals are the total number of measurements minus the number of sites (factor levels). So we have 113-6=107. If you are wondering how this works, the simple explanation is that we subtract the number of mean values that we use in the calculation of the sum of squares. In the case of the site, one overall mean is used (nsites-1). In the case of the residuals the mean of each site is subtracted from the observations for each site (nobs-6). So we can now make a formal statement that we back up with the statistical analysis. We write that There is statistically significant variability in mean shell lengths between sites F(5, 107) = 6.17, p &lt;0.001. 9.1.7 Homogeneity of variance You might have spotted an issue with this test, particularly if you have been reading the text books thoroughly. The traditional analysis of variance assumes homogeneity of variances. The boxplots suggest that there is a lot of variation in between sites in the amount of variation in shell length. A test for homogeneity of variance that is often recommended is Bartlett’s test bartlett.test(d$Lshell~d$Site) ## ## Bartlett test of homogeneity of variances ## ## data: d$Lshell by d$Site ## Bartlett&#39;s K-squared = 18.528, df = 5, p-value = 0.002352 As always, a low p-value suggests that the null hypothesis can be rejected, which in this case is homogeneity of variances. So on technical grounds the test we have just conducted is not quite right. One of the assumptions is not met. This can be a major issue for more complex designs. It is easy to let R make a correction for this when running a one way anova by asking for a oneway test (Welch’s test). oneway.test(d$Lshell~d$Site) ## ## One-way analysis of means (not assuming equal variances) ## ## data: d$Lshell and d$Site ## F = 9.6559, num df = 5.000, denom df = 31.194, p-value = 1.207e-05 The test has now taken into account the issue and it still gives a significant result. Using Welch’s procedure is a useful backup to reinforce and defend your conclusions when the assumption of homogeneity of variance is violated. We will look at this and similar issues in more detail in the course. However you still need to look at the pattern of differences more carefully. 9.2 Alternative to the one way test Another slightly more sophisticated way of handling homogenity of variance in R is to use a procedure called White’s adjustment. library(sandwich) library(car) ## Loading required package: carData mod&lt;-lm(Lshell~Site, data=d) Anova(mod,white.adjust=&#39;hc3&#39;) ## Coefficient covariances computed by hccm() ## Analysis of Deviance Table (Type II tests) ## ## Response: Lshell ## Df F Pr(&gt;F) ## Site 5 9.9682 7.541e-08 *** ## Residuals 107 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The F ratio after making an appropriate correction is actually rather higher. Write down a statement that you could include in a report that expresses this formally It should begin along these lines … “Bartlet’s test showed significant heteroscedasticity in the residuals for all metals. White-corrected covariance matrices were therefore be included in a linear model in order to adjust for lack of homogeneity. Corrected one way Anova showed …” 9.2.1 Determining where the differences lie There is a problem with the simple conclusion drawn from analysis of variance with or without correction. From a scientific perspective is it is hardly an unsurprising discovery. We would have expected some difference between sites, particularly if we look at so many different places. It is much more likely that we are really interested in specific differences between sites. However this raises an additional issue. If we have a hypothesis before we start regarding which site is most likely to be different we could run one single test. However if we are looking at fifteen separate comparisons that is a problem. The conventional significance value is set at 0.05. In other words one chance in twenty of obtaining the data (or more extreme) under the null hypothesis. If we run a lot of tests we increase the chances of at least one being significant even if the null holds. In fact it is quite easy to calculate the probability of getting at least one significant result if we run 15 tests. The easy way is to calculate the probability of all the tests being negative and subtract from 1. 1-0.95^15 ## [1] 0.5367088 So there is a 54% chance of getting at least one significant result at the 0.05 cut off level even if the null is true for every comparison. This is a bit like buying a large number of lottery tickets. If you buy enough you increase your overall chances of winning even though the chances of any single ticket winning remains the same. What is the probability of getting at least one significant result at the 0.05 level if you run twenty tests? Without thinking it is tempting to say that it is one. However this is not the correct answer. Try calculating it. One way of reducing the number of tests is to compare all the means with a control group. This is done in an experiment with treatments, and is the contrasts are thus known as treatment contrasts. We can obtain these in R from the output of aov using summary.lm. summary.lm(mod) ## ## Call: ## lm(formula = Lshell ~ Site, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.906 -8.340 1.031 9.231 30.550 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 100.769 2.624 38.405 &lt; 2e-16 *** ## SiteSite_2 8.467 3.748 2.259 0.0259 * ## SiteSite_3 6.037 5.409 1.116 0.2669 ## SiteSite_4 -3.619 5.409 -0.669 0.5049 ## SiteSite_5 18.697 3.925 4.763 6.02e-06 *** ## SiteSite_6 2.471 3.748 0.659 0.5111 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.38 on 107 degrees of freedom ## Multiple R-squared: 0.2239, Adjusted R-squared: 0.1876 ## F-statistic: 6.173 on 5 and 107 DF, p-value: 4.579e-05 This shows that only site2 and site5 are significantly different from site 1. If you go back to the confidence interval plot and look at the overlap you can see how this pattern emerges. 9.2.2 Bonferoni corrections Treatment contrasts method assumes that there is a planned contrast and there is something unique about site1. Another way of looking at the issue is to effectively make comparisons between all the pairs of sites. In this case we MUST compensate in some way for all the test. There are a lot of ways of doing this. The simplest is called the Bonferoni correction. This just involves changing the critical value by dividing by the number of tests. So if we call a p-value of 0.05 significant, but run 15 tests we would look for values of 0.05/15= 0.0033 before claiming a significant result. 9.2.3 Tukey’s honest significant difference A slightly more subtle method is called Tukey’s Honest Significant Difference (HSD) test. We can run this in R for all pairwise comparisons and plot the results. The confidence intervals and p-values are all adjusted. This produces a lot of output, as you would expect. It effectively runs the fifteen separate t-tests while making allowances for multiple comparisons. The output also includes 95% confidence intervals for the differences between the means. If these confidence intervals include zero then the test will not be significant. The easiest way to see the pattern is to plot the results. mod&lt;-aov(data=d,Lshell~Site) TukeyHSD(mod) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Lshell ~ Site, data = d) ## ## $Site ## diff lwr upr p adj ## Site_2-Site_1 8.466769 -2.408905 19.342443 0.2201442 ## Site_3-Site_1 6.037019 -9.660664 21.734702 0.8737518 ## Site_4-Site_1 -3.619231 -19.316914 12.078452 0.9849444 ## Site_5-Site_1 18.697436 7.305950 30.088922 0.0000867 ## Site_6-Site_1 2.470769 -8.404905 13.346443 0.9859123 ## Site_3-Site_2 -2.429750 -18.201132 13.341632 0.9976925 ## Site_4-Site_2 -12.086000 -27.857382 3.685382 0.2355928 ## Site_5-Site_2 10.230667 -1.262165 21.723498 0.1103764 ## Site_6-Site_2 -5.996000 -16.977781 4.985781 0.6105029 ## Site_4-Site_3 -9.656250 -29.069479 9.756979 0.7004668 ## Site_5-Site_3 12.660417 -3.470986 28.791819 0.2123990 ## Site_6-Site_3 -3.566250 -19.337632 12.205132 0.9862071 ## Site_5-Site_4 22.316667 6.185264 38.448069 0.0015143 ## Site_6-Site_4 6.090000 -9.681382 21.861382 0.8718474 ## Site_6-Site_5 -16.226667 -27.719498 -4.733835 0.0011239 plot(TukeyHSD(mod)) Notice that the comparison that we made between site1 and site2 that was significant using either a single t-test or treatment contrasts now is not shown as significant after the correction has been made for making numerous unplanned comparisons. If you really wanted to claim that the difference was significant you would have to be able to justify that you had thought that site2 ought to be particularly different before obtaining the data, and that the result was not just obtained by so called ``data dredging’’. So, there are some rather subtle aspects of data analysis. Under the more rigorous procedure only site 5 really stands out as being different from the rest. The fact that different ways of looking at the data can apparently lead to different conclusions is one of the aspects of statistics that worries many students (and researchers). Next term we will look at ways of avoiding common statistical pitfalls, together with methods to extract all the important information from a data set in order to fully address your scientific question. 9.2.4 The Kruskal Wallace non parametric test. The Kruskal Wallace test is often used in place of a one way anova when researchers are worried about the assumption of normality and want to use a non-parametric procedure. The disadvantage of a Kruskall Wallace test is that it is not easy to communicate the absolute size of any differences that are detected. The test does not lead to the development of a statistical model. kruskal.test(d$Lshell~d$Site) ## ## Kruskal-Wallis rank sum test ## ## data: d$Lshell by d$Site ## Kruskal-Wallis chi-squared = 27.884, df = 5, p-value = 3.835e-05 The differences refer to the location parameter, which is similar to the median, but not exactly the same measurement. This is difficult to communicate. Another tactic is to bootstrap the median itself. We saw how bootstrapping can be used last week. I have provided a small function to do this that can be used with a ggplot. median_cl_boot &lt;- function(x, conf = 0.95) { lconf &lt;- (1 - conf)/2 uconf &lt;- 1 - lconf require(boot) bmedian &lt;- function(x, ind) median(x[ind]) bt &lt;- boot(x, bmedian, 1000) bb &lt;- boot.ci(bt, type = &quot;perc&quot;) data.frame(y = median(x), ymin = quantile(bt$t, lconf), ymax = quantile(bt$t, uconf)) } g0+stat_summary(fun.data=median_cl_boot,geom=&quot;errorbar&quot;) + stat_summary(fun.y = median, geom = &quot;point&quot;, colour = &quot;red&quot;) ## Loading required package: boot ## ## Attaching package: &#39;boot&#39; ## The following object is masked from &#39;package:car&#39;: ## ## logit How do you interpret this figure? 9.3 Power analysis Power analysis should always be caaried out when designing any study that involves one way analysis of variance. It is very easy to do. The idea behind a power analysis is to “reverse engineer” the analysis of variance. Usually when we carry out analysis of variance we want to show that there is some difference between groups. However if the sample size is too small we might not be able to reject the null hypothesis. The null hypothesis is that the observations could all have been drawn from the same population. Any difference between means is just down to chance variability that arises from variability within the groups. Power analysis asks a “what if” question. If you have a small pilot study you might be able to provide the parameters for the question. Let’s set up a scenario involving four groups. We can either guess some rough estimates of the group means why might expect, find some literature on the subject or carry out a small pilot study to provide some data. groupmeans &lt;- c(120, 130, 140, 150) n&lt;-length(groupmeans) bvar&lt;-var(groupmeans) Notice that we need to calculate the variance between the group means and get the number of groups. We then need to estimate the standard deviation. The best way to do this is through a short pilot study, but we can also make some informed assumptions regarding the value. The within sample variance is just this value squared. sdev&lt;-20 wvar&lt;-sdev^2 So the number of groups is 4. The variance between the the group means is calculated from the four observations and is 166.7. We estimate the standard deviation (think in terms of how far from the mean would you expect 66% of the observations to lie). Now we can set the power to the alpha level we want. If the power is set to 0.9 we would expect to find a significant difference in 90% of the studies, but we might obtain a type 2 error 10% of the time. A type two error is when we fail to reject the null hypothesis when it is in fact false, or in other words not find a significant difference even though it does exist. alpha&lt;-0.9 power.anova.test(groups = n, between.var = bvar, within.var = wvar, power = alpha) ## ## Balanced one-way analysis of variance power calculation ## ## groups = 4 ## n = 12.3635 ## between.var = 166.6667 ## within.var = 400 ## sig.level = 0.05 ## power = 0.9 ## ## NOTE: n is number in each group 9.4 Bayesian methods There is a growing tendency for Bayesian and frequentist methods to be combined for applied data analysis. This may seem odd, as there are intrinsic philosophical differences between the two approaches taken to inference (Ellison 2004). However, in a pragmatic sense, as long as non informative priors are being used, Bayesian methods and frequentist methods produce results that can be effectively identical (Edwards 1996). Without informative priors confidence intervals and credible intervals amount to much the same thing. As Bayesian model fitting through wrappers to MCMC algorithms have become incorporated into R, users of some libraries for fitting mixed effects models may even be unaware that they have applied a Bayesian technique. Writing bespoke models in JAGS or Stan can be difficult and there are pitfalls. Really complex models are best left to specialist statisticians. However there are times when specifying simple models can help to solve routine problems, in combination with other methods. library(rjags) ## Loading required package: coda ## Linked to JAGS 4.2.0 ## Loaded modules: basemod,bugs library(tidyverse) ## ── Attaching packages ────────────────────── tidyverse 1.2.1 ── ## ✔ tibble 2.1.1 ✔ purrr 0.3.2 ## ✔ tidyr 0.8.3.9000 ✔ dplyr 0.8.0.1 ## ✔ readr 1.3.1 ✔ stringr 1.4.0 ## ✔ tibble 2.1.1 ✔ forcats 0.3.0 ## ── Conflicts ───────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks plotly::filter(), stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::recode() masks car::recode() ## ✖ purrr::some() masks car::some() library(rjags) library(ggmcmc) library(polspline) library(propagate) ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## Loading required package: tmvtnorm ## Loading required package: mvtnorm ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## expand ## Loading required package: stats4 ## Loading required package: gmm ## Loading required package: Rcpp ## Loading required package: ff ## Loading required package: bit ## Attaching package bit ## package:bit (c) 2008-2012 Jens Oehlschlaegel (GPL-2) ## creators: bit bitwhich ## coercion: as.logical as.integer as.bit as.bitwhich which ## operator: ! &amp; | xor != == ## querying: print length any all min max range sum summary ## bit access: length&lt;- [ [&lt;- [[ [[&lt;- ## for more help type ?bit ## ## Attaching package: &#39;bit&#39; ## The following object is masked from &#39;package:base&#39;: ## ## xor ## Attaching package ff ## - getOption(&quot;fftempdir&quot;)==&quot;/tmp/RtmpbRvjFD&quot; ## - getOption(&quot;ffextension&quot;)==&quot;ff&quot; ## - getOption(&quot;ffdrop&quot;)==TRUE ## - getOption(&quot;fffinonexit&quot;)==TRUE ## - getOption(&quot;ffpagesize&quot;)==65536 ## - getOption(&quot;ffcaching&quot;)==&quot;mmnoflush&quot; -- consider &quot;ffeachflush&quot; if your system stalls on large writes ## - getOption(&quot;ffbatchbytes&quot;)==16777216 -- consider a different value for tuning your system ## - getOption(&quot;ffmaxbytes&quot;)==536870912 -- consider a different value for tuning your system ## ## Attaching package: &#39;ff&#39; ## The following objects are masked from &#39;package:bit&#39;: ## ## clone, clone.default, clone.list ## The following objects are masked from &#39;package:utils&#39;: ## ## write.csv, write.csv2 ## The following objects are masked from &#39;package:base&#39;: ## ## is.factor, is.ordered ## Loading required package: minpack.lm library(multcomp) ## Loading required package: survival ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:boot&#39;: ## ## aml ## Loading required package: TH.data ## ## Attaching package: &#39;TH.data&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## geyser library(DT) 9.5 The mussels data set I have used a very simple data set for introductory classes on one way Anova. Although finding differences between measurements attributable to a factor such as site of origin does not allow direct causal inference, it is a commonly needed as a screening analysis of observational data by ecologists, It is therefore useful as a demonstration of the concepts. 9.5.1 Heterogeneity of variance and unbalanced sample sizes Note the difference in sample sizes between sites and the differences in sample standard deviations. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/mussels.csv&quot;) d %&gt;% group_by(Site) %&gt;% summarise(n=n(),mean=round(mean(Lshell),1),sd=round(sd(Lshell),1)) %&gt;% DT::datatable() Boxplots of the data also suggest notable heterogeneity in variances between measurements taken from different sites, although the assumption of normality seems to be reasonable. library(ggplot2) theme_set(theme_bw()) g0 &lt;- ggplot(d,aes(x=Site,y=Lshell)) g_box&lt;-g0+ geom_boxplot() g_box 9.5.2 Confidence intervals Conventional confidence intervals based on the standard errors around the group means suggest that there are differences between sites. The conventional advice given to students is that if such confidence intervals do not overlap then an unpaired t-test between the two sites will be significant. If they do overlap, then a test may be needed to establish a significant difference. g_mean&lt;-g0+stat_summary(fun.y=mean,geom=&quot;point&quot;) g_mean&lt;-g_mean+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;) g_mean 9.5.3 Conventional One way ANOVA and multiple comparisons d$Site&lt;-as.factor(as.numeric(d$Site)) ## Change to a site number for factto levels. This is for brevity in glht output mod&lt;-lm(data=d,Lshell~Site) anova(lm(data=d,Lshell~Site)) ## Analysis of Variance Table ## ## Response: Lshell ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Site 5 5525 1105 6.1732 4.579e-05 *** ## Residuals 107 19153 179 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the introductory classes I explain the meaning of treatment contrasts and both the potential utility and the pitfalls involved in pair-wise comparisons between sites, making allowance for multiple comparisons using Tukey’s adjustment. 9.6 Treatment contrasts The default treatment contrasts are more suitable in the context of an experiment with a reference level. summary(mod) ## ## Call: ## lm(formula = Lshell ~ Site, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.906 -8.340 1.031 9.231 30.550 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 100.769 2.624 38.405 &lt; 2e-16 *** ## Site2 8.467 3.748 2.259 0.0259 * ## Site3 6.037 5.409 1.116 0.2669 ## Site4 -3.619 5.409 -0.669 0.5049 ## Site5 18.697 3.925 4.763 6.02e-06 *** ## Site6 2.471 3.748 0.659 0.5111 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.38 on 107 degrees of freedom ## Multiple R-squared: 0.2239, Adjusted R-squared: 0.1876 ## F-statistic: 6.173 on 5 and 107 DF, p-value: 4.579e-05 9.7 Multiple comparisons plot(glht(mod, linfct = mcp(Site = &quot;Tukey&quot;))) summary(glht(mod, linfct = mcp(Site = &quot;Tukey&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: lm(formula = Lshell ~ Site, data = d) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 2 - 1 == 0 8.467 3.748 2.259 0.21191 ## 3 - 1 == 0 6.037 5.409 1.116 0.86857 ## 4 - 1 == 0 -3.619 5.409 -0.669 0.98415 ## 5 - 1 == 0 18.697 3.925 4.763 &lt; 0.001 *** ## 6 - 1 == 0 2.471 3.748 0.659 0.98517 ## 3 - 2 == 0 -2.430 5.435 -0.447 0.99756 ## 4 - 2 == 0 -12.086 5.435 -2.224 0.22728 ## 5 - 2 == 0 10.231 3.960 2.583 0.10534 ## 6 - 2 == 0 -5.996 3.784 -1.584 0.60000 ## 4 - 3 == 0 -9.656 6.690 -1.443 0.69112 ## 5 - 3 == 0 12.660 5.559 2.278 0.20441 ## 6 - 3 == 0 -3.566 5.435 -0.656 0.98548 ## 5 - 4 == 0 22.317 5.559 4.015 0.00139 ** ## 6 - 4 == 0 6.090 5.435 1.121 0.86661 ## 6 - 5 == 0 -16.227 3.960 -4.097 0.00104 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) 9.8 Sum to zero contrasts I would advise students not to take too much notice of individual comparisons unless there is a good underlying motive. Site 5 does seem to be distinctly different to all the other sites.A sum contrast allows the means of each site apart from the last to be compared to the overall mean. contrasts(d$Site) &lt;- &quot;contr.sum&quot; mod&lt;-lm(data=d,Lshell~Site) summary(mod) ## ## Call: ## lm(formula = Lshell ~ Site, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.906 -8.340 1.031 9.231 30.550 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 106.1114 1.4384 73.773 &lt; 2e-16 *** ## Site1 -5.3421 2.5804 -2.070 0.0408 * ## Site2 3.1246 2.6158 1.195 0.2349 ## Site3 0.6949 4.1214 0.169 0.8664 ## Site4 -8.9614 4.1214 -2.174 0.0319 * ## Site5 13.3553 2.7841 4.797 5.24e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.38 on 107 degrees of freedom ## Multiple R-squared: 0.2239, Adjusted R-squared: 0.1876 ## F-statistic: 6.173 on 5 and 107 DF, p-value: 4.579e-05 So that would be more or less job done in class. I would point out that in a real study we would look into the characteristics of site 5 in more detail, but could ignore the other differences. 9.8.1 Heterogenity of variance More astute students who spot the heterogeneity of variance are also shown how to run code for one-way tests and White’s correction. This is not shown here. An alternative, more advanced, approach would be to abandon the ANOVA model completely and run bootstrapped comparisons between sites using resampling from the original data. All this would probably be overkill unless the study really deserved a thorough analysis for a write up. 9.9 Bayesian credible inference Clearly, in a practical sense, all the useful contextual inference that can be extracted from this very small data set has been found. The most important violations of assumptions are not always visible in the numerical data and are much more likely to be attributable to faults in sampling design and lack of independence between measurements. However, if it was important to determine differences between sites it would do no harm to consider other approaches. there might just be some better interpretation. 9.9.1 Pooled variance model using JAGS A conventional pooled variance ANOVA can be written in JAGS very simply. data=list(y=d$Lshell, ind=as.numeric(d$Site), N=length(d$Lshell), p=length(levels(d$Site)), overall_mean=mean(d$Lshell)) pooled_var=&quot; model { ####### Likelihood for (i in 1:N) { # Loop through observations mu[i]&lt;-Beta[ind[i]] # The expected values are just the group means y[i] ~ dnorm(mu[i],tau) # Values treated as from a single normal } ############## Uninformative priors for (j in 1:p) { Beta[j]~dnorm(0,0.0001) Effect[j]&lt;-Beta[j]-overall_mean ### Calculate difference from overall mean ################### Calculate pair wise differences for (n in 1:(j-1)){ Difbeta[n,j]&lt;-Beta[n]-Beta[j] } } tau ~ dgamma(scale, rate) ## Prior for normal distribution precision. scale ~ dunif(0, 1) ### Hyper parameters for tau. rate ~ dunif(0, 1) } &quot; Notice that the pairwise comparisons and a measure of effect size as a difference between the group mean and the overall mean were written into the code. So if we just monitor these nodes we can produce simple caterpillar plots for the parameters of interest. A full analysis would include diagnostics for chain mixing etc, which can make Bayesian analysis more time consuming. model=jags.model(textConnection(pooled_var),data=data) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 113 ## Unobserved stochastic nodes: 9 ## Total graph size: 275 ## ## Initializing model update(model,n.iter=1000) output=coda.samples(model=model,variable.names=c(&quot;Difbeta&quot;,&quot;Effect&quot;),n.iter=100000,thin=10) ms &lt;-ggs(output) mt&lt;-filter(ms,grepl(&quot;Difbeta&quot;,Parameter)) ggs_caterpillar(mt) +geom_vline(xintercept = 0,col=&quot;red&quot;) mt&lt;-filter(ms,grepl(&quot;Effect&quot;,Parameter)) ggs_caterpillar(mt) +geom_vline(xintercept = 0,col=&quot;red&quot;) Notice that the highest posterior differences, which equate closely to 95% confidence intervals in this case, suggest more differences that do not include zero than the Tukey corrected comparisons. One reason for this is that corrections for multiple comparisons are not part of the philosophy taken to formal inference under a Bayesian framework. This could be criticised by a referee! In particular there seems to be a credible difference between sites 2 and 4, but site 4 has a lower sample size. Although we are now using Bayesin inference we have probably not finished. 9.10 Independent variances model As there is clear heterogeneity of variance involved it can easily be taken into account by simply not pooling variances. This does make sense, although it implies that pairwise comparisons are all independent of an overall model. ## The model is similar to the pooled variance model with some modifications noted ind_var=&quot; model { ### Likeihood for (i in 1:N) { ## Loop through observations mu[i]&lt;-Beta[ind[i]] ## Now set an independent tau for each group y[i] ~ dnorm(mu[i],tau[ind[i]]) } for (j in 1:p) { Beta[j]~dnorm(0,0.0001) ## Set up a prior for each grous tau[j] ~ dgamma(scale, rate) Effect[j]&lt;-Beta[j]-overall_mean for (n in 1:(j-1)){ Difbeta[n,j]&lt;-Beta[n]-Beta[j] } } scale ~ dunif(0, 1) rate ~ dunif(0, 1) } &quot; model=jags.model(textConnection(ind_var),data=data) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 113 ## Unobserved stochastic nodes: 14 ## Total graph size: 280 ## ## Initializing model update(model,n.iter=1000) output=coda.samples(model=model,variable.names=c(&quot;Difbeta&quot;,&quot;Effect&quot;),n.iter=100000,thin=10) ms &lt;-ggs(output) mt&lt;-filter(ms,grepl(&quot;Difbeta&quot;,Parameter)) ggs_caterpillar(mt) +geom_vline(xintercept = 0,col=&quot;red&quot;) mt&lt;-filter(ms,grepl(&quot;Effect&quot;,Parameter)) ggs_caterpillar(mt) +geom_vline(xintercept = 0,col=&quot;red&quot;) Notice now that the heterogeneity shown initially by the boxplots and independently calculated confidence intervals is now clearly apparent. 9.11 Random effects model Finally we could try something a little bit more sophisticated. As there are six sites we have just enough replication to fit a simple multi level model. We could suggest that the variability between sites forms a normal distribution. This approach follows (Gelman, Hill, and Yajima 2012). Mutliple comparisons are not needed under a Bayesian approach that includes shrinkage to the mean. rand_mod=&quot; model { ### Likeihood for (i in 1:N) { ## Loop through observations mu[i]&lt;-mu_r+Beta[ind[i]] ## This time Beta is added to an overall mean y[i] ~ dnorm(mu[i],tau[ind[i]]) ## Set an independent tau for each group agan. A pooled variance model would also work here } for (j in 1:p) { Beta[j]~dnorm(0,tau_r) ## A single tau represents the variance between group # means tau[j] ~ dgamma(scale, rate) for (n in 1:(j-1)){ Difbeta[n,j]&lt;-Beta[n]-Beta[j] } } scale ~ dunif(0, 1) rate ~ dunif(0, 1) tau_r ~ dgamma(scale,rate) sigma_r &lt;- 1/sqrt(tau_r) mu_r ~ dnorm(0,0.00001) ## Prior for the overall mean }&quot; model=jags.model(textConnection(rand_mod),data=data) ## Warning in jags.model(textConnection(rand_mod), data = data): Unused ## variable &quot;overall_mean&quot; in data ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 113 ## Unobserved stochastic nodes: 16 ## Total graph size: 288 ## ## Initializing model update(model,n.iter=1000) output=coda.samples(model=model,variable.names=c(&quot;sigma_r&quot;,&quot;mu_r&quot;,&quot;Difbeta&quot;,&quot;Beta&quot;),n.iter=100000,thin=10) ms &lt;-ggs(output) mt&lt;-filter(ms,grepl(&quot;Beta&quot;,Parameter)) ggs_caterpillar(mt) +geom_vline(xintercept = 0,col=&quot;red&quot;) ms &lt;-ggs(output) mt&lt;-filter(ms,grepl(&quot;Difbeta&quot;,Parameter)) ggs_caterpillar(mt) +geom_vline(xintercept = 0,col=&quot;red&quot;) mt&lt;-filter(ms,grepl(&quot;Beta&quot;,Parameter)) ggplot(data=mt,aes(x=mt$value,col=Parameter,fill=Parameter)) + geom_density(alpha=0.2) This is arguably the most appropriate analysis in this particular case. The Bayesian analysis has accounted for the unbalanced sample sizes and provided credible intervals for effect sizes based on the assumption that site means themselves form a normal distribution,. Now, only site 5 stands out as being of any interest at all. 9.12 Extensions and conclusion In a practical sense, adding the Bayesian analysis has not led to much insight here. However it is surprisingly easy to include it. If inferences on effect sizes are contextually important is it worth looking at them from different perspectives in order to test the importance of the assumptions being used. If all analyses lead to exactly the same conclusions then inference is strengthened. In a more advanced setting some additional considerations of random effects and potentially informative prior knowledge can be added to the Bayesian models in a bespoke, contextually appropriate, manner. 9.13 References Long, J. S. and Ervin, L. H. (2000) Using heteroscedasity consistent standard errors in the linear regression model. The American Statistician 54, 217–224. http://www.jstor.org/stable/2685594 White, H. (1980) A heteroskedastic consistent covariance matrix estimator and a direct test of heteroskedasticity. Econometrica 48, 817–838. Fox, J. (2008) Applied Regression Analysis and Generalized Linear Models, Second Edition. Sage. References "],
["fitting-curves-to-data.html", "Chapter 10 Fitting curves to data 10.1 Data exploration 10.2 Polynomials 10.3 Splines 10.4 Complex shapes", " Chapter 10 Fitting curves to data In the previous class we have assumed that the underlying relationship between variables takes the form of a straight line. However in ecology the relationship between variables can have more complex forms. Sometimes the form can be predicted from theory. However we are often simply interested in finding a model that describes the data well and provides insight into processes. 10.1 Data exploration The data we will first look at is analysed by Zuur et al (2007) but here it has been modified slightly in order to illustrate the point more clearly. The data frame we will look at contains only two variables selected from a large number of measurements taken on sediment cores from Dutch beaches. The response variable is the richness of benthic invertebrates. The explanatory variable we are going to look at here is sediment grain size in mm. The data also contains measurements on height from mean sea level and salinity. 10.1.1 Visualisation library(ggplot2) d&lt;-read.csv(&quot;/home/aqm/course/data/marineinverts.csv&quot;) DT::datatable(d) The first step in any analysis should be to look at the data. Base graphics are good enough for a quick figure at this stage. attach(d) plot(richness~grain) Or in ggplot theme_set(theme_bw()) g0&lt;-ggplot(data=d,aes(x=grain,y=richness)) g1&lt;-g0+geom_point() g1 There seems to be a pattern of an intial steep decline in richness as grain size increases, followed by a plateau. Let’s try fitting a linear regression to the data and plotting the results with confidence intervals. Note This is not necessarily the correct analysis for these data. In fact there are many reasons why it is clearly an incorrect way of modelling the relationship. We will explore this as the class progresses, and in a later class we will revisit the data set in order to use a much better model. The exercise at this point is for illustrative, didactic purposes. You should think about the reasons for not using a simple model at every step, in order to understand why more advanced methods are needed. Plotting confidence intervals using base graphics requires a few steps in base graphics, but you can see explicitly that these are constructed using the model predictions. mod&lt;-lm(richness~grain) plot(richness~grain) x&lt;-seq(min(grain),max(grain),length=100) matlines(x,predict(mod,newdata=list(grain=x),interval=&quot;confidence&quot;)) It is, of course, usually much quicker and easier to use ggplots. g2&lt;-g1+geom_smooth(method=&quot;lm&quot;) g2 The relationship is modelled quite well by a straight line, but it does not look completely convincing. There are two many points above the line on the left hand side of the figure and too many below on the right. If look at the first diagnostic plot for the model this is also apparent. The residuals are correlated as a result of the model not being of the correct form. Look at the diagnostic plots. anova(mod) ## Analysis of Variance Table ## ## Response: richness ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## grain 1 385.13 385.13 23.113 1.896e-05 *** ## Residuals 43 716.52 16.66 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod) ## ## Call: ## lm(formula = richness ~ grain) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.8386 -2.0383 -0.3526 2.5768 11.6620 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.669264 2.767726 6.745 3.01e-08 *** ## grain -0.046285 0.009628 -4.808 1.90e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.082 on 43 degrees of freedom ## Multiple R-squared: 0.3496, Adjusted R-squared: 0.3345 ## F-statistic: 23.11 on 1 and 43 DF, p-value: 1.896e-05 par(mfcol=c(2,2)) plot(mod) 10.1.2 T values and significance in summary output An element of the R output that you should be aware of at this point is the summary table. If you look at this table you will see that for each parameter there is an estimate of its value together with a standard error. The confidence interval for the parameter is approximately plus or minus twice the standard error. The T value is a measure of how far from zero the estimated parameter value lies in units of standard error. So generally speaking t values of 2 or more will be statistically significant. This may be useful, but it certainly does not suggest that parameter is useful in itself. To evaluate whether a parameter should be included requires taking a “whole model” approach. Remember that a p-value only tells you how likely you would be to get the value of t (or any other statistic with a known distribution such as an F value) if the null model were true. It doesn’t really tell you that much directly about the model you actually have. 10.1.3 Testing for curvilearity We can check whether a strait line is a good representation of the pattern using the reset test that will have a low p-value if the linear form of the model is not a good fit. library(lmtest) ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric resettest(richness ~ grain) ## ## RESET test ## ## data: richness ~ grain ## RESET = 19.074, df1 = 2, df2 = 41, p-value = 1.393e-06 The Durbin Watson test which helps to confirm serial autocorrelation that may be the result of a misformed model will often also be significant when residuals cluster on one side of the line. dwtest(richness~grain) ## ## Durbin-Watson test ## ## data: richness ~ grain ## DW = 1.7809, p-value = 0.1902 ## alternative hypothesis: true autocorrelation is greater than 0 In this case it was not, but this may be because there were too few data points. 10.2 Polynomials The traditional method for modelling curvilinear relationships when a functional form of the relationship is not assumed is to use polynomials. Adding quadratic, cubic or higher terms to a model gives it flexibility and allows the line to adopt different shapes. The simplest example is a quadratic relationship which takes the form of a parabola. \\(y=a+bx+cx^{2} +\\epsilon\\) where \\(\\epsilon=N(o,\\sigma^{2})\\) x&lt;-1:100 y&lt;-10+2*x-0.02*x^2 plot(y~x,type=&quot;l&quot;) If the quadratic term has a small value the curve may look more like a hyperbola over the data range. x&lt;-1:100 y&lt;-10+2*x-0.01*x^2 plot(y~x,type=&quot;l&quot;) We can add a quadratic term to the model formula in R very easily. The syntax is lm(richness ~ grain+I(grain^2)). The “I” is used to isolate the expression so that it is interpreted literally in mathematical terms. Notice that we are still using lm, i.e. a general linear model. This is because mathematically the terms still enter in a “linear” manner. So linear models can produce curves! We canplot the predictions explicitly using base graphics. mod2&lt;-lm(richness~grain+I(grain^2)) plot(richness~grain) x&lt;-seq(min(grain),max(grain),length=100) matlines(x,predict(mod2,newdata=list(grain=x),interval=&quot;confidence&quot;)) Or using ggplot. g1+geom_smooth(method=&quot;lm&quot;,formula=y~x+I(x^2), se=TRUE) anova(mod2) ## Analysis of Variance Table ## ## Response: richness ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## grain 1 385.13 385.13 29.811 2.365e-06 *** ## I(grain^2) 1 173.93 173.93 13.463 0.00068 *** ## Residuals 42 542.59 12.92 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod2) ## ## Call: ## lm(formula = richness ~ grain + I(grain^2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.5779 -2.5315 0.2172 2.1013 8.3415 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.0133538 9.6721492 5.481 2.21e-06 *** ## grain -0.2921821 0.0675505 -4.325 9.19e-05 *** ## I(grain^2) 0.0004189 0.0001142 3.669 0.00068 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.594 on 42 degrees of freedom ## Multiple R-squared: 0.5075, Adjusted R-squared: 0.484 ## F-statistic: 21.64 on 2 and 42 DF, p-value: 3.476e-07 We now have a higher value of R squared and a better fitting model. But the shape does not look right. The quadratic is constrained in form and has started to rise at the high end of the x axis. This does not make a great deal of sense. We can give the model more flexibility by adding another term. mod3&lt;-lm(richness~grain+I(grain^2)+I(grain^3)) plot(richness~grain) x&lt;-seq(min(grain),max(grain),length=100) matlines(x,predict(mod3,newdata=list(grain=x),interval=&quot;confidence&quot;)) g1+geom_smooth(method=&quot;lm&quot;,formula=y~x+I(x^2)++I(x^3), se=TRUE) anova(mod3) ## Analysis of Variance Table ## ## Response: richness ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## grain 1 385.13 385.13 42.542 7.820e-08 *** ## I(grain^2) 1 173.93 173.93 19.212 7.944e-05 *** ## I(grain^3) 1 171.42 171.42 18.936 8.768e-05 *** ## Residuals 41 371.17 9.05 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod3) ## ## Call: ## lm(formula = richness ~ grain + I(grain^2) + I(grain^3)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.5250 -1.8841 -0.2896 2.2259 7.9431 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.955e+02 3.373e+01 5.796 8.44e-07 *** ## grain -1.784e+00 3.474e-01 -5.134 7.27e-06 *** ## I(grain^2) 5.420e-03 1.153e-03 4.700 2.93e-05 *** ## I(grain^3) -5.386e-06 1.238e-06 -4.352 8.77e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.009 on 41 degrees of freedom ## Multiple R-squared: 0.6631, Adjusted R-squared: 0.6384 ## F-statistic: 26.9 on 3 and 41 DF, p-value: 8.838e-10 This produces a better fit statistically, but things are now getting very confusing. It is not clear what the third term is actually doing. The confidence intervals are quite wide, so we could ignore the sharp downturn, as any shape within the confidence intervals is permissible. But the model still does not look right. The advantages of polynomials is that they do result in a formula that can be written down and provided as a predictive model. The major disadvantage is that the formula is rather complex and has no intrinisic biological or ecological basis. You must also be very careful never to use these models to predict values that fall outside the range used for fitting. Also the formulae produced by fitting polynomials are often used without regard to the confidence intervals. Uncertainty is part of the statistical model and should be taken into account. 10.3 Splines A commonly used alternative to polynomials is to fit a so called smoother of some description. There are many different ways to go about this, making the subject seem complicated. However for most practical purposes they produce similar results and we can rely on the software to make most of the decisions. The most commonly used smoothers are splines of some type. These work by fitting curves to sections of the data and then splicing the results together. This gives the curves much greater flexibility that polynomials. Almost any shape can be fitted. alt text The issue with this involves complexity. If we let the curves become too flexible we could fit a line to that passed through all the data points. But this would not be useful and would leave no degrees of freedom. The degree of waviness is selected in R automatically by cross validation if we use the mgcv package. There is no guarantee that the model will be biologically meaningful, but many times the selection produces a curve that fits the data well and can be interpreted. library(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-24. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. mod4&lt;-gam(richness~s(grain)) summary(mod4) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## richness ~ s(grain) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.6889 0.4601 12.36 2.6e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(grain) 3.615 4.468 15.92 3.25e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.619 Deviance explained = 65.1% ## GCV = 10.616 Scale est. = 9.5269 n = 45 The summary gives a p-value for the term and also shows the estimated degrees of freedom. The more complex the response, the more degrees of freedom are used in fitting the model. Unlike regression models there is no formula associated with the model. So it can be difficult to communicate the results. The usual way of presenting the model is graphically. plot(mod4) Notice that the plot shows differences from the mean value (intercept) associated with the smoothed term in the model. Splines can suffer from some of the same problems as polynomials, but they often lead to a curve that has more intrinsic meaning. g1+stat_smooth(method = &quot;gam&quot;, formula = y ~ s(x)) 10.4 Complex shapes The next data set is on the Gonadosomatic index (GSI, i.e., the weight of the gonads relative to total body weight) of squid Measurements were taken from squid caught at various locations and months in Scottish waters. squid&lt;-read.csv(&quot;/home/aqm/course/data/squid.csv&quot;) We can plot out the data using a conditional box and whisker plot. squid$month&lt;-as.factor(squid$MONTH) g0&lt;-ggplot(data=squid,aes(x=month,y=GSI)) g1&lt;-g0+geom_boxplot() g1+facet_wrap(&quot;Sex&quot;) It seems sensible to split the data by gender. males&lt;-subset(squid,Sex==&quot;Male&quot;) females&lt;-subset(squid,Sex==&quot;Female&quot;) Now we can try representing the pattern of change over the year using a spline model fit using mgcv. mod1&lt;-gam(GSI~s(MONTH),data=females) plot(mod1) summary(mod1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## GSI ~ s(MONTH) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.27244 0.06651 49.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(MONTH) 8.93 8.999 156.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.53 Deviance explained = 53.4% ## GCV = 5.5387 Scale est. = 5.4944 n = 1242 g0&lt;-ggplot(data=females, aes(x=MONTH,y=GSI)) g1&lt;-g0+geom_point() g1+stat_smooth(method = &quot;gam&quot;, formula = y ~ s(x)) The statistics are OK, but the biology seems wrong. The dip in the curve in October does not seem to make sense. Although the number of knots in the spline are determined by cross validation we can lower them in order to produce a simpler model. mod2&lt;-gam(GSI~s(MONTH,k=8),data=females) plot(mod2) summary(mod2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## GSI ~ s(MONTH, k = 8) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.27244 0.06848 47.79 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(MONTH) 6.815 6.987 179.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.502 Deviance explained = 50.5% ## GCV = 5.8608 Scale est. = 5.8239 n = 1242 g1+stat_smooth(method = &quot;gam&quot;, formula = y ~ s(x,k=8)) mod3&lt;-gam(GSI~s(MONTH,k=7),data=females) plot(mod3) summary(mod3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## GSI ~ s(MONTH, k = 7) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.27244 0.06896 47.46 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(MONTH) 5.047 5.655 213.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.495 Deviance explained = 49.7% ## GCV = 5.9348 Scale est. = 5.9059 n = 1242 g1+stat_smooth(method = &quot;gam&quot;, formula = y ~ s(x,k=7)) By reducing the number of knots we have models which use fewer degrees of freedom. However the fit as measured by the deviance explained (R squared in this case) is reduced. We can test whether the first model is significantly better. We find that it is. anova(mod1,mod2,mod3,test=&quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: GSI ~ s(MONTH) ## Model 2: GSI ~ s(MONTH, k = 8) ## Model 3: GSI ~ s(MONTH, k = 7) ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 1232.0 6769.5 ## 2 1234.0 7187.8 -2.0115 -418.25 37.844 &lt; 2.2e-16 *** ## 3 1235.3 7299.4 -1.3317 -111.67 15.263 1.391e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So we are left with a difficulty. The statistical criteria lead to a wavy model, while common sense suggest that there is some issue that has not been taken into account. In such a situation we should look for additional variables that have not been measured. Perhaps some of the squid that were caught actually came from a separate population with a different timing of reproduction. "],
["non-linear-models.html", "Chapter 11 Non-linear models 11.1 Fitting a rectangular hyperbola 11.2 Real data 11.3 Quantile regression 11.4 Summary 11.5 Exercise", " Chapter 11 Non-linear models Statistical modelling involves measures of fit. However scientific modelling often brings in other elements, including theory that is used to propose a model for the data. These theoretical models are often non-linear in the statistical sense. The terms do not enter in a linear manner. Such models can be difficult to fit to real life data, but are often used when building process based ecological models. 11.1 Fitting a rectangular hyperbola For example, resource use is commonly modelled using a function with an asymptote. The equation below is a version of Holling’s disk equation that has been rewriten as a generalised rectangular hyperbola. This is identical in form to the Michaelis-Menton equation for enzyme kinetics. \\[C=\\frac{sR}{F+R}\\] Where C is resource consumption, R is the amount or density of theresource, s is the asymptotic value and F represents the density of resource at which half the asymptotic consumption is expected to occur. This model is not linear in its parameters. The data below have been simulated from this equation in order to illustrate the model fitting process. d&lt;-read.csv(&quot;/home/aqm/course/data/Hollings.csv&quot;) plot(d,pch=21,bg=2) There are many ways to fit a theoretical non-linear model in R. One of the simplest uses least squares, so some of the assumptions made are similar to linear regression. It is much harder to run diagnosis on these sorts of models. Even fitting them can be tricky. Diagnostics are arguably less important, as the interest lies in finding a model that matches our understanding of how the process works, rather than fitting the data as well as possible. We have to provide R with a list of reasonable starting values for the model. At present R does not provide confidence bands for plotting, but this is less important in the case of non linear models. It is much more interesting to look at the confidence intervals for the parameters themselves. nlmod&lt;-nls(Consumption~s*Resource/(F+Resource),data = d,start = list( F = 20,s=20)) newdata &lt;- data.frame(Resource=seq(min(d$Resource),max(d$Resource),l=100)) a &lt;- predict(nlmod, newdata = newdata) plot(d,main=&quot;Non-linear model&quot;) lines(newdata$Resource,a,lwd=2) confint(nlmod) ## Waiting for profiling to be done... ## 2.5% 97.5% ## F 33.97300 43.66697 ## s 19.58576 20.32930 g0&lt;-ggplot(data=d,aes(x=Resource,y=Consumption)) g1&lt;-g0+geom_point() g1+geom_smooth(method=&quot;nls&quot;,formula=y~s*x/(F+x),method.args=list(start = c( F = 20,s=20)), se=FALSE) The nice result of fitting this form of model is that we can now interpret the result in the context of the process. If the resource represented biomass of ragworm in mg m2 and consumption feeding rate of waders we could now estimate the maximum rate of feeding lies between 19.5 and 20.3 mg hr. The density at which birds feed at half this maximum rate, if the theoretical model applies, lies beyond the range of the data that we obtained (34 to 44 mg m2). Non linear models can thus be extrapolated beyond the data range in a way that linear models, polynomials and splines cannot. However this extrapolation relies on assuming that the functional form of the model is sound. It is common to find that parameters are estimated with very wide confidence intervals even when a model provides a good fit to the data. Reporting confidence intervals for key parameters such as the asymptote is much more important in this context than reporting R2 values. This can be especially important if the results from non linear model fitting are to be used to build process models. You should not usually fit a non-linear model of this type to data, unless you have an underlying theory. When the data are taken from a real life situation it is always best to explore them first using other approaches before thinking about fitting a non-linear model. When we obtain data from nature we do not know that the equation really does represent the process. Even if it does, there will certainly be quite a lot of random noise around the underlying line. 11.2 Real data Smart, Stillman and Norris were interested in the functional responses of farmland birds. In particular they wished to understand the feeding behaviour of the corn bunting Miliaria calandra L, a bird species whose decline may be linked to a reduction of food supply in stubble fields. The authors tested five alternative models of the functional responses of corn buntings. They concluded that Holling’s disk equation provided the most accurate fit to the observed feeding rates while remaining the most statistically simple model tested. d&lt;-read.csv(&quot;/home/aqm/course/data/buntings.csv&quot;) plot(rate~density,data=d,pch=21,bg=2) The classic version of Hollings disk equation used in the article is written as \\[ R=\\frac{aD}{1+aDH} \\] Where F = feeding rate (food items) D = food density (food items \\(m^{-2}\\)) a = searching rate (\\(m^{2}s^{-1}\\)) H = handling time (s per food item). HDmod&lt;-nls(rate~a*density/(1+a*density*H),data = d,start = list(a =0.001,H=2)) confint(HDmod) ## Waiting for profiling to be done... ## 2.5% 97.5% ## a 0.002593086 0.006694939 ## H 1.713495694 1.976978655 newdata &lt;- data.frame(density=seq(0,max(d$density),l=100)) HDpreds &lt;- predict(HDmod, newdata = newdata) plot(rate~density,data=d,pch=21,bg=2,ylim=c(0,1)) lines(newdata$density,HDpreds,lwd=2) g0&lt;-ggplot(data=d,aes(x=density,y=rate)) g1&lt;-g0+geom_point() g1+geom_smooth(method=&quot;nls&quot;,formula=y~a*x/(1+a*x*H),method.args=list(start = c(a = 0.01,H=2)), se=FALSE) The figure in the article shows the five models that were considered. alt Note that models with a vigilance term could not be fit to the data directly due to lack of convergence. 11.2.1 Calculating the R squared The fit of the models in the paper is presented below. alt R does not calculate R squared for non linear models by default. The reason is that statisticians do not accept this as a valid measure. However ecologists are used to seeing R squared values when models are fit. You can calculate them easily enough from a non linear model. First you need the sum of squares that is not explained by the fitted model. This is simply the variance multiplied by n-1. nullss&lt;-(length(d$rate)-1)*var(d$rate) nullss ## [1] 3.544056 We get the residual sum of squares (i.e. the unexplained variability) when we print the model HDmod ## Nonlinear regression model ## model: rate ~ a * density/(1 + a * density * H) ## data: d ## a H ## 0.003966 1.841890 ## residual sum-of-squares: 2.795 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 3.807e-06 So, R squared is just one minus the ratio of the two. Rsq&lt;-1-2.795/nullss Rsq*100 ## [1] 21.13555 Which agrees with the value given in the paper. 11.2.2 Including the vigilance term A very interesting aspect of this article is that the terms of the model were in fact measured independently. Vigilance, which is defined as the proportion of the time spent being vigilant, rather than feeding, is included in most of the models which are presented in the paper as alternatives to the classic disk equation. Most of these are rather complex conditional models, but the simplest is model 2. \\[ R=\\frac{(1-v)aD}{1+aDH} \\] The vigilance parameter for model2 cannot be estimated from the data. However it was also measured independently. alt Notice that vigilance is fundamentally independent of seed density within this range of densities. The measured value (approximately 0.4) can be included and the model refit with this value included. Vigmod&lt;-nls(rate~(1-0.4)*a*density/(1+a*density*H),data = d,start = list(a =0.001,H=2)) confint(Vigmod) ## Waiting for profiling to be done... ## 2.5% 97.5% ## a 0.004321811 0.01115823 ## H 1.028097415 1.18618719 When the measured vigilance is included in the model as an offset term the handling time is reduced by about a half. This makes sense. We may assume that most of the measured vigilance is in fact reducing the rate of feeding as the seed density is so high that search time is very small. However note that we could not possibly have fitted this model without some prior knowledge of the offset value. 11.3 Quantile regression Another way to look at the issue is to use an alternative approach to fitting statistical models. The traditional approach to fitting a model to data assumes that we are always interested in the centre of any pattern. The error term was classically assumed to be uninteresting random noise around an underlying signal. However in many situations this noise is actually part of the phenomenon we are studying. It may sometimes be attributed to process error, in other words variability in the process we are actually measuring, rather than error in our measurements. This may possibly be occurring here. If handling time in fact sets an upper limit on feeding rate, and if we can assume that feeding rate has been measured fairly accurately, then the upper part of the data cloud should be estimating true handling time. Birds that are feeding more slowly than this may be doing something else. For example, they may be spending time being vigilant. So there is possibly some additional information in the data that has not been used in the published paper. We can use quantile regression to fit a non linear model around the top 10% of the data and the bottom 10%. library(quantreg) ## Loading required package: SparseM ## ## Attaching package: &#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve QuantMod90&lt;-nlrq(rate~a*density/(1+a*density*H),data = d,start = list(a =0.001,H=2),tau=0.9) QuantMod10&lt;-nlrq(rate~a*density/(1+a*density*H),data = d,start = list(a =0.001,H=2),tau=0.1) QuantPreds90 &lt;- predict(QuantMod90, newdata = newdata) QuantPreds10 &lt;- predict(QuantMod10, newdata = newdata) plot(rate~density,data=d,pch=21,bg=2,ylim=c(0,1)) lines(newdata$density,HDpreds,lwd=2, col=1) lines(newdata$density,QuantPreds90,lwd=2,col=2) lines(newdata$density,QuantPreds10,lwd=2,col=3) If handling time limits the maximum rate at which seed can be consumed, then the estimate based on the upper 10% of the data should be closer to the true handling time. So if we look at the summaries of these models we should be able to get a handle on vigilance without the prior knowledge. summary(QuantMod90) ## ## Call: nlrq(formula = rate ~ a * density/(1 + a * density * H), data = d, ## start = list(a = 0.001, H = 2), tau = 0.9, control = list( ## maxiter = 100, k = 2, InitialStepSize = 1, big = 1e+20, ## eps = 1e-07, beta = 0.97), trace = FALSE) ## ## tau: [1] 0.9 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## a 0.00821 0.00126 6.51352 0.00000 ## H 1.37329 0.04688 29.29680 0.00000 summary(QuantMod10) ## ## Call: nlrq(formula = rate ~ a * density/(1 + a * density * H), data = d, ## start = list(a = 0.001, H = 2), tau = 0.1, control = list( ## maxiter = 100, k = 2, InitialStepSize = 1, big = 1e+20, ## eps = 1e-07, beta = 0.97), trace = FALSE) ## ## tau: [1] 0.1 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## a 0.00117 0.00052 2.25610 0.02557 ## H 2.64950 0.29736 8.91011 0.00000 The upper limit (pure handling time) is 1.37 (se= 0.047) The lower estimate that may include time spent being vigilant is estimated using quantile regression as 2.65 (se= 0.31) Vigilance thus could, arguably, be estimated as the difference between the upper and lower estimates of handling time divided by the upper value. As the uncertainty around these values has been provided by the quantile regression in the form of a standard error we can use a montecarlo prodedure to find confidence intervals by simulating from the distributions and finding the percentiles of the result. quantile((rnorm(10000,2.65,0.31)-rnorm(10000,1.37,0.047))/rnorm(10000,2.65,0.31),c(0.025,0.5,0.975)) ## 2.5% 50% 97.5% ## 0.2472801 0.4861285 0.7824305 This is within the range of the measured value. There are some interesting elements here that could be discussed in the context of the explanation of the study methods and results. 11.4 Summary Many relationships in ecology do not form strait lines. If we only have data, and no underlying theory, we can fit a model to the underlying shape using traditional methods such as polynomials or more contemporary models such as splines and other forms of local weighted regression. However these models cannot be extrapolated beyond the data range. A very different approach involves finding a model “from first principles”. Often models are constrained to pass through the origin and by some fundamental limit (asymptote). Model fitting then involves finding a justifiable form for the curve lying between these two points. In some situations data can be used to “mediate” between competing models. In other situations the best we can achieve is to find justifiable estimates, with confidence intervals, for the parameters of a non linear model. 11.5 Exercise A researcher is interested in establishing a predicitive equation to estimate the heights of trees based on measurements of their diameter at breast height. Your task is to try to find a defensible model for three different data sets. pines1&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/pinus_allometry1.csv&quot;) oaks&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/quercus_allometry.csv&quot;) pines2&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/pinus_allometry2.csv&quot;) "],
["analysis-of-covariance-nested-data-and-mixed-effects.html", "Chapter 12 Analysis of covariance, nested data and mixed effects 12.1 Introduction 12.2 Whale teeth and isotope ratios 12.3 Plotting the data 12.4 Fitting a regression 12.5 Interpreting the results 12.6 Exercise 12.7 Finding a general pattern 12.8 Analysis of covariance", " Chapter 12 Analysis of covariance, nested data and mixed effects 12.1 Introduction We’ve seen that regression is often not the best available technique to use for bivariate analysis.The books written by Zuur and associates show some of the big challenges involved in analysing real life ecological data. The idealised assumptions of linear regression are very rarely met in full. Zuur writes Always apply the simplest statistical technique on your data, but ensure it is applied correctly! And here is a crucial problem. In ecology, the data are seldom modelled adequately by linear regression models. If they are, you are lucky. If you apply a linear regression model on your data, then you are implicitly assuming a whole series of assumptions, and once the results are obtained, you need to verify all of them. What should we do if we violate all the assumptions? The answer is simple: reject the model. But what do we do if we only violate one of the assumptions? And how much can we violate the assumptions before we are in trouble? This sounds frightening. If model assumptions are rarely met, how can we ever trust them? Do we need the help of an expert statistician such as Alain Zuur to analyse all ecological data? Perhaps all we can use are simple tests. But do we really know that their assumptions hold? The best advice to a student aiming to analyse data for an MSc dissertation is simply to always make the best possible attempt at diagnosing issues with the chosen analysis. Be aware of all the assumptions used and be critical at all times. But don’t despair if some assumptions are not met. Always point out the problems when discussing the analysis. In many cases the issues will in fact turn out to be unimportant. The key message held within most data sets can often be extracted, even if models have minor violations of assumptions. In other cases there may be more serious problems. Sometimes a more advanced analysis than was orginally planned may be necessary. If you have spotted a problem and understand its implications, it could be easier than you think to build a slightly more complex model. You will not be the first person to have come across the issue. The biggest mistakes usually involve failing to account for lack of independence in the data. This chapter runs through an example of this in practice. The data initially seem simple enough. An important issue arises when the same relationship between two variables is repeated multiple times in a data set. In the past you may have handled this situation by subsetting the data various times and repeating the analysis for each subset. However it is possible to build more oomprehensive models that look at a population of relationships. 12.2 Whale teeth and isotope ratios In Zuur et al. (2009) a rather complex analysis involving generalised additive models and correlated residuals is developed and justified as a means of ensuring that all model assumptions are met. The results of the analysis are also presented in a paper that shows how the statistical analysis was used to answer an interesting scientific question. It is well worth reading this paper before analysing the data. The researchers aimed to piece together information regarding feeding behaviour and migration of Sperm whales, based on analysis of the composition of their teeth. Specifically they looked at carbon and nitrogen isotope ratios. N isotope ratios can be indicative of trophic position in marine communities. Trophic level is expected to be higher for larger individuals of the same species. Sperm whales are approximately 4m long at birth, with males growing to 18 m.The relative trophic position of the different stages in the life of several male sperm whales can be investigated through the use of N stable isotope ratios. It was expected that the trophic position would increase with age, as the animals become larger. Whale teeth have growth rings. To obtain data the researchers analysed isotope ratios from the bands of individual teeth. The first task in the analysis could be simply to establish that the expectation of increased N isotope ratio with age is supported by the data. We will concentrate on that before looking at more complex issues. 12.2.1 Moby’s tooth Let’s first look at a single tooth that was obtained from a famous whale called Moby that was washed up on the shores of the Firth of Forth in 1997 after many rescue attempts. Moby was 15.2 metres in length and weighed 38.5 tons. Moby’s bones went on display in the Royal Scottish Museum, Chamber’s street soon after death. The original display failed to attract the public due to the appalling smell. Moby’s skull has since been de-oderised and is now redisplayed. It has even been associated with a Turner prize. Moby is clearly a star turn. But, how does his life history compare to that of other Sperm Whales? This is the topic we will analyse in this class. Whales&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/whaleteeth.csv&quot;) Moby&lt;-subset(Whales,Whale==&quot;Moby&quot;) 12.3 Plotting the data We should first plot the data to look at the pattern. Notice the way a more complex label is set up for the y axis. There are many tricks such as this in R that are rather hard to discover. If you need to use Greek symbols on the axes of your own figures try adapting this code. ylabel &lt;-expression(paste(delta^{15}, &quot;N&quot;)) xlabel&lt;-&quot;Estimated age&quot; library(ggplot2) theme_set(theme_bw()) g0&lt;-ggplot(data=Moby,aes(x=Age,y=X15N)) g1&lt;-g0+geom_point() +xlab(xlabel) +ylab(ylabel) g1 12.4 Fitting a regression OK, so this all looks quite simple at this stage. The data points lie more or less along a straight line. So we could fit a regression. mod&lt;-lm(data=Moby,X15N~Age) summary(mod) ## ## Call: ## lm(formula = X15N ~ Age, data = Moby) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.07102 -0.28706 0.04346 0.33820 1.13724 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.748940 0.163559 71.83 &lt;2e-16 *** ## Age 0.113794 0.006186 18.40 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4859 on 40 degrees of freedom ## Multiple R-squared: 0.8943, Adjusted R-squared: 0.8917 ## F-statistic: 338.4 on 1 and 40 DF, p-value: &lt; 2.2e-16 g1+stat_smooth(method=&quot;lm&quot;)+labs(y = ylabel,x=xlabel) 12.4.1 Diagnostics Diagnostics show that most of the assumptions of a regression appear to be met. However an issue shows up when we look at the first diagnostic plot. plot(mod,which=1) There is a clear pattern in the residuals. 12.4.2 Testing for serial correlation In fact the residuals are serially correlated. This can be checked using a Durbin Watson test. library(lmtest) dwtest(Moby$X15N~Moby$Age) ## ## Durbin-Watson test ## ## data: Moby$X15N ~ Moby$Age ## DW = 1.1458, p-value = 0.0009539 ## alternative hypothesis: true autocorrelation is greater than 0 We can confirm this using the acf function. acf(residuals(mod)) If any of the lines apart from the first extends above or below the blue line the data are significantly serially correlated. In other words the last value can be used to partially predict the next value. If one residual is negative, the next is also likely to be negative and vice versa. The correlation is significant at lag 1 and lag 2. 12.5 Interpreting the results Does the serial correlation matter? Well in a purely statistical sense it could matter quite a lot. Zuur et al take the effect into account through the use of a rather cunning statistical trick using an ARIMA model. However most MSc students would be unlikely to be aware of this rather advanced method. In this context it is not really worth the effort. The only real difference between a model that includes autocorrelation and one that does not involves the size of the p-value and the width of the confidence bands. Providing that a straight line is a reasonable description of the underlying pattern (and we will come on to that later), the model is not changed. There is something much more important to consider however. We have spotted the serial correlation (lack of independence) between years that occurs in the case of Moby. However, you may have realised that in reality all the data we have just analysed are totally lacking in independence anyway! They have been taken from a single whale. So, in one sense we have been conducting statistics with a sample size of one. In effect, the relationship that we have established applies only to Moby. It would also be really nice to have data from some other teeth from Moby to check even this. We certainly cannot generalise it to other whales. We may have found out some interesting information along the way. The researchers were interested in understanding the trophic ecology of mature sperm whales in some detail. If they are right in assuming that high levels of delta N are associated with feeding on larger prey then there may be some indication that Moby spent several years in a row feeding on prey that may have been larger than expected for his age, followed by a switch to rather smaller prey. This is quite a speculative interpretation, but it may tell us something. 12.6 Exercise Now try implementing an appropriate method to look at alternatives to a straight line to capture the empirical relationship between age and delta N measurements in Moby’s tooth. Comment on your findings. 12.7 Finding a general pattern We actually have data for fifteen whales. So we could compare the pattern shown by Moby with the population of whale teeth. So let’s plot out the data from all fifteen whales at once. g0&lt;-ggplot(data=Whales,aes(x=Age,y=X15N,col=Whale)) g1&lt;-g0+geom_point() + xlab(xlabel) +ylab(ylabel) g1 There is still a clear pattern, but with much more scatter this time. This is to be expected, as each whale has its own life history that contributes variability. We can look at this by plotting out the data for each individual whale using a facet wrap to split the data. g0&lt;-ggplot(data=Whales,aes(x=Age,y=X15N)) g1&lt;-g0+geom_point()+geom_line()+ labs(y = ylabel,x=xlabel) g1+facet_wrap(&quot;Whale&quot;) Each whale seems to have its own pattern. Most are approximately linear, like Moby, but some show a very different pattern For the moment we will ignore this and continue to fit a simple regression model to all the data. Note THIS IS THE WRONG WAY TO WORK WITH THE DATA! mod1&lt;-lm(X15N~Age,data=Whales) summary(mod1) ## ## Call: ## lm(formula = X15N ~ Age, data = Whales) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5041 -0.7013 -0.1141 0.6209 3.3444 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.228411 0.108598 112.6 &lt;2e-16 *** ## Age 0.095715 0.005631 17.0 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9953 on 305 degrees of freedom ## Multiple R-squared: 0.4865, Adjusted R-squared: 0.4848 ## F-statistic: 288.9 on 1 and 305 DF, p-value: &lt; 2.2e-16 confint(mod1) ## 2.5 % 97.5 % ## (Intercept) 12.01471479 12.4421063 ## Age 0.08463424 0.1067958 The problem with this model is that we have far too many degrees of freedom. The model has assumed that each data point is independent, which clearly is not true. 12.8 Analysis of covariance The data that we have available to predict N15 levels consists of two variables. One is Age and the other is the identity of the individual whale from which the data was obtained. This is a categorical variable. If we continue to put to one side the issue of independence of the data points within the regression for each whale we can analyse the data using a technique known as analysis of covariance. Analysis of covariance takes into account the differences between each group of observations. It is used to answer the following questions. Does the level of a categorical factor affect the response variable? Does the slope of a regression line differ between levels of the categorical variable? In our case the first question implies that each whale may have a different mean value for \\(\\delta^{15}N\\). If you read the paper carefully you will see that the researchers were aware that this is a rather trivial question. They looked at more subtle differences related to time of birth and life history. However we will ignore this for the moment too and carry out the analysis in order to illustrate the point. As mentioned previously, R has a very consistent syntax that makes model fitting very easy. All we need to do to add in an effect for each whale to the model is to write lm(X15N~Age+Whale). The line of R code preceding this sets the contrasts for the model. This determines the output from the summary of the model. In this case we want to contrast the results for other whales with Moby (number 11 in the list). contrasts(Whales$Whale)&lt;-contr.treatment(levels(Whales$Whale),base=11) mod2&lt;-lm(X15N~Age+Whale,data=Whales) anova(mod2) ## Analysis of Variance Table ## ## Response: X15N ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Age 1 286.21 286.215 431.039 &lt; 2.2e-16 *** ## Whale 10 106.27 10.627 16.004 &lt; 2.2e-16 *** ## Residuals 295 195.88 0.664 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod2) ## ## Call: ## lm(formula = X15N ~ Age + Whale, data = Whales) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.70699 -0.51434 -0.07552 0.47030 2.64139 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.256788 0.174661 70.175 &lt; 2e-16 *** ## Age 0.092183 0.005159 17.869 &lt; 2e-16 *** ## WhaleI1/98 -0.010461 0.181371 -0.058 0.95404 ## WhaleM143/96D -0.229444 0.220575 -1.040 0.29909 ## WhaleM143/96E 0.385063 0.235813 1.633 0.10355 ## WhaleM2583/94(1) -0.936579 0.216095 -4.334 2.01e-05 *** ## WhaleM2583/94(10) -0.357032 0.209785 -1.702 0.08983 . ## WhaleM2583/94(7) -0.901441 0.223214 -4.038 6.87e-05 *** ## WhaleM2679/93 -0.059622 0.185189 -0.322 0.74772 ## WhaleM2683/93 0.267238 0.227314 1.176 0.24069 ## WhaleM447/98 1.210339 0.192499 6.288 1.16e-09 *** ## WhaleM546/95 0.685273 0.219544 3.121 0.00198 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8149 on 295 degrees of freedom ## Multiple R-squared: 0.6671, Adjusted R-squared: 0.6547 ## F-statistic: 53.73 on 11 and 295 DF, p-value: &lt; 2.2e-16 The anova result is very clear. There is a significant additive effect that can be attributed to the individual. So we can see that whales do differ in their baseline N15 levels. In order to interpret the model summary you need to be aware that the intercept represents the value at age zero for Moby, which was set to be the “control” or baseline for the contrasts. The other whale values are compared to the baseline. Thus if a coefficient for an individual whale is not significant it means that the intercept for that whale was not different to that of Moby. So, the answer to the first question, just as we would expect, is a definite yes. Six of the ten whales that we compared with Moby have significantly different levels of N15 after allowing for age. How do we answer the second question? This involves adding an interaction term to the model. The interaction implies that each whale could have it’s own intercept AND it’s own slope. If the interaction term is significant then we have a very complex model that cannot be easily generalised. mod3&lt;-lm(X15N~Age*Whale,data=Whales) anova(mod3) ## Analysis of Variance Table ## ## Response: X15N ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Age 1 286.215 286.215 734.509 &lt; 2.2e-16 *** ## Whale 10 106.266 10.627 27.271 &lt; 2.2e-16 *** ## Age:Whale 10 84.828 8.483 21.769 &lt; 2.2e-16 *** ## Residuals 285 111.055 0.390 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The interaction term is highly significant. This is not suprising when we consider the lattice plot, which showed a series of very different patterns for each whale. If we summarise the model now we will obtain even more complex output which shows how the coefficients depend on the identity of each whale. summary(mod3) ## ## Call: ## lm(formula = X15N ~ Age * Whale, data = Whales) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.58547 -0.40951 -0.01552 0.37746 2.41087 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.748940 0.210125 55.914 &lt; 2e-16 *** ## Age 0.113794 0.007947 14.320 &lt; 2e-16 *** ## WhaleI1/98 1.464565 0.303845 4.820 2.34e-06 *** ## WhaleM143/96D 1.905086 0.376915 5.054 7.73e-07 *** ## WhaleM143/96E 0.720710 0.364725 1.976 0.049115 * ## WhaleM2583/94(1) -1.517527 0.336649 -4.508 9.58e-06 *** ## WhaleM2583/94(10) 0.747337 0.328174 2.277 0.023512 * ## WhaleM2583/94(7) -0.559979 0.346500 -1.616 0.107179 ## WhaleM2679/93 -0.750516 0.296705 -2.529 0.011962 * ## WhaleM2683/93 0.509286 0.412830 1.234 0.218351 ## WhaleM447/98 1.557803 0.305940 5.092 6.45e-07 *** ## WhaleM546/95 3.269282 0.341382 9.577 &lt; 2e-16 *** ## Age:WhaleI1/98 -0.065573 0.011918 -5.502 8.36e-08 *** ## Age:WhaleM143/96D -0.142106 0.022432 -6.335 9.23e-10 *** ## Age:WhaleM143/96E -0.004390 0.027327 -0.161 0.872476 ## Age:WhaleM2583/94(1) 0.065493 0.020050 3.267 0.001222 ** ## Age:WhaleM2583/94(10) -0.065797 0.018155 -3.624 0.000343 *** ## Age:WhaleM2583/94(7) -0.007142 0.022432 -0.318 0.750421 ## Age:WhaleM2679/93 0.041481 0.012471 3.326 0.000996 *** ## Age:WhaleM2683/93 -0.001922 0.025478 -0.075 0.939929 ## Age:WhaleM447/98 -0.012176 0.013906 -0.876 0.381993 ## Age:WhaleM546/95 -0.194624 0.021171 -9.193 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6242 on 285 degrees of freedom ## Multiple R-squared: 0.8112, Adjusted R-squared: 0.7973 ## F-statistic: 58.33 on 21 and 285 DF, p-value: &lt; 2.2e-16 Now we can see that six of the whales do not just have a different baseline level of \\(\\delta^{15}N\\) to Moby, the change over time when modelled as a linear response is also different for these individuals. 12.8.1 More about interactions The model above could also be written in R longhand as: mod3&lt;-lm(X15N~Age+Whale+Age:Whale,data=Whales) The {*} symbol is simply short hand for this. It does not imply multiplication. The interaction component is Age:Whale. Don’t confuse statistical interactions with ecological interactions such so those that occur in a food web. A statistical interaction implies that we need to know additional information in order to predict something. For example if an experiment showed an interaction between nitrogen fertilizer level and soil type we could not be sure that adding nitrogen always enhances growth in the same way. Perhaps it has no effect at all when applied to heavy clay soils, although it increases growth generally. Finding interactions suggest complexity. This can often be interesting in itself. 12.8.2 Plotting the model The model with interactions can be plotted using ggplots as a series of individual regression lines for each whale. Notice that the slope differs between each of the panels. g0&lt;-ggplot(data=Whales,aes(x=Age,y=X15N,group=Whale)) g1&lt;-g0+geom_point()+ labs(y = ylabel,x=xlabel) +geom_smooth(method=&quot;lm&quot;) g1+facet_wrap(&quot;Whale&quot;) "],
["introducing-mixed-effects-modelling.html", "Chapter 13 Introducing mixed effects modelling 13.1 Using the nlme package 13.2 Using the package lme4 13.3 Ggplots from lme output 13.4 Mixed effect gamm models 13.5 Summary", " Chapter 13 Introducing mixed effects modelling Analysis of covariance is often used when there are a small number of factor levels. If we had two or three whales either subsetting for each whale or analysis of covariance would be the only appropriate methods. There would not be enough replication to think about the data as having been drawn from a larger population of whales that it could represent. Each case would be treated as being distinct. However, as we get more data analysis of covariance becomes clumsy and less appropriate as a technique. We can see that in this example. A mixed effects model treats each level of a factor as a random effect. This is a powerful technique when we have enough data as we are effectively trying to draw out some underlying population level pattern, which we may want to use when comparing the response of each individual. Mixed effects modelling also has some advantages for data sets with missing data or unbalanced numbers of observations for each group. The example was adapted from Zuur’s book on mixed effects models. Mixed effects are also known as hierarchical models. It is important to be aware of the potential usefulness of such models and the difference between fixed and random effects. The last model we fitted used a regression equation for each whale in turn. This approach assumes the identity of the whale is a fixed factor. We need to know each whale’s name in order to use the model! If we want to generalise for all whales we would be much better to think of the whales as compromising a random sample from all the whales that we might have found. The following R code uses a mixed effects model approach to fit a separate regression line for each whale. To cut a long story short, and to simplify the overall idea, you may think of the result as being rather similar to taking the mean slope and intercept from all the individual regressions and finding confidence intervals for the pooled result. This was a traditional way of analysing data of this type and it is still used by some researchers. It is not “wrong” in essence, but modern methods are both easier to apply and more formally correct due to the manner in which unbalanced samples are handled. Look at the plot we produced for a regression fitted to each whale to get an idea of what the model is based on. Imagine averaging the slopes and intercepts shown in this figure in order to get an intuitive idea of the basis for the model. 13.1 Using the nlme package The nlme package name stands for “non linear mixed effects”. It has some poweful functions for fitting non linear models with random effects. More recently the lme4 package has become more popular for linear mixed effects as the algorithm used is prefered by mathematical statisticians. In almost all applied situations the results from using either package for linear mixed effects models are identical. Plotting the models is easy using nlme as the package has a grouping function for the data and a series of default plots that use the lattice package. Lattice plots have now largely given way to ggplots when formally presenting results as ggplots are more flexible and nicer looking. However the default in nlme is easy to use and easy to interpret. 13.1.1 Intercept only model library(nlme) w&lt;-groupedData(X15N~Age|Whale,Whales) intercept.mod&lt;-lme(X15N~Age,random=~1|Whale,data=w) print(plot(augPred(intercept.mod,level=c(0,1)))) 13.1.2 Random slopes model slope.mod&lt;-lme(fixed=X15N~Age,random=X15N~Age|Whale,data=w) print(plot(augPred(slope.mod,level=c(0,1)))) The fixed effect is the underlying trend of increasing \\(\\delta^{15}N\\) levels with age. The random effect is the variation in slope and intercept between whales. We can test which model is prefered on statistical criteria by comparing them. anova(intercept.mod,slope.mod) ## Model df AIC BIC logLik Test L.Ratio p-value ## intercept.mod 1 4 792.3965 807.2777 -392.1982 ## slope.mod 2 6 669.3825 691.7044 -328.6913 1 vs 2 127.0139 &lt;.0001 So the slope model provides a significantly better fit to the data, just as we found when taking whale as a fixed effect. 13.1.2.1 Confidence intervals for the effects intervals(slope.mod) ## Approximate 95% confidence intervals ## ## Fixed effects: ## lower est. upper ## (Intercept) 11.60509740 12.41013148 13.2151656 ## Age 0.03376861 0.07914293 0.1245172 ## attr(,&quot;label&quot;) ## [1] &quot;Fixed effects:&quot; ## ## Random Effects: ## Level: Whale ## lower est. upper ## sd((Intercept)) 0.84344384 1.33071563 2.0994926 ## sd(Age) 0.04669552 0.07443052 0.1186389 ## cor((Intercept),Age) -0.97238683 -0.90157346 -0.6787395 ## ## Within-group standard error: ## lower est. upper ## 0.5750660 0.6242716 0.6776874 By taking into account the lack of independence we now get a much wider confidence intervals for the coefficients than we did when we very naively fitted a regression to the pooled data without taking into account the identity of the whale from which the data was obtained. However we now have a generalisable model based on a sample of size of n=15, rather than the model with far too many degrees of freedom. The mixed model is better than the model used in the analysis of covariance, as this could only be used for prediction if you provide the identity of the particular whale that you are interested in. Notice that the parameters estimated for the random effects are standard deviations. If you can imagine actually fitting all 15 separate models, writing down the interecept and the slope 15 times and finding the mean and standard deviation of each you have approximated the operation carried out by the mixed effects modelling. The standard deviations are approximations, so there are also confidence intervals for these provided by the full fitted model. There is a whole lot more to mixed effects modelling, but this result is quite simple to understand in this context. It provides a robust solution to the issue of finding confidence intervals for the mean slopes and intercepts when analysis of covariance shows that seperate regression models are needed for each level of a factor, but we do want to treat the levels of the factor as random draws from a population as there are too many factor levels to be concerned with comparisons between each one. Mixed effects also handles unbalanced samples, as if one whale has fewer observations the overall model gives the data less weight. Finding an underlying trend in N15 levels is only part of the story told in the paper. An issue that has been set well to one side in this still simplified example is whether a linear regression is the best approach to finding an underlying pattern. In fact Zuur et al used GAMs in order to capture the non linear relationship that some individuals seem to have. 13.2 Using the package lme4 The alternative package to nlme is lme4. This uses a more sophisticated approach to actually fiting the model that is considered more reliable mathematically by some statisticians. In most real life situations there is little difference between the results from each package and they can usually be used interchangeably. A slightly unusual aspect of lme4 is that it does not provide p-values for the test of signficance of the effects by default. This was a deliberate decision made by the author of the package (Douglas Bates). The argument is that the degrees of freedom are ill-defined for many technical reasons. As p-values are only really useful if a statistic such as t is close to the boundary of significance the argument goes that there is no point in calculating them unless they are exact. A t value of 3 is going to be significant whatever and a t value of 1 will not be. However some people demand p-values and they can be calculated quite easily. The package Lmertest adds these calculations to the output of lme4. The syntax used to fit the models is slightly different but follows the same general pattern as nlme. A random effect for the intercept alone is written as (1|Whale). library(lmerTest) intercept.mod2&lt;-lmer(X15N~Age+(1|Whale),data=w) intercept.mod2 ## Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] ## Formula: X15N ~ Age + (1 | Whale) ## Data: w ## REML criterion at convergence: 784.3965 ## Random effects: ## Groups Name Std.Dev. ## Whale (Intercept) 0.6139 ## Residual 0.8149 ## Number of obs: 307, groups: Whale, 11 ## Fixed Effects: ## (Intercept) Age ## 12.25874 0.09245 anova(intercept.mod2) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Age 216.11 216.11 1 301.39 325.46 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The slope model can be fit in the same way. slope.mod2&lt;-lmer(X15N~Age+(Age|Whale),data=w) slope.mod2 ## Linear mixed model fit by REML [&#39;lmerModLmerTest&#39;] ## Formula: X15N ~ Age + (Age | Whale) ## Data: w ## REML criterion at convergence: 657.3826 ## Random effects: ## Groups Name Std.Dev. Corr ## Whale (Intercept) 1.33124 ## Age 0.07446 -0.90 ## Residual 0.62427 ## Number of obs: 307, groups: Whale, 11 ## Fixed Effects: ## (Intercept) Age ## 12.41013 0.07914 ## convergence code 0; 1 optimizer warnings; 0 lme4 warnings anova(slope.mod2) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Age 4.5882 4.5882 1 9.8319 11.773 0.006585 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.2.1 Profile confidence intervals A more accurate, but much slower, profiling technique is used to calculate confidence intervals in lme4. The result may be slightly different from nlme. # Note that this takes around 30 seconds to run as it is based on simulated profiling confint.merMod(slope.mod2) ## 2.5 % 97.5 % ## .sig01 0.85660240 2.0833944 ## .sig02 -0.97150830 -0.6859207 ## .sig03 0.04712678 0.1173118 ## .sigma 0.57633298 0.6792690 ## (Intercept) 11.57425698 13.2460978 ## Age 0.03191633 0.1262261 13.3 Ggplots from lme output The lme4 package does not include the augpred function. However it is easy to reproduce the same sort of figures using ggplots. The trick is to predict the fixed effect alone and the fixed and random effects for each data point then add the results to the figure as lines. Whales$fixed&lt;-predict(slope.mod2,re.form=NA) Whales$rand&lt;-predict(slope.mod2) g0&lt;-ggplot(Whales,aes(x=Age,y=X15N)) g1&lt;-g0+geom_point() g1&lt;-g1+geom_line(aes(x=Age,y=fixed),colour=2,lwd=1) g1&lt;-g1+geom_line(aes(x=Age,y=rand),colour=3,lwd=1) g1&lt;-g1+labs(y = ylabel,x=xlabel,title=&quot;Fixed and random effects&quot;) g1+facet_wrap(&quot;Whale&quot;) 13.4 Mixed effect gamm models You could argue (rightly), after looking at al the data. that straight lines do not provide a good model for the pattern in the data at all. However analyses are not just about finding the best fitting model. The model has to have some underyling purpose and feed into some narrative about the data. A fitted linear trend is often used as an easily communicated baseline for comparison rather than as a perfect model for the data. So it may still be useful to be able to state how much “on avererage” you expect \\(\\delta^{15}N\\) levels to increase per year. It is however possible to fit smoothers using a mixed effects approach. The approach could be “overkill” for some purposes. If you just need to find the overall average change in \\(\\delta^{15}N\\) per year it would not be useful. However it could also help show the differences between the pattern for each whale and so fit the overall population level pattern if the trend is not linear. So .. and do note that this is rather advanced stuff that is not an essential part of the course, I will show how this can be done using the gamm4 package in R. Fitting the model uses the same syntax, but with a smooth term for the fixed effects. The interepretation of the smoother is more difficult. library(gamm4) gamm.mod&lt;-gamm4(X15N~s(Age),data=w,random = ~ (Age|Whale)) gamm.mod ## $mer ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## REML criterion at convergence: 619.3 ## Random effects: ## Groups Name Std.Dev. Corr ## Whale (Intercept) 1.35294 ## Age 0.07361 -0.91 ## Xr s(Age) 2.66779 ## Residual 0.57755 ## Number of obs: 307, groups: Whale, 11; Xr, 8 ## Fixed Effects: ## X(Intercept) Xs(Age)Fx1 ## 13.8111 -0.2626 ## ## $gam ## ## Family: gaussian ## Link function: identity ## ## Formula: ## X15N ~ s(Age) ## ## Estimated degrees of freedom: ## 5.66 total = 6.66 ## ## lmer.REML score: 619.3 The only real way to understand any model based on a smoother is to plot it out. The effects can be extracted and added to plots. Whales$fixed&lt;-predict(gamm.mod$gam) Whales$rand&lt;-predict(gamm.mod$mer) g0&lt;-ggplot(Whales,aes(x=Age,y=X15N)) g1&lt;-g0+geom_point() g1&lt;-g1+geom_line(aes(x=Age,y=fixed),colour=2,lwd=1) g1&lt;-g1+geom_line(aes(x=Age,y=rand),colour=3,lwd=1) g1&lt;-g1+labs(y = ylabel,x=xlabel,title=&quot;Fixed and random splines&quot;) g1+facet_wrap(&quot;Whale&quot;) Notice that there is now an overall curve for the fixed effect with separate curves for each whale. The overall fixed effect model is the average of all these curves. It is possible to go one step futher and add confidence intervals to a fixed effect plot, because the standard error for the fixed effects can also be found Whales$fixedse&lt;-predict(gamm.mod$gam,se=T)$se g0&lt;-ggplot(Whales,aes(x=Age,y=X15N)) g1&lt;-g0+geom_point() g1&lt;-g1+geom_line(aes(x=Age,y=fixed),colour=2,lwd=1) g1&lt;-g1+geom_line(aes(x=Age,y=fixed+2*fixedse),colour=2,lty=2) g1&lt;-g1+geom_line(aes(x=Age,y=fixed-2*fixedse),colour=2,lty=2) g1&lt;-g1+geom_line(aes(x=Age,y=rand),colour=3,lwd=1) g1&lt;-g1+labs(y = ylabel,x=xlabel,title=&quot;Fixed and random splines&quot;) g1+facet_wrap(&quot;Whale&quot;) The real reason for all this in practice would be to compare the overall pattern with that shown by each individual whale. The overall fixed effect as fitted by a spline may be a useful model as a general description of the pattern shown by the population. The normal caveats about the representativiness of the sample apply of course. In this particular case there is also the issue that the whales may have lived at different periods. Notice that the confidence intervals for the smoother are now much wider than you would obtain if the random effect of each whale was not included in the model. This is because the sample size is effectively only 15 and the random variability between whales is being properly taken into account. The model potentially generalises to the population from which the sample (n=15) was taken. g0&lt;-ggplot(Whales,aes(x=Age,y=X15N,col=Whale)) g1&lt;-g0+geom_point() g1&lt;-g1+geom_line(aes(x=Age,y=fixed),colour=1,lwd=1) g1&lt;-g1+geom_line(aes(x=Age,y=fixed+2*fixedse),colour=1,lty=2) g1&lt;-g1+geom_line(aes(x=Age,y=fixed-2*fixedse),colour=1,lty=2) g1&lt;-g1+labs(y = ylabel,x=xlabel,title=&quot;Fixed effect confidence intervals&quot;) g1 Look at the difference between the “naive” smooth that overpredicts by just using all the points as if they were all independent and the much more robust model that is based on the population level fixed effect. g0&lt;-ggplot(Whales,aes(x=Age,y=X15N)) g1&lt;-g0+geom_point() g1&lt;-g1+geom_line(aes(x=Age,y=fixed),colour=1,lwd=1) g1&lt;-g1+geom_line(aes(x=Age,y=fixed+2*fixedse),colour=1,lty=2) g1&lt;-g1+geom_line(aes(x=Age,y=fixed-2*fixedse),colour=1,lty=2) g1&lt;-g1+labs(y = ylabel,x=xlabel,title=&quot;Fixed and random CIs&quot;) g1+geom_smooth(method=&quot;gam&quot;,formula=y~s(x)) You should also notice that the general shape of the response is still very similar. The point of going through all the process of modelling the random effect is just to reveal how much (or how little) confidence we have that the shape of the response can be generalised. The appropriate model for the data takes into account the small sample of whales, even though there are many individual data points. The message seems to be that whales begin to feed on similar sized food items overall once they reach an age of 30 years. 13.5 Summary This example shows that a lot can be learned about the data by first applying relatively simple (and statistically incorrect) techniques, even though the models that are first tried will later need major modification in order to prevent violations of assumptions. Once all the issues have been spotted, solutions can usually be found, although they may involve using more advanced techniques used in combination. This approach may need help from a more experienced analyst. The problem of lack of independence are very common in many data sets. You can explore the data using simple techniques in the knowledge that an ajustment may be needed before finally presenting results in order to ensure the findings are robust from a statistical perspective. Notice that constructing the more sophisticated models doesn’t necessarily change the underlying message in the data. If we divide models into an informative component, that we are most interested in, and a stochastic component (variability) that is of less scientific interest, then the statistical sophistication is largely needed mainly to produce a better model for the stochastic element. Looking at the data carefully will usually produce insight into the scientific issu. Finding the right model allows us to produce more defensible confidence intervals (or p-values), even though our basic conclusions may not change. This is a common element in statistical modelling. Don’t expect complex models to necessarily “tell you more” than simpler models. This may look like a lot of work for little reward, but the fundamental reason for using more sophisticated models is to reduce the number of assumptions that are being violated and thus carry out truly reproducible research. In this case we end up with a generalised pattern based on the data that we are not overly confident about, but may be a useful summary of the general trend at the population level. So while we would not be suprised at all to find whale teeth that follow very different patterns to that shown in Moby’s particular case, we would confidently expect that a new sample of fifteen whales would produce a similar pattern to that found using the mixed effects model, providing the sample has been drawn from the same potential population. The example has also been used to introduce analysis of covariance, a technique that uses both a categorical variable (factor) and a numerical variable as explanatory variables in the model formula. You can now apply the technique to slightly simpler data in the examples below in order to practice. 13.5.1 Exercises Analysis of covariance can be used to look in more detail at the mussels data again. You first have to make sure that R treats site as a factor. Mussels&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/mussels.csv&quot;) Mussels$Site&lt;-as.factor(Mussels$Site) Use linear analysis of covariance. Is there a difference in the relationship between BTvolume and shell length between sites? How did you reach your conclusion? Soybean growth curves Try these growth curves. They are best fit using non-linear mixed effects, but you could still try using the methods above for this exercise. data(&quot;Soybean&quot;) d&lt;-subset(Soybean,Soybean$Variety==&quot;F&quot;) d&lt;-subset(d,d$Year==1990) g0&lt;-ggplot(d,aes(x=Time,y=weight)) g1&lt;-g0+geom_point() g1+facet_wrap(&quot;Plot&quot;) Here is another similar but very difficult data set to practice on (optional). It has also been analysed by Zuur. The measurements are lengths and ash free dry weight (AFD) of 398 wedge clams (Donax hanleyanus) taken from a beach in Argentina during six different months (Ieno, unpublished data). The main issue is the extremely unbalanced nature of the data. Can we do anything useful at all with this tricky observational data set? There are limits that even the most sophisticated statistics cannot remove. Clams&lt;-read.table(&quot;https://tinyurl.com/aqm-data/Clams.txt&quot;,head=T) Clams$MONTH&lt;-as.factor(Clams$MONTH) The data frame already contains log transformed versions of the variables. Try first fitting a regression to the untransformed data and then use the transformed version. Which is most suitable for modelling using regression? Now use analysis of covariance to investigate the effect of month on the relationship. Does this work? What would you do if faced with real life data of this sort? "],
["design-and-analysis-of-experiments-part-1.html", "Chapter 14 Design and analysis of experiments part 1 14.1 Introduction 14.2 Basic concepts of experimental design 14.3 Types of design 14.4 Visualising the data 14.5 Completely randomised design with subsampling 14.6 Randomized Complete Block Design 14.7 Illustation of how block effects work 14.8 An observational example of sub-sampling 14.9 Summary 14.10 Exercises", " Chapter 14 Design and analysis of experiments part 1 14.1 Introduction Observational studies are much more common in environmental science and ecology than in other fields. When we analyse observational data we are seeking to gain evidence in support of scientific hypotheses. However this evidence tends to be indirect and open to a range of interpretations. In contrast, a well designed experiment involves a planned manipulation of nature that should allow us to test hypotheses directly. A statistical hypothesis never completely coincides with the scientific hypothesis, however in an experimental setting the two concepts may be closely linked. Experimental studies are rare in ecology because they tend to be costly to set up, may require large areas and may need to run for many years in order to provide convincing data. So we often look for natural experiments that arise on an ad hoc basis to provide data. Sometimes unintentional human interventions such as oil spills, hunting, landscape fragmentation etc provide such systems that we can study. Although these rarely follow conventional experimental design, familiarity with experimental concepts may help in the selection of an appropriate analysis. Many of the analytical methods and concepts used by experimental ecologists and environmental scientist have been derived from agricultural experiments. There are good reasons for this. The most influential statistician of all time, R A Fisher, was employed for many years at Rothamstead agricultural research centre. During that time he developed the theory and practical application of analysis of variance. The language used to describe experimental designs is based around this work. Although this helps to provide a common language it can sometimes be confusing. Terms such as “split plot” can be used for a design with no real plots, “Repeat measures” can occur when the measures are not actually taken on more than one occasion. Blocks may refer to non spatial entities. The key to understanding experimental design is to realise that the design of the experiment and the design of the statistical model used to analyse it go hand in hand. If you can understand the structure of the data and the model used to analyse it you can either design experiments to produce appropriate data or design surveys that provide observational data that can be analysed using methods designed for experiments. 14.2 Basic concepts of experimental design 14.2.1 1. Replication The most important element to recognise when designing any form of experiment that will be analysed using statistical methods is the amount of replication involved. This can be easy to determine if the experimental unit is well defined. For example if a treatment is applied to an animal then the number of replicates is usually going to be the number of animals used in the experiment. However if the animals share a single environment, such as a cage or enclosure, then that may become the experimental unit thus possibly reducing the number of replicates. It can be more complex when treatment is applied to plots of land due to spatial auto correlation and lack of independence. Ecological experiments have often been criticised for involving so called “pseudoreplication”. This occurs when the true number of independent replicates is much lower than the number claimed by the researcher. One cause of this may be that sub samples taken from what should properly be regarded as a single experimental unit are analysed as if they constitute true replicate samples. This may become clearer after we have looked at some examples. The amount of replication needed to establish the significance and size of the effect of some intervention depends to a large extent on the amount of natural variability in the system of study. Some laboratory based experiments can be based on extremely small numbers of replicates. For example, if drugs are administered to a set of genetically identical mice there may be no need for more than three animals in the control and treatment group in order to establish a statsitically significant effect. The reason for this is the response of all the animals without intervention would be very similar and all are likely to respond in identical ways to the treatment. However this experiment would be limited to establishing the effect of the drug on that single genotype. If a set of individuals captured at random from a wild population were used the study would have to be much larger. If the intention was to test the drug on the wild population at large it would have to be even larger in order to account for all the extraneous source of variability in the environment. Similarly if a researcher is interested in the effects of some treatment on plant growth and works with genetically similar plants grown on identical media in a greenhouse, for example using hydroponics or a very uniform potting compost, then the experiment may be need few replicates. However if the plants are grown in fields with varying soil conditions both within and between fields the experimental design will need to take this variability into account. Ecologists and environmental scientists face more challenges than other researcher as a result of the variability in natural systems. At the same time it may be difficult or impossible to find enough naturally ocurring independent replicates to use robust analytical methods. 14.2.2 2. Treatment levels A second concept is that of treatment levels. The most basic type of experiment involves two levels of a treatment. The first level is simply no intervention (control). So, if we wanted to look at the effects of pesticides on pollinator abundance we would design an experiment with replicated units in which no pesticide was applied (control) and others in which the same level was applied. It is easy to see that the two level model could be extended to include more categories, such as high, medium and low levels of application. The classic analyses use fixed, categorical treatment levels. However in an observational context we often obtain continuous measurements. This influences the way we analyse data as we may substitute regression and analysis of covariance for a classic ANOVA. However linear statistical models have an underlying mathematical structure that can be used for either form of data. We can build models with combinations of both types of variables. 14.2.3 3. Randomisation The third concept to understand is randomisation. When designing an experiment the units should be assigned a treatment level at random which should not depend on their current status and ideally should not even be known by the experimenter. So, for example, in drug trials the participants are assigned a treatment at random. In a blind trial they themselves do not know whether they have been given a new drug or a placebo. In a double blind trial neither does the researcher. If random selection is used there should be no criteria used to choose those that receive the treatment. As you can imagine, this can sometimes raise ethical issues, but is a fundamental feature of statistically rigorous experimental design. In many situations involving “natural” experiments there is no randomisation in the application of treatments. For example a forest fire may burn some trees and leave others intact but the selection of trees for treatment by fire is far from random. Parasites may attack hosts that are weakened by some other cause. Often spatial prximity is a key factor in selection for treatment. This leads to spatial autocorrelation and a lack of independence. Once more this makes applying the analytical methods designed for analysing true experiments more challenging when data is derived from observations of the natural system. 14.2.4 4. Interactions We use the word interaction frequently in ecology to refer to effects such as competition, depredation, parasitism and other elements involved in the systems we study. However in statistics the word has a specific meaning that applies to experiments or observational studies involving more than a single factor. An interaction occurs when the effects of the factor are not additive. In other words the level of one factor influence the effect of another. Take for example an experiment in which saplings were grown at two light levels, full sun and 50% shade and two irrigation treatments applied. Let’s call them high and low. Plants need both water and light to grow so we might expect the plants grown in full sun with high levels of irrigation to grow faster than those with low levels of irrigation. However what if the plants fail to respond to increased watering at low light levels, or even grow more slowly, perhaps due to waterlogged roots. In this case there would be an interaction as the two effects do not add together. The effect of irrigation is conditional on the level of light. 14.2.5 5. Fixed and random effects The distinction between whether we treat factors as having fixed or random effects is rather subtle and subjective. Simply put fixed effects are those that we are most interested in while random effects represent natural variability that we may wish to quantify and control for, but which is not directly influential on our scientific hypotheses. When we analyse experimental data using R or any other statistical package we can declare some effects to be fixed and others as random. If for example we carried out the same experiment at five different sites that were selected at random from a large set of possible locations we may treat site as random effect, as we are not interested in the specific effect of any single named site. However if we deliberately selected the sites as representing different conditions that we may be interested in then the effect of site would be fixed. When we include a variable in a model as a random effect we are only interested in the amount of variability, not the specific source of variability at the level of a single observation. So instead of looking at specfic effects we look at the variance or standard deviation of each random effect. These effects may be at a range of levels. For example we might look at the variability in yield between single plants and between fields. If no treatment has been assigned then in both cases we would only be interested in the variances. We usually fit “mixed effects” models however as we are typically interested in some form of fixed effect in addition to random variation. It can often be rather difficult to decide which effects should be treated as random, and when it is appropriate to do so. The good news is that in many situations it doesn’t really matter. The p-values and confidence intervals for the fixed effects that we are most interested in may not be influenced by the choice at all. The bad news is that in other cases the decision can be very important. Failure to declare a random effect as an error term may lead to type one errors, in other words, declaring a result to be statistically significant when it is not. In other situations, involving nested effects at different levels, the use of a mixed effects model is essential. Again, this may become clearer after looking at examples. 14.3 Types of design 14.3.1 Completely randomised design This is the simplest form of experiment. In this design, all levels of the treatment or all combinations of treatment levels are assigned to experimental units completely at random. Often one of the treatment levels is considered to be a control (i.e. no intervention). If all the factor levels involve some form of treatment one of them can be considered a reference level or baseline. There must be replication of each treatment level. The statistical model is a simple one way Anova. Let’s look at a simple experiment of this type. We’ll take an agricultural example. A researcher measured soil moisture at a certain depth following irrigation using four different techniques always using the same amount of water. The idea was to find out if there were any differences between them in order to find the most efficient technique in which less moisture was lost through evaporation. Let’s first assume that the moisture levels were measured once in each plot. The layout looks like this. alt text library(ggplot2) library(reshape) library(multcomp) d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/irrigation1.csv&quot;) str(d) ## &#39;data.frame&#39;: 16 obs. of 3 variables: ## $ plot : int 1 2 3 4 5 6 7 8 9 10 ... ## $ irrigation: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 1 1 1 2 2 2 2 3 3 ... ## $ moisture : num 12.3 12.6 11.4 12.1 12.4 ... 14.4 Visualising the data d$plot&lt;-factor(d$plot) d$irrigation&lt;-factor(d$irrigation) g0&lt;-ggplot(d,aes(x=irrigation,y=moisture)) g0+geom_point() Plotting the means with confidence intervals is a good way of identifying patterns that can be tested for significance. g1&lt;-g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;) g1+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) OK. So we can analyse the experiment using Anova as we have seen previously. mod&lt;-lm(moisture~irrigation,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: moisture ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## irrigation 3 10.6108 3.5369 6.4626 0.007504 ** ## Residuals 12 6.5675 0.5473 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Nothing new here. You should be able to interpret the table easily. 14.4.1 Comparisons The multcomp package provides a useful way of summarising the results by testing the general linear hypothesis for each treatment level. By default this will use the treatment that is lowest in alphabetical order as the “control” or baseline. plot(glht(mod)) summary(glht(mod)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Fit: lm(formula = moisture ~ irrigation, data = d) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) == 0 12.0750 0.3699 32.644 &lt;0.001 *** ## irrigationB == 0 0.7417 0.5231 1.418 0.418 ## irrigationC == 0 -1.0583 0.5231 -2.023 0.172 ## irrigationD == 0 1.0583 0.5231 2.023 0.172 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) So, although the anova can be reported as having detected significant variation between treatments (F 3,12 =6.4, p&lt;0.01) there are no significant differences between treatment A and the others. As no treatment was a natural control the best way to proceed in this case is to look at multiple comparisons to determine where the significant difference lie. We need to make an appropriate adjustment such as Tukey’s. plot(glht(mod, linfct = mcp(irrigation = &quot;Tukey&quot;))) summary(glht(mod, linfct = mcp(irrigation = &quot;Tukey&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: lm(formula = moisture ~ irrigation, data = d) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## B - A == 0 0.7417 0.5231 1.418 0.51255 ## C - A == 0 -1.0583 0.5231 -2.023 0.23295 ## D - A == 0 1.0583 0.5231 2.023 0.23316 ## C - B == 0 -1.8000 0.5231 -3.441 0.02196 * ## D - B == 0 0.3167 0.5231 0.605 0.92839 ## D - C == 0 2.1167 0.5231 4.046 0.00737 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) So it appears that treatment C leads to significantly lower soil water content than either B or D. This may be the type of irrigation method to reject an inefficient. The other methods were indistinguishable based on the results from this rather small study. A larger study would be needed to detect significant differences. 14.5 Completely randomised design with subsampling In this case, all levels of the treatment or all combinations of treatment levels are assigned to experimental units completely at random, but measurements are taken by sub sampling within the experimental unit. Ignoring the dependence that comes from sub sampling would lead to a form of pseudo-replication and this can result in falsely claiming significance (type one error). In the case of the previous example, it reality the researchers had taken three sub samples from each experimental unit. The previous analysis used the mean that had already been calculated from these three measurements. If the original data were used there may be a temptation to analyse the measurements as if they were all independent. This would be wrong. In fact the sub samples are nested within the experimental units. 14.5.1 Wrong analyisis for the subsampling Any form of sub sampling potentially can lead to type one errors (p-values too small) if it is not recognised for what it is. The reason for this is that the denominator degrees of freedom (the measure of the amount of independent replicatation) in the anova table is too large. Look carefully at the structure of this data frame. Each level of the factor now has three sub samples. However these are all measures of the same quantity. irrigation2&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/irrigation2.csv&quot;) d&lt;-irrigation2 str(d) ## &#39;data.frame&#39;: 48 obs. of 4 variables: ## $ irrigation: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ plot : int 1 1 1 2 2 2 3 3 3 4 ... ## $ subplot : int 1 2 3 1 2 3 1 2 3 1 ... ## $ moisture : num 12.6 11.9 12.3 13 12.4 12.4 11.3 11.9 10.9 12.5 ... This is what happens if we ignore the sub sampling and run an analysis using all the data. d$plot&lt;-factor(d$plot) mod&lt;-lm(moisture~irrigation,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: moisture ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## irrigation 3 31.832 10.6108 18.673 5.84e-08 *** ## Residuals 44 25.002 0.5682 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that this leads to a much smaller p-value. There are too many degrees of freedom in the denominator due to the pseudoreplication. We have already seen a good solution. Just take the means of each set of sub samples and analyse these as measures taken from each experimental unit. This is often by far the simplest approach. It is easy to understand and easy to explain to others. Another way of dealing with sub sampling is to account for it by adding a random effect for the experimental unit from which sub-samples are taken. There are many different ways of doing this in R. If use the aov wrapper to fit linear models for analysis of variance we declare an error term at the plot level and add this to the model formula. mod&lt;-aov(moisture~irrigation+Error(plot),data=d) summary(mod) ## ## Error: plot ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## irrigation 3 31.83 10.611 6.463 0.0075 ** ## Residuals 12 19.70 1.642 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 32 5.3 0.1656 You should see that the result is almost identical to that produced through analysing the means, as it should be. One point to be aware of is that when you have nested sub samples within a single level of the treatment, as occurs here, you can’t include plot as a fixed effect in the model. If you try to do this you will find that some coefficients cannot be estimated. mod&lt;-lm(moisture~irrigation+plot,data=d) summary(mod) ## ## Call: ## lm(formula = moisture ~ irrigation + plot, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93333 -0.26667 0.03333 0.26667 0.66667 ## ## Coefficients: (3 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.2667 0.2350 52.206 &lt; 2e-16 *** ## irrigationB 0.9667 0.3323 2.909 0.006542 ** ## irrigationC -1.2333 0.3323 -3.712 0.000782 *** ## irrigationD 0.9667 0.3323 2.909 0.006542 ** ## plot2 0.3333 0.3323 1.003 0.323319 ## plot3 -0.9000 0.3323 -2.708 0.010762 * ## plot4 -0.2000 0.3323 -0.602 0.551492 ## plot5 -0.8667 0.3323 -2.608 0.013719 * ## plot6 -1.3333 0.3323 -4.013 0.000338 *** ## plot7 0.5333 0.3323 1.605 0.118314 ## plot8 NA NA NA NA ## plot9 0.8667 0.3323 2.608 0.013719 * ## plot10 0.3667 0.3323 1.103 0.278059 ## plot11 -1.3000 0.3323 -3.912 0.000448 *** ## plot12 NA NA NA NA ## plot13 -0.6000 0.3323 -1.806 0.080388 . ## plot14 0.7000 0.3323 2.107 0.043084 * ## plot15 -0.5000 0.3323 -1.505 0.142206 ## plot16 NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.407 on 32 degrees of freedom ## Multiple R-squared: 0.9067, Adjusted R-squared: 0.863 ## F-statistic: 20.74 on 15 and 32 DF, p-value: 2.466e-12 The model can also be fit using either the lmer package or the older nlme package. The differences between the two are rather technical. In general terms nlme can be more convenient for mixed effects models which involve a response to a continuous variable, particularly if the response is non-linear. The newer lme4 package can fit some very complex model involving multi-layer interactions that nlme cannot. However by default lme4 does not provide p-values due to a deliberate decision made by its author. There is a highly technical academic argument regarding the validity of the calculations. Fortunately, as we will often do want to report p-values, the relevant calculations have been added by the author of the lmerTest package. library(lmerTest) mod&lt;-lmer(moisture~irrigation+(1|plot),data=d) anova(mod) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## irrigation 3.2111 1.0703 3 12 6.4625 0.007505 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The p-values are the same as would result from using the means of the sub samples in a one way anova. So there is no obvious advantage in using the raw data rather than pooled means. In many cases the simplest way to analyse the data is to just take the means of the sub samples and use those, as in the first example. The exception to this is if you are interested in variability within the sub samples. Notice that the mixed effect does provide you with an estimate of this. summary(mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: moisture ~ irrigation + (1 | plot) ## Data: d ## ## REML criterion at convergence: 83.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6115 -0.6833 0.1057 0.5153 1.3200 ## ## Random effects: ## Groups Name Variance Std.Dev. ## plot (Intercept) 0.4921 0.7015 ## Residual 0.1656 0.4070 ## Number of obs: 48, groups: plot, 16 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.0750 0.3699 11.9998 32.644 4.32e-13 *** ## irrigationB 0.7417 0.5231 11.9998 1.418 0.1817 ## irrigationC -1.0583 0.5231 11.9998 -2.023 0.0659 . ## irrigationD 1.0583 0.5231 11.9998 2.023 0.0659 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) irrgtB irrgtC ## irrigationB -0.707 ## irrigationC -0.707 0.500 ## irrigationD -0.707 0.500 0.500 We can use the glht function with the output from fitting this model as before, and reach an identical conclusion. plot(glht(mod, linfct = mcp(irrigation = &quot;Tukey&quot;))) summary(glht(mod, linfct = mcp(irrigation = &quot;Tukey&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: lmer(formula = moisture ~ irrigation + (1 | plot), data = d) ## ## Linear Hypotheses: ## Estimate Std. Error z value Pr(&gt;|z|) ## B - A == 0 0.7417 0.5231 1.418 0.48816 ## C - A == 0 -1.0583 0.5231 -2.023 0.17930 ## D - A == 0 1.0583 0.5231 2.023 0.17937 ## C - B == 0 -1.8000 0.5231 -3.441 0.00302 ** ## D - B == 0 0.3167 0.5231 0.605 0.93041 ## D - C == 0 2.1167 0.5231 4.046 &lt; 0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Always watch out for subsampling in any data set. Never treat subsamples as independent replicates 14.6 Randomized Complete Block Design Blocks occur when the variability between the response within sets of experimental units is expected to be lower than the variability between sets of units. The term block comes from agricultural experiments and is easiest to understand in this context. If different fields have different inherent fertility levels we would expect the yield of wheat from parcels placed within each field to be similar. However the yield would vary between fields. The idea of blocking is to account for this variability in the design of the experiment. This can increase statistical power and reduce the number of experimental units needed. Here is an example of a randomised complete block design. Four fields are chosen for an experiment. In each field four plots are selected and allocated a treatment level at random. Let’s say the treatment involves planting four different varieties of maize and recording the total yield. The fields are blocks. Each field may have a different soil type and thus a different intrinsic fertility. In the case of a blocked experiment we are not interested in the effect of the block. It is a confounding variable. However it adds to the variability between treatment levels and thus may make it harder to spot real differences between them. alt text Block effects need to be taken into account as if they are not the analysis may have too high a p-value (reduced power or type 2 error). The situation is rather complicated conceptually as we can only estimate the effects of the blocks. We don’t really know what they are. RCB&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/RCB.csv&quot;) d&lt;-RCB str(d) ## &#39;data.frame&#39;: 16 obs. of 3 variables: ## $ block: Factor w/ 4 levels &quot;b1&quot;,&quot;b2&quot;,&quot;b3&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ treat: Factor w/ 4 levels &quot;t1&quot;,&quot;t2&quot;,&quot;t3&quot;,..: 1 2 3 4 1 2 3 4 1 2 ... ## $ yield: num 18.1 22.6 21.5 30.8 26 ... 14.6.1 Wrong analysis ignoring the effect of block Let’s first look at all the sources of variability combined at the level of the treatments. g1&lt;-g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;) g1+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) We can fit a model ignoring the blocking effects. mod&lt;-lm(yield~treat,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 216.45 72.150 1.6991 0.2201 ## Residuals 12 509.56 42.463 Notice that this suggests that there is no significant effect of the treatments. 14.6.2 Treating block as a random effect This time we will fit a model with an error term at the block level using aov. mod&lt;-aov(yield~treat+Error(block),data=d) summary(mod) ## ## Error: block ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 3 446.1 148.7 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 216.45 72.15 10.23 0.00294 ** ## Residuals 9 63.49 7.05 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Again, the same result can be obtained using the more powerful lmerTest package. mod&lt;-lmer(yield~treat+(1|block),data=d) anova(mod) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## treat 216.45 72.15 3 9.0001 10.227 0.002941 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Just as previously we can look at the pattern of effects using the glht function from the multcomp package. par(mar=c(4,8,4,2)) plot(glht(mod)) par(mar=c(4,8,4,2)) plot(glht(mod, linfct = mcp(treat = &quot;Tukey&quot;))) 14.6.3 Treating block as a fixed effect If we treat block as fixed we obtain identical results for the treatment effects. It may be advantageous to treat the blocks as fixed if we wish to identify differences between them. For example one field may have a very different soil fertility to the others. As blocking effects confound detection of treatment effects even when we allow for them in the statistical model we might decide not to use this block in further trials in order to have a more homogeneous set of initial conditions. The difference between using a fixed and random effect in this situation is arbitrary and depends on circumstances. mod&lt;-aov(yield~treat+block,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 216.45 72.150 10.227 0.0029416 ** ## block 3 446.06 148.688 21.076 0.0002086 *** ## Residuals 9 63.49 7.055 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 216.5 72.15 10.23 0.002942 ** ## block 3 446.1 148.69 21.08 0.000209 *** ## Residuals 9 63.5 7.05 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 par(mar=c(4,8,4,2)) plot(glht(mod)) par(mar=c(4,8,4,2)) plot(glht(mod, linfct = mcp(treat = &quot;Tukey&quot;))) par(mar=c(4,8,4,2)) plot(glht(mod, linfct = mcp(block = &quot;Tukey&quot;))) 14.7 Illustation of how block effects work We could handle the block effect “by hand”. Each block of four treatments has a mean yield that we can calculate in R and add to our data frame. d$block_mean&lt;-rep(with(d,tapply(yield,block,mean)),each=4) Look at the results of this in the data table. Now we can subtract the block mean from the yields and re-plot our confidence intervals. d$yield2&lt;-d$yield-d$block_mean g0&lt;-ggplot(d,aes(x=treat,y=yield2)) g0+geom_boxplot() g1&lt;-g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;) g1+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) The pattern that we detected using the analysis which took into account blocks is now clearly shown. Now if we run an Anova you can see that the sum of squares and the mean squares are correct. The residual sum of squares is also correct. The difference is that the denominator degrees of freedom is too high as we have not compensated for the fact that the block means were estimated, by reducing the df by three, The mean squares in the denominator of the F Ratio is therefore slightly too low. Looking closely at the table should help you understand the logic. mod&lt;-lm(yield2~treat,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: yield2 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treat 3 216.451 72.150 13.636 0.0003583 *** ## Residuals 12 63.494 5.291 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Although this produces an identical result in terms of the sum of squares it is preferable to declare the block effect as either an additional additive fixed effect or a random effect in order to clearly show that the block effects are estimated by the model and thus lead to reduced degrees of freedom. 14.8 An observational example of sub-sampling A researcher wants to establish if algal mat cover has an effect on species richness. The mat cover is measured once but richness five times at each site. This is an examle of a regression with sub-sampling. The issue to be dealt with is the difference in the amount of replication of the measure of algal mat and the measure of richness. It can be handled either by taking means or by using a model with a random effect included to handle nested observations. set.seed(1) mat&lt;-rep(sample(1:100,10),each=5) mat ## [1] 27 27 27 27 27 37 37 37 37 37 57 57 57 57 57 89 89 89 89 89 20 20 20 20 20 ## [26] 86 86 86 86 86 97 97 97 97 97 62 62 62 62 62 58 58 58 58 58 6 6 6 6 6 site&lt;-rep(1:10,each=5) richness&lt;-round(30-0.1*mat+rnorm(50,0,4),0) richness ## [1] 24 29 30 30 26 32 28 24 17 31 24 24 28 28 27 25 24 21 13 24 28 27 22 26 30 ## [26] 27 21 23 21 16 19 19 20 25 23 23 23 27 26 21 21 26 27 24 28 31 27 31 25 35 plot(mat,richness) d&lt;-data.frame(site,mat,richness) 14.8.1 Plot the raw data g0&lt;-ggplot(d,aes(x=mat,y=richness)) g0+geom_point()+geom_smooth(method=&quot;lm&quot;) mod1&lt;-lm(richness~mat,data=d) summary(mod1) ## ## Call: ## lm(formula = richness ~ mat, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5530 -1.9363 0.3984 2.3292 5.6351 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.90919 1.01005 29.611 &lt; 2e-16 *** ## mat -0.09071 0.01645 -5.515 1.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.423 on 48 degrees of freedom ## Multiple R-squared: 0.3879, Adjusted R-squared: 0.3752 ## F-statistic: 30.42 on 1 and 48 DF, p-value: 1.366e-06 14.8.2 Group to take mean richness at each site with same algal coverage library (dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following object is masked from &#39;package:reshape&#39;: ## ## rename ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union d %&gt;% group_by(site,mat) %&gt;% summarise(richness=mean(richness)) -&gt;d2 g0&lt;-ggplot(d2,aes(x=mat,y=richness)) g0+geom_point()+geom_smooth(method=&quot;lm&quot;) mod2&lt;-lm(richness~mat,data=d2) summary(mod2) ## ## Call: ## lm(formula = richness ~ mat, data = d2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.49502 -0.39841 -0.03172 0.41128 1.46120 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.909188 0.549077 54.47 1.43e-11 *** ## mat -0.090708 0.008941 -10.15 7.62e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8322 on 8 degrees of freedom ## Multiple R-squared: 0.9279, Adjusted R-squared: 0.9189 ## F-statistic: 102.9 on 1 and 8 DF, p-value: 7.618e-06 14.8.3 Use raw data with a random effect for site library(nlme) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## The following object is masked from &#39;package:lme4&#39;: ## ## lmList library(merTools) ## Loading required package: arm ## ## arm (Version 1.10-1, built: 2018-4-12) ## Working directory is /home/rstudio/webpages/books/AQM_book library(ggeffects) mod3&lt;-lme4::lmer(richness~mat +(1|site),data=d) ## boundary (singular) fit: see ?isSingular fit&lt;-ggpredict(mod3, terms = &quot;mat&quot;) ggplot(fit, aes(x, predicted)) + geom_point(data=d,aes(y=richness,x=mat)) + geom_line() + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1) library(lmerTest) mod4&lt;-lmer(richness~mat+(1|site),data=d) ## boundary (singular) fit: see ?isSingular summary(mod4) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: richness ~ mat + (1 | site) ## Data: d ## ## REML criterion at convergence: 268.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.7906 -0.5656 0.1164 0.6804 1.6461 ## ## Random effects: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.00 0.000 ## Residual 11.72 3.423 ## Number of obs: 50, groups: site, 10 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 29.90919 1.01005 48.00000 29.611 &lt; 2e-16 *** ## mat -0.09071 0.01645 48.00000 -5.515 1.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## mat -0.878 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular anova(mod4) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## mat 356.48 356.48 1 48 30.419 1.366e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 14.9 Summary Including random effects can allow for subsampling which leads to non-independent repllication. It often will lead to similar results to the simpler technique of taking means of the subsamples. If the number of subsamples actually varies at each observational point then a random effects model is better than one using means, as the mean will be based on a different number of observations and may vary in reliability. A random effects model can take this into account. In other situations the result will be very similar. Blocking differs from subsampling as the treatment is repeated within the block (which it isn’t in a nested subsampling design). Blocks can be treated as either fixed or random effects. The result will be the same. Treat blocks as fixed effects if you are interested in them for some reason. Treat them as random effects if you never want to look at individual blocks as such. Watch out for subsampling which leads to different amounts of repication for each variable in observational studies! This often occurs and it can lead to erroneous conclusions. 14.10 Exercises A researcher is interested in whether dark corrugated iron strips are used more frequently by sand lizards than plain corrugated iron. The researcher has 20 pieces of iron of each type and places them on five different sites at Arne. The strips are inspected every day for two weeks in spring. The total number of sandlizards found under each strip is recorded each day as the response variable (data may also be collected on weather conditions etc.. but you can ignore this). Design a fake data set that could be used as the template for a simple analysis of the data using an appropriate analysis of variance. Run an analysis on the fake data. Answer the folowing questions. What feature of the design may be considered to lead to blocking? How many levels of the factor are there? How might subsampling be handled? Which feature of the response variable may cause particular difficulties when working with real life data? "],
["repeat-measures-designs.html", "Chapter 15 Repeat measures designs 15.1 Paired t-test 15.2 Mixed effects model 15.3 Repeat measures with subsampling", " Chapter 15 Repeat measures designs Repeated measures on the same experimental subject can occur for many reasons. The typical situation is when the measures are repeated in time, but repeated measures can also arise in other ways. The key to understanding when a design involves repeated measures is to realise that measures which involve different levels of the fixed factor of interest are taken from the same experimental unit. A blocked design is therefore an example of repeated measures in space if a block is regarded as an experimental unit. However blocks are usually thought of in terms of groups of experimental units. The terminology in the literature can be slightly confusing in this respect as it may depend on the discipline involved and blocked designs can be called repeated measures in some circumstances. Let’s think of a simple example of a repeated measurement in a laboratory setting. The blood pressure of ten rats was measured before and after the injection of a drug. library(lmerTest) library(ggplot2) library(effects) library(reshape) library(multcomp) library(dplyr) d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/rats.csv&quot;) Repeat measures such as these lead to a situation that is fundamentally similar to a design with blocks. Each treatment level is replicated once within each subject. However the total variability also has a between subject component as each rat may have a different baseline blood pressure. This needs to be taken into account. g0&lt;-ggplot(d,aes(x=time,y=resp)) g0+geom_boxplot() g1&lt;- g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;) g1+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) 15.1 Paired t-test Just as in the blocked design failure to take into account between subject variability reduces the power of the analysis. We can see this by running two t-tests. The first is unpaired. The second is paired. d2&lt;-melt(d,id=1:2,m=&quot;resp&quot;) d2&lt;-cast(d2,id~time) t.test(d2$Base,d2$Treat) ## ## Welch Two Sample t-test ## ## data: d2$Base and d2$Treat ## t = -1.2157, df = 16.68, p-value = 0.241 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -21.820632 5.881527 ## sample estimates: ## mean of x mean of y ## 80.32928 88.29883 t.test(d2$Base,d2$Treat,paired=TRUE) ## ## Paired t-test ## ## data: d2$Base and d2$Treat ## t = -4.3212, df = 9, p-value = 0.00193 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -12.141652 -3.797453 ## sample estimates: ## mean of the differences ## -7.969552 15.2 Mixed effects model The design can be analysed using a mixed effect model with rat id as a random effect. First let’s see what happens if we don’t include the random effect. mod&lt;-lm(resp~time,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: resp ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## time 1 317.6 317.57 1.478 0.2398 ## Residuals 18 3867.7 214.87 You should see that the p-value is the same as we got from the unpaired t-test. Now making rat id a random effect. mod&lt;-lmer(resp~time+(1|id),data=d) summary(mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: resp ~ time + (1 | id) ## Data: d ## ## REML criterion at convergence: 135.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.42938 -0.49183 0.06957 0.55204 1.08978 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 197.86 14.066 ## Residual 17.01 4.124 ## Number of obs: 20, groups: id, 10 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 80.329 4.635 9.740 17.329 1.21e-08 *** ## timeTreat 7.970 1.844 9.000 4.321 0.00193 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## timeTreat -0.199 par(mar=c(4,8,4,2)) plot(glht(mod)) Now the p-value is the same as that obtained by a paired t-test. Alternatively code this achieve the same result. mod&lt;-aov(resp~time+Error(id),data=d) summary(mod) ## ## Error: id ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 9 3715 412.7 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## time 1 317.6 317.6 18.67 0.00193 ** ## Residuals 9 153.1 17.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You should also be able to see the fundamental similarity between this situation and one involving blocking with respect to the specification of the model. However in a classic block design the blocks are considered to be groups of experimental units rather than individual experimental units. This can lead to confusion and model mispecification if you are not careful. A block design must have repeated measures with different levels of treatment within each block, just as a repeat measure design has different treatment levels within each subject. These simple examples can be extended to produce more complex designs. 15.3 Repeat measures with subsampling One way the repeat measures design can be extended is with sub sampling. Take this example. A researcher is interested in the impact of sewage treatment plants outfalls on the diversity of river invertebrates. In order to produce a paired design kick samples are taken 100m above the outfall and 100m below the outfall in five rivers. This leads to a total of ten sites. At each site three kicks are obtained. These three measurements are thus sub-samples. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/river.csv&quot;) g0&lt;-ggplot(d,aes(x=sewage,y=nspecies)) 15.3.1 Visualising the data If we pool all the data points taken below and above the sewage outfall we would not be taking into account the variability between rivers. We would also not be taking into account the potential for “pseudo-replication” as a result of the sub-sampling. g0+geom_boxplot() We can still obtain confidence intervals, but they would be based on a simplistic model that does not take into account the true data structure. g1&lt;-g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;)+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) g1 We could try splitting the data by river in order to visualise the pattern of differences. g1+facet_wrap(~id) 15.3.2 Incorrect model specification. We could think of river id as a fixed effect. This may be reasonable if we are interested in differences between rivers. However it does not take into account the fact that the data consists of sub samples at each site. So the model below would be wrong. mod&lt;-lm(nspecies~sewage+id,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: nspecies ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sewage 1 564.48 564.48 25.0596 9.454e-06 *** ## id 4 850.40 212.60 9.4382 1.344e-05 *** ## Residuals 44 991.12 22.53 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 15.3.3 Mixed effect model The data could be modelled as nested random effects. There is random variability between rivers and there is also random variability between sites at each river. The fixed effect that we are most interested in concerns whether the site is above or below the sewage outfall. mod&lt;-lmer(nspecies~sewage+(1|id/site),data=d) summary(mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: nspecies ~ sewage + (1 | id/site) ## Data: d ## ## REML criterion at convergence: 300.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.59547 -0.71758 -0.05326 0.60059 2.36669 ## ## Random effects: ## Groups Name Variance Std.Dev. ## site:id (Intercept) 2.694 1.641 ## id (Intercept) 17.789 4.218 ## Residual 21.300 4.615 ## Number of obs: 50, groups: site:id, 10; id, 5 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 33.560 2.225 5.272 15.086 1.54e-05 *** ## sewageBelow -6.720 1.668 4.001 -4.029 0.0157 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## sewageBelow -0.375 par(mar=c(4,8,4,2)) plot(glht(mod)) We could also treat river as a fixed effect if we are specifically interested in differences between rivers. mod&lt;-lmer(nspecies~sewage+id+(1|site),data=d) anova(mod) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## sewage 345.7 345.7 1 4 16.2300 0.01575 * ## id 520.8 130.2 4 4 6.1127 0.05374 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: nspecies ~ sewage + id + (1 | site) ## Data: d ## ## REML criterion at convergence: 275.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.54770 -0.70432 -0.04561 0.61090 2.41441 ## ## Random effects: ## Groups Name Variance Std.Dev. ## site (Intercept) 2.696 1.642 ## Residual 21.300 4.615 ## Number of obs: 50, groups: site, 10 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 31.360 2.043 4.000 15.350 0.000105 *** ## sewageBelow -6.720 1.668 4.000 -4.029 0.015751 * ## idriver_2 0.100 2.637 4.000 0.038 0.971572 ## idriver_3 3.700 2.637 4.000 1.403 0.233304 ## idriver_4 9.500 2.637 4.000 3.602 0.022718 * ## idriver_5 -2.300 2.637 4.000 -0.872 0.432392 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) swgBlw idrv_2 idrv_3 idrv_4 ## sewageBelow -0.408 ## idriver_2 -0.645 0.000 ## idriver_3 -0.645 0.000 0.500 ## idriver_4 -0.645 0.000 0.500 0.500 ## idriver_5 -0.645 0.000 0.500 0.500 0.500 par(mar=c(4,8,4,2)) plot(glht(mod)) Notice that both models provide the same p-value for the effect of the sewage outfalls. Using nested random effects produces an almost identical result to that which would be obtained by taking the means of the sub samples, as we have seen in previous examples. # dd&lt;-melt(d,m=&quot;nspecies&quot;) reshape method of aggregation # dd1&lt;-cast(dd,id+sewage~variable,mean) ## Dplyr method d %&gt;% group_by(id,sewage) %&gt;% summarise(nspecies=mean(nspecies)) -&gt;dd mod&lt;-lm(nspecies~id+sewage,data=dd) anova(mod) ## Analysis of Variance Table ## ## Response: nspecies ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## id 4 170.080 42.520 6.1127 0.05374 . ## sewage 1 112.896 112.896 16.2300 0.01575 * ## Residuals 4 27.824 6.956 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["factorial-designs.html", "Chapter 16 Factorial designs 16.1 Model fitting 16.2 Experiment with interactions 16.3 Full factorial with blocking 16.4 Split plot", " Chapter 16 Factorial designs Factorial designs refer to situations in which more than one factor is of interest. There are various reasons to use more than one factor in an experiment. In some cases it may save time and money to measure the effects of two factors simultaneously. This makes sense if the factors are additive, with no interactions. In other situations we may actually be interested in looking for interactions between two factors. Altering the level of one factor may alter the effect of another. Let’s look at an example. An experimenter was interested in the effect of irrigation on the yield of two varieties of wheat. The design involved assigning variety and irrigation treatment at random to 24 plots in order to have 6 replicates of each combination of treatment levels (v1:i1,v1:i2,v2:i1 and v2:i2) For the moment let’s ignore the possibilities of setting up the experiment within blocks and assume that the experimental units are uniform prior to treatment and are independent of each other. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/factorial.csv&quot;) We can look at the results as box plots by conditioning on one of the factors using facet wrapping. g0&lt;-ggplot(d,aes(x=irrigation,y=yield)) g0+geom_boxplot()+facet_wrap(~variety) We can also look at confidence intervals. g1&lt;-g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;) g1&lt;-g1+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) g1+facet_wrap(~variety) 16.1 Model fitting There are no random effects in this design, so a simple linear model can be used. Typing an asterix (*) in the model formula fits a model with main effects and an interaction. mod&lt;-lm(yield~irrigation*variety,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## irrigation 1 113.600 113.600 28.9293 2.897e-05 *** ## variety 1 208.128 208.128 53.0020 4.845e-07 *** ## irrigation:variety 1 0.001 0.001 0.0003 0.9854 ## Residuals 20 78.536 3.927 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod) ## ## Call: ## lm(formula = yield ~ irrigation * variety, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4064 -0.9752 0.4121 1.0838 2.7629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.18679 0.80899 12.592 5.78e-11 *** ## irrigationi2 4.33623 1.14409 3.790 0.00115 ** ## varietyv2 5.87464 1.14409 5.135 5.05e-05 *** ## irrigationi2:varietyv2 0.03003 1.61798 0.019 0.98538 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.982 on 20 degrees of freedom ## Multiple R-squared: 0.8038, Adjusted R-squared: 0.7744 ## F-statistic: 27.31 on 3 and 20 DF, p-value: 2.836e-07 You should be able to see that the interaction term in this case is not significant. The effects package allows us to look at the pattern of effects visually. e &lt;- Effect(c(&quot;irrigation&quot;,&quot;variety&quot;), mod) plot(e) You can see that the lines between the two levels of the factors follow the same slope but are moved up. This is indicative of additive effects. We could therefore fit a simpler model mod&lt;-lm(yield~irrigation+variety,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## irrigation 1 113.600 113.60 30.375 1.811e-05 *** ## variety 1 208.128 208.13 55.651 2.478e-07 *** ## Residuals 21 78.537 3.74 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod) ## ## Call: ## lm(formula = yield ~ irrigation + variety, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3989 -0.9827 0.4196 1.0876 2.7704 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.1793 0.6837 14.888 1.24e-12 *** ## irrigationi2 4.3512 0.7895 5.511 1.81e-05 *** ## varietyv2 5.8897 0.7895 7.460 2.48e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.934 on 21 degrees of freedom ## Multiple R-squared: 0.8038, Adjusted R-squared: 0.7851 ## F-statistic: 43.01 on 2 and 21 DF, p-value: 3.747e-08 16.2 Experiment with interactions Let’s look at data from the same experimental set up, but in this case there is an interaction. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/factorial2.csv&quot;) head(d) ## variety irrigation yield ## 1 v1 i1 8.747092 ## 2 v1 i2 7.367287 ## 3 v2 i1 13.328743 ## 4 v2 i2 23.190562 ## 5 v1 i1 10.659016 ## 6 v1 i2 5.359063 The pattern is apparent in the box plots. g0&lt;-ggplot(d,aes(x=irrigation,y=yield)) g0+geom_boxplot()+facet_wrap(~variety) g1&lt;-g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;) g1&lt;-g1+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) g1+facet_wrap(~variety) Now if we fit a model we will find a very significant interaction. mod&lt;-lm(yield~irrigation*variety,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## irrigation 1 0.74 0.74 0.1885 0.6688 ## variety 1 586.83 586.83 149.4427 9.789e-11 *** ## irrigation:variety 1 96.72 96.72 24.6313 7.484e-05 *** ## Residuals 20 78.54 3.93 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod) ## ## Call: ## lm(formula = yield ~ irrigation * variety, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4064 -0.9752 0.4121 1.0838 2.7629 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.187 0.809 12.592 5.78e-11 *** ## irrigationi2 -3.664 1.144 -3.202 0.00447 ** ## varietyv2 5.875 1.144 5.135 5.05e-05 *** ## irrigationi2:varietyv2 8.030 1.618 4.963 7.48e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.982 on 20 degrees of freedom ## Multiple R-squared: 0.897, Adjusted R-squared: 0.8816 ## F-statistic: 58.09 on 3 and 20 DF, p-value: 4.713e-10 The first variety does not respond positively to irrigation. In fact yield may be reduced through over watering. In this case the effects are not additive. They are conditional on the level of one of the factors. In this situation the main effects of irrigation or variety can be difficult to interpret as they represent the average effect taken over both levels of the other factor. e &lt;- Effect(c(&quot;irrigation&quot;,&quot;variety&quot;), mod) plot(e) Interactions are very common in many ecological situations. Although some textbooks on experimental design treat interactive effects as undesirable, finding interactions is an interesting result. In this case the discussion of the results could concentrate on finding an explanation for the difference in response between the two varieties. 16.3 Full factorial with blocking The previous example treated each experimental unit as independent. In many situations there is some spatial dependence between experimental units. alt text d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/factorial_block.csv&quot;) head(d) ## variety field field_effect irrigation yield ## 1 v1 field_1 -2.5058152 i1 8.469043 ## 2 v1 field_1 -2.5058152 i2 13.970834 ## 3 v2 field_1 -2.5058152 i1 13.645747 ## 4 v2 field_1 -2.5058152 i2 16.883408 ## 5 v1 field_2 0.7345733 i1 13.758136 ## 6 v1 field_2 0.7345733 i2 16.514260 Once again, the potential advantage of taking into account blocking is to increase the power of the analysis. If the intrinsic variability between blocks is contributing to the variability in the response of the experimental units then accounting for it in the statistical model will increase power. This can be seen in this example by first plotting the raw data. g0&lt;-ggplot(d,aes(x=irrigation,y=yield)) g0+geom_boxplot()+facet_wrap(~variety) g1&lt;-g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;) g1&lt;-g1+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) g1+facet_wrap(~variety) 16.3.1 Model not taking into account blocks If we don’t take into account the variability attributable to the blocks in the model we lose statistical power. mod&lt;-lm(yield~variety*irrigation,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## variety 1 135.24 135.238 6.6790 0.01772 * ## irrigation 1 87.57 87.567 4.3246 0.05064 . ## variety:irrigation 1 2.96 2.962 0.1463 0.70614 ## Residuals 20 404.97 20.248 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Effects plot. e &lt;- Effect(c(&quot;irrigation&quot;,&quot;variety&quot;), mod) plot(e) 16.3.2 Model with block as a random effect. Block can be fitted as a random effect using lmer. mod&lt;-lmer(yield~variety*irrigation+(1|field),data=d) summary(mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: yield ~ variety * irrigation + (1 | field) ## Data: d ## ## REML criterion at convergence: 103 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.98922 -0.49404 -0.04189 0.55337 1.34688 ## ## Random effects: ## Groups Name Variance Std.Dev. ## field (Intercept) 16.991 4.122 ## Residual 3.257 1.805 ## Number of obs: 24, groups: field, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 11.172 1.837 6.426 6.082 0.000695 *** ## varietyv2 4.045 1.042 15.000 3.882 0.001474 ** ## irrigationi2 3.118 1.042 15.000 2.992 0.009119 ** ## varietyv2:irrigationi2 1.405 1.474 15.000 0.954 0.355380 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) vrtyv2 irrgt2 ## varietyv2 -0.284 ## irrigation2 -0.284 0.500 ## vrtyv2:rrg2 0.201 -0.707 -0.707 Now notice how the effects plot shows more effects of the treatment. e &lt;- Effect(c(&quot;irrigation&quot;,&quot;variety&quot;), mod) plot(e) 16.4 Split plot In all the previous examples there have been alternatives available to the use of mixed models with random and fixed effects. In the case of situations involving sub sampling the dependence between sub samples can be dealt with by taking means for each experimental unit. In the cases in which variability occurs between blocks it is possible to include blocks in a model as a fixed effect. Repeat measures designs can use differences within subjects as responses. However there are some more complex situations in which the sum of squares attributed to the variability attributed to different treatments should be compared with different error terms. A classic example is the split plot design in agriculture. The split plot design arose simply as a form of reducing the cost and difficulty of setting up field experiments. When treating field plots it can sometimes be much easier to set up some treatments for larger areas than a smaller one. Imagine the case in the yield experiment we have been looking at in which whole fields are planted with single varieties of wheat and each field is split into two forms of irrigation treatment. In some respects you could argue that not much has changed from the block design. However if each main field has a different response then the error term for variety will have fewer degrees of freedom than that for irrigation as the same variety has been planted in each field which is then “split” into two levels of irrigation. So effectively there is less independent replication of variety than of irrigation. If this is not taken into account there may be an accusation of “pseudo replication” with respect to the effect of variety as the anlysis would potentially treat all subplots as independent replicates without taking into account the fact that they are nested within field. However this is a problem if there are shared effects at the field level. If it were possible to ensure as much independence between subplots within the same field as between subplots as a whole it wouldn’t matter. Here is a dataframe with results from the split plot experiment shown above. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/split_plot.csv&quot;) str(d) ## &#39;data.frame&#39;: 24 obs. of 5 variables: ## $ variety : Factor w/ 2 levels &quot;v1&quot;,&quot;v2&quot;: 1 1 2 2 1 1 2 2 1 1 ... ## $ field : Factor w/ 12 levels &quot;field_1&quot;,&quot;field_10&quot;,..: 1 1 5 5 6 6 7 7 8 8 ... ## $ field_effect: num -2.506 -2.506 0.735 0.735 -3.343 ... ## $ irrigation : Factor w/ 2 levels &quot;i1&quot;,&quot;i2&quot;: 1 2 1 2 1 2 1 2 1 2 ... ## $ yield : num 6.25 8.06 17.98 20.64 6.63 ... 16.4.1 Visualising the data We can look at the data as boxplots. g0&lt;-ggplot(d,aes(x=irrigation,y=yield)) g0+geom_boxplot()+facet_wrap(~variety) g1&lt;-g0+stat_summary(fun.data=mean_cl_normal,geom=&quot;point&quot;) g1&lt;- g1+stat_summary(fun.data=mean_cl_normal,geom=&quot;errorbar&quot;,colour=&quot;black&quot;) g1+facet_wrap(~variety) 16.4.2 Incorrect model The naive way of analysing the data would overlook the potential for a field effect. mod&lt;-lm(yield~variety*irrigation,data=d) anova(mod) ## Analysis of Variance Table ## ## Response: yield ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## variety 1 141.55 141.550 7.5656 0.01233 * ## irrigation 1 93.88 93.875 5.0175 0.03661 * ## variety:irrigation 1 3.06 3.059 0.1635 0.69027 ## Residuals 20 374.19 18.710 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 16.4.3 Correct model mod&lt;-lmer(yield~variety*irrigation+(1|field),data=d) anova(mod) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## variety 8.503 8.503 1 10 4.0100 0.07308 . ## irrigation 93.875 93.875 1 10 44.2701 5.684e-05 *** ## variety:irrigation 3.059 3.059 1 10 1.4424 0.25743 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: yield ~ variety * irrigation + (1 | field) ## Data: d ## ## REML criterion at convergence: 107.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.31217 -0.32739 0.08055 0.44805 1.21676 ## ## Random effects: ## Groups Name Variance Std.Dev. ## field (Intercept) 16.589 4.073 ## Residual 2.121 1.456 ## Number of obs: 24, groups: field, 12 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 11.2319 1.7659 11.1971 6.361 4.95e-05 *** ## varietyv2 5.5711 2.4973 11.1971 2.231 0.047051 * ## irrigationi2 4.6695 0.8407 10.0000 5.554 0.000243 *** ## varietyv2:irrigationi2 -1.4279 1.1890 10.0000 -1.201 0.257430 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) vrtyv2 irrgt2 ## varietyv2 -0.707 ## irrigation2 -0.238 0.168 ## vrtyv2:rrg2 0.168 -0.238 -0.707 par(mar=c(4,12,4,2)) plot(glht(mod)) mod&lt;-aov(yield~variety*irrigation+Error(field),data=d) summary(mod) ## ## Error: field ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## variety 1 141.5 141.6 4.01 0.0731 . ## Residuals 10 353.0 35.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## irrigation 1 93.88 93.88 44.270 5.68e-05 *** ## variety:irrigation 1 3.06 3.06 1.442 0.257 ## Residuals 10 21.21 2.12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["generalised-linear-models-1.html", "Chapter 17 Generalised linear models 17.1 Poisson regression 17.2 GGplot 17.3 Showing the results with logged y 17.4 Log link function explained 17.5 Comparing the results 17.6 Models with binomial errors 17.7 The logit link function 17.8 Exercises", " Chapter 17 Generalised linear models So far we have assumed throughout that the variability in our models takes an approximately normal form. This is the assumption used in the classical parametric statistical tests and in regression, ANOVA and ANCOVA. Violations of the assumption often lead to the adoption of simple non parametric tests instead of more informative model based procedures due to worries about not meeting the assumptions needed for parametric modelling. However the set of models, known as Generalised Linear Models (GLMs) can use any known distribution for the errors. These are very powerful techniques. They are not much more difficult to apply using R than the methods that you have already seen. However careful thought is required in order to find the correct form for the model. 17.1 Poisson regression Let’s look at the marine invertebrates data that we saw earlier. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/marineinverts.csv&quot;) str(d) ## &#39;data.frame&#39;: 45 obs. of 4 variables: ## $ richness: int 0 2 8 13 17 10 10 9 19 8 ... ## $ grain : num 450 370 192 194 197 ... ## $ height : num 2.255 0.865 1.19 -1.336 -1.334 ... ## $ salinity: num 27.1 27.1 29.6 29.4 29.6 29.4 29.4 29.6 29.6 29.6 ... Plotting species richness against grain size again. attach(d) plot(d$richness~d$grain) In the previous analysis we looked at how allowing the model to adopt a curved form led to a better fit. However the issue of the inappropriate use of the normal distribution to represent the error term was ignored. One way of thinking about the situation is to remember that the description of a regression line includes some statement about the errors. \\(y=a+bx+\\epsilon\\) where \\(\\epsilon=N(o,\\sigma^{2})\\) This equation should be able to describe the process that leads to each data point. The model has a deterministic component (the regression line) and a stochastic component (the error term). However when the points are counts a continuous error term is incorrect. Although the mean value (trend) does not have to be an integer value, the actual data values do. So the errors around the trend should be discrete. The poisson distribution can represent this. For any value of lambda (which is continuous) the probability distribution of values is discrete. The poisson distribution automatically builds in heterogeniety of variance as the variance of a poisson distribution is in fact equal to lambda. par(mfcol=c(2,2)) barplot(dpois(0:5,lambda=0.1),names=0:5,main=&quot;Lambda=0.1&quot;) barplot(dpois(0:5,lambda=0.5),names=0:5,main=&quot;Lambda=0.5&quot;) barplot(dpois(0:5,lambda=1),names=0:5,main=&quot;Lambda=1&quot;) barplot(dpois(0:5,lambda=2),names=0:5,main=&quot;Lambda=2&quot;) Let’s think of a regression line with poisson errors with a=0, and b=1. \\(y=a+bx+\\epsilon\\) where \\(\\epsilon=poisson(lambda=y)\\) Something interesting happens in this case. Lambda is a measure of the central tendency, but for most of the regression line no observations can actually take the value of lambda. A point can only fall on the line when lambda happens to be an integer. lambda&lt;-seq(0,10,length=200) plot(lambda,rpois(200,lambda),pch=21,bg=2) lines(lambda,lambda,lwd=2) abline(v=0:10,lty=2) This is the motive for fitting using maximum likelihood. A point that falls a long way away from the deterministic component of a model contributes more to the model’s deviance than one that is close. A model with a low total deviance has a higher likelihood than one with a high deviance. The probabilities (that contribute to the deviance) are determined from assumptions regarding the form of the stochastic component of the model. The normal distribution is only one form of determining these probabilities. There are many other possible distributions for the error term. So let’s fit the model again, this time using poisson regression. By default this uses a log link function. This is usually appropriate for count data that cannot fall below zero. In this case the logarithmic link function also deals nicely with the problem of curvilinearity of the response. mod1&lt;-glm(data=d,richness ~ grain,family=poisson) summary(mod1) ## ## Call: ## glm(formula = richness ~ grain, family = poisson, data = d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4828 -1.3897 -0.0732 0.8644 2.5838 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.238393 0.299033 14.174 &lt; 2e-16 *** ## grain -0.009496 0.001179 -8.052 8.16e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 179.75 on 44 degrees of freedom ## Residual deviance: 105.35 on 43 degrees of freedom ## AIC: 251.35 ## ## Number of Fisher Scoring iterations: 5 confint(mod1) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 3.65451714 4.827458978 ## grain -0.01185141 -0.007224939 Plotting the model shows it’s form. Note that with when fitting a GLM in R we can ask for the standard errors and produce approximate confidence intervals using them. plot(d$richness ~ d$grain) x&lt;-seq(min(d$grain),max(d$grain),length=100) a&lt;-predict(mod1,newdata=list(grain=x),type=&quot;response&quot;,se=T) lines(x,a$fit-2*a$se.fit,lty=2) lines(x,a$fit+2*a$se.fit,lty=2) lines(x,a$fit) 17.2 GGplot Its easy to add a glm to a ggplot scatterplot. However be careful to add in the methods.args. library(ggplot2) g0&lt;-ggplot(d,aes(x=grain,y=richness)) glm1&lt;-g0+geom_point()+stat_smooth(method=&quot;glm&quot;,method.args=list( family=&quot;poisson&quot;), se=TRUE) +ggtitle(&quot;Poisson regression with log link function&quot;) glm1 17.3 Showing the results with logged y This is not a good approach, as the zeros are lost, but it demonstrates the idea. plot(d$richness ~d$grain, log=&quot;y&quot;) ## Warning in xy.coords(x, y, xlabel, ylabel, log): 3 y values &lt;= 0 omitted from ## logarithmic plot x&lt;-seq(min(grain),max(grain),length=100) a&lt;-predict(mod1,newdata=list(grain=x),type=&quot;response&quot;,se=T) lines(x,a$fit-2*a$se.fit,lty=2) lines(x,a$fit+2*a$se.fit,lty=2) lines(x,a$fit) 17.4 Log link function explained The coefficients of the model when we ask for a summary are rather hard to undertand. summary(mod1) ## ## Call: ## glm(formula = richness ~ grain, family = poisson, data = d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4828 -1.3897 -0.0732 0.8644 2.5838 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.238393 0.299033 14.174 &lt; 2e-16 *** ## grain -0.009496 0.001179 -8.052 8.16e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 179.75 on 44 degrees of freedom ## Residual deviance: 105.35 on 43 degrees of freedom ## AIC: 251.35 ## ## Number of Fisher Scoring iterations: 5 The slope is given as -0.009. What does this mean? Unlike a regeression slope it is NOT the change in y for a unit change in x, as we are using a logarithmic link function. In generalized linear models, there is always some sort of link function, which is the link between the mean of Y on the left and the predictor variable on the right. It is possible to use the identity link, which leaves the result the same, but typically some other link function is used. The identity link is not a good choice for data with many zeros. Formally the link function is .. \\(f(y|x ) = a + bx\\) I.e. Some function of the conditional value of y on x, ignoring the residual error, is predicted by a regression equation rather than simply y. The log link function exponentiates the linear predictors. It does not actually involve a log transform of the outcome variable. \\(y = exp(a + bx)\\) Which could be also written as .. \\(y = e^{a +bx}\\) As the logarithm used is the natural logarithm this implies that expected value of y is multiplied by \\(exp(b)\\) as we increase the value of x by 1 unit. This is not intuitive. Exponentiating the coefficients in R for the model above produces this result.. exp(coef(mod1)) ## (Intercept) grain ## 69.2963902 0.9905485 So, the intercept,for a grain size of zero is 69.3 and for each unit increase in grain size the diversity is changed by 99.055 % of the previous value. This is a process of exponential decay, as the richness is falling away steadily with each unti increase in grain size, but the model never leads to a predicted species richness below zero. One way to make all this a little more understandable is to divide the natural logarithm of 2 (0.69) by the raw slope coefficient, which was found to be -0.009. log(2)/(coef(mod1)[2]) ## grain ## -72.99001 This is using the formula for the half life, or doubling time, in an expenential decay or growth model. So, in order to double the expected species richness we therefore would have to change the grain size by -72.99 units. When presenting the results to a mathematically sophisticated audience you can safely place the coefficients within the equation and expect the audience to make sense of it. \\(y = e^{a +bx}\\) When explaining the result in words you can say that a change in grain size of -72.99 leads to doubling of expected species richness. Showing a scatterplot with the fitted line is usually the easiest way to visualise the model and to make sense of it intuitively. 17.4.1 Likelihood and deviance In order to fully understand all the elements used when analysing a GLM we also need at least an intuitive understanding of the concepts of likelihood and deviance. Models are fit by maximising the likelihood. But, what is the likelihood? To try to inderstand this, let’s first obtain some simple count data. We can simulate the counts from a poisson distribution. set.seed(1) x&lt;-rpois(10,lambda=2) x ## [1] 1 1 2 4 1 4 4 2 2 0 barplot(table(x)) We can fit a simple model that just involves the intercept (mean) using R. This is. mod&lt;-glm(x~1,poisson(link=&quot;identity&quot;)) summary(mod) ## ## Call: ## glm(formula = x ~ 1, family = poisson(link = &quot;identity&quot;)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.04939 -0.84624 -0.06957 0.85560 1.16398 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1000 0.4583 4.583 4.59e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 10.427 on 9 degrees of freedom ## Residual deviance: 10.427 on 9 degrees of freedom ## AIC: 36.066 ## ## Number of Fisher Scoring iterations: 3 coef(mod) ## (Intercept) ## 2.1 confint(mod) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## 1.325155 3.130620 lam&lt;-coef(mod) Now, under the poisson model we can calculate a probability of getting any integer from a poisson distribution with a mean of lambda using a standard formula that is built into R. So the probability of getting a zero is dpois(0,lambda=2.1) dpois(0,lambda=2.1)=0.122 dpois(1,lambda=2.1)=0.257 dpois(2,lambda=2.1)= 0.27 dpois(3,lambda=2.1)=0.189 dpois(4,lambda=2.1)=0.099 What this means is that we have a probability (likelihood) for each of the data points given the model parameter (lambda). We can look at this as a barplot of counts of each probability value. dx&lt;-dpois(x,lambda=lam) dx ## [1] 0.25715850 0.25715850 0.27001642 0.09923104 0.25715850 0.09923104 ## [7] 0.09923104 0.27001642 0.27001642 0.12245643 barplot(table(round(dx,3))) The probability of getting EXACTLY the data that we have is the product of all these probabilities, as we find the combined probability of independent events by multiplying them together. Because this is going to result in very small numbers it is usually easier to work with logarithmns and add them together. Hence the term log likelihood that you will see used in all treatments of GLMs. loglik&lt;-sum(log(dx)) loglik ## [1] -17.03292 logLik(mod) ## &#39;log Lik.&#39; -17.03292 (df=1) OK, so that was not too difficult. Notice as well that this calculation gave us the maximum likelihood. If we had used any other value as an estimate for lambda we would have got a lower value expressed as a negative value. sum(log(dpois(x,lambda=1))) ## [1] -21.6136 sum(log(dpois(x,lambda=lam))) ## [1] -17.03292 sum(log(dpois(x,lambda=3))) ## [1] -18.54274 In order to simplify matters further we remove the sign and work with -2 log likelihood. -2*sum(log(dpois(x,lambda=lam))) ## [1] 34.06584 The AIC which we will look at later in the course as a way of comparing two models combines the -2 log likelihood with the number of parameters (k). In this case we have just one parameter so AIC adds 2 to the number we previously calculated. AIC=2k-2ln(L) AIC(mod) ## [1] 36.06584 Now finally, what does the deviance refer to? Well, even a model which has a separate parameter for each data point will still have a likelihood below one. The deviance refers to the difference in -2 log likelihood between the fully saturated model and the actual model. We can get the -2 log likelihood for this model as well. dpois(x,lambda=x) ## [1] 0.3678794 0.3678794 0.2706706 0.1953668 0.3678794 0.1953668 0.1953668 ## [8] 0.2706706 0.2706706 1.0000000 satmod&lt;--2*sum(log(dpois(x,lambda=x))) satmod ## [1] 23.63838 Just to confirm, this should give us the deviance. -2*loglik-satmod ## [1] 10.42746 deviance(mod) ## [1] 10.42746 Notice that we had ten data points (residual degrees of freedom = n-1 = 9) and a residual deviance that is around 10. This is an indication that the assumption of Poisson distributed residuals is a reasonable one as for mathematical reasons that we need not go into we would expect an addition of just under 1 to the deviance for each additional data point. Going back to the summary of the model summary(mod) ## ## Call: ## glm(formula = x ~ 1, family = poisson(link = &quot;identity&quot;)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.04939 -0.84624 -0.06957 0.85560 1.16398 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1000 0.4583 4.583 4.59e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 10.427 on 9 degrees of freedom ## Residual deviance: 10.427 on 9 degrees of freedom ## AIC: 36.066 ## ## Number of Fisher Scoring iterations: 3 We can see that in this artificial case the null deviance and the residual deviance are identical. This is because the null is “true”. There is nothing to report in the model apart from the intercept, i.e. a single value for lambda. If we use this concept in a model with a predictor variable we should see a difference between these two numbers. The larger the diffence, the more of the deviance is “explained” by our predictor. We want to reduce the deviance bt fitting a model, so if there is a relationship the residual deviance should always be lower than the null deviance. 17.4.1.1 Overdispersion If the residual deviance is larger than residual degrees of freedom we have overdispersion (extra, unexplained variation in the response). summary(mod1) ## ## Call: ## glm(formula = richness ~ grain, family = poisson, data = d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4828 -1.3897 -0.0732 0.8644 2.5838 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.238393 0.299033 14.174 &lt; 2e-16 *** ## grain -0.009496 0.001179 -8.052 8.16e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 179.75 on 44 degrees of freedom ## Residual deviance: 105.35 on 43 degrees of freedom ## AIC: 251.35 ## ## Number of Fisher Scoring iterations: 5 This means that in fact the measured variance in the data, after taking into account the regression line, is still larger than the lambda values over the range of the regression. This is extra variability that goes beyond that expected under the assumption that the residuals are poisson distributed. This is the diagnostic tool which is used in Poisson regression. The point is that under a poisson distribution the variance is fixed. It is always identical to the mean (lamda). This may not be a reasonable assumption, but it is the assumption being made. If it is not met we will need to make some compensation for this in order to produce a more justifiable model. 17.4.1.2 Quasi-poisson regression A simple way of dealing with over dispersion is to use so called quasi-poisson regression. This finds a weight so that instead of assuming that the variance is equal to lambda the assumption is made that it is equal to some multiple of lambda. The multiple is estimated from the data. The effect is to reduce the significance of the regression term and widen the confidence intervals. It is a rather outdated technique that has some problems, but we’ll try it anyway. mod2&lt;-glm(data=d,richness ~ grain,family=quasipoisson) summary(mod2) ## ## Call: ## glm(formula = richness ~ grain, family = quasipoisson, data = d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4828 -1.3897 -0.0732 0.8644 2.5838 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.238393 0.441806 9.593 3.00e-12 *** ## grain -0.009496 0.001743 -5.450 2.29e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 2.182862) ## ## Null deviance: 179.75 on 44 degrees of freedom ## Residual deviance: 105.35 on 43 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 AIC(mod2) ## [1] NA confint(mod2) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 3.376924 5.11119746 ## grain -0.013008 -0.00616704 Notice that the confidence intervals are wider. However we cannot obtain a value for AIC from a quasi model as the likelihood function is not fully defined. This limits the application of quasi poisson models, so we’ll pass on quickly to a rather more useful approach.. glm2&lt;-g0+geom_point()+geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;quasipoisson&quot;), se=TRUE) + ggtitle(&quot;Quasipoisson regression&quot;) glm2 17.4.2 Negative binomial regression As we have seen, there is a problem with quasi poisson regression.There is no defined form for the likelihood. Therefore it is impossible to calculate AIC. This makes it difficult to run model comparisons using quasi poisson models. An alternative is to fit the model assuming a negative binomial distribution for the error terms. This is a well defined model for over dispersed count data. library(MASS) mod3&lt;-glm.nb(data=d,richness ~ grain) summary(mod3) ## ## Call: ## glm.nb(formula = richness ~ grain, data = d, init.theta = 4.008462461, ## link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7113 -1.0326 -0.1109 0.5508 1.5622 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.886804 0.467175 8.320 &lt; 2e-16 *** ## grain -0.008155 0.001713 -4.762 1.92e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(4.0085) family taken to be 1) ## ## Null deviance: 79.302 on 44 degrees of freedom ## Residual deviance: 51.719 on 43 degrees of freedom ## AIC: 235.37 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 4.01 ## Std. Err.: 1.66 ## ## 2 x log-likelihood: -229.37 confint(mod3) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 3.04704214 4.753670460 ## grain -0.01133949 -0.005064891 Notice that the AIC for the negative binomial model is much lower than that for the (incorrect) poisson model. The residual deviance is now not much larger than the residual degrees of freedom. It is very important to include the overdispersion rather than use the assumption that the variance is equal to lambda that is built into poisson regression. AIC(mod1) ## [1] 251.3523 AIC(mod3) ## [1] 235.3695 The variance of the negative binomial is \\(var=\\mu+\\frac{\\mu^{2}}{\\theta}\\) So theta controls the excess variability compared to Poisson. The smaller the value of theta the more skewed the distribution becomes. par(mfcol=c(2,2)) hist(rnegbin(n=10000,mu=10,theta=100),main=&quot;Theta=100&quot;,col=&quot;grey&quot;) hist(rnegbin(n=10000,mu=10,theta=10),main=&quot;Theta=10&quot;,col=&quot;grey&quot;) hist(rnegbin(n=10000,mu=10,theta=1),main=&quot;Theta=1&quot;,col=&quot;grey&quot;) hist(rnegbin(n=10000,mu=10,theta=0.1),main=&quot;Theta=0.1&quot;,col=&quot;grey&quot;) Plotting the model produces a very similar result to that shown by the quasipoisson model. glm3&lt;-g0+geom_point()+geom_smooth(method=&quot;glm.nb&quot;, se=TRUE) +ggtitle(&quot;Negative binomial regression&quot;) glm3 17.5 Comparing the results library(ggplot2) g0&lt;-ggplot(d,aes(x=grain,y=richness)) glm1&lt;-g0+geom_point()+stat_smooth(method=&quot;glm&quot;,method.args=list( family=&quot;poisson&quot;), se=TRUE) glm3&lt;-glm1+geom_point()+geom_smooth(method=&quot;glm.nb&quot;, se=TRUE,color=&quot;red&quot;) glm3 library(pscl) ## Classes and Methods for R developed in the ## Political Science Computational Laboratory ## Department of Political Science ## Stanford University ## Simon Jackman ## hurdle and zeroinfl functions by Achim Zeileis modh&lt;-hurdle(d$richness~grain,dist=&quot;negbin&quot;) summary(modh) ## ## Call: ## hurdle(formula = d$richness ~ grain, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.5804 -0.8495 -0.1085 0.6236 2.0657 ## ## Count model coefficients (truncated negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.915924 0.458802 8.535 &lt; 2e-16 *** ## grain -0.008220 0.001724 -4.767 1.87e-06 *** ## Log(theta) 1.510256 0.510987 2.956 0.00312 ** ## Zero hurdle model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.34139 3.39460 2.163 0.0306 * ## grain -0.01531 0.01005 -1.524 0.1275 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta: count = 4.5279 ## Number of iterations in BFGS optimization: 13 ## Log-likelihood: -114.5 on 5 Df AIC(modh) ## [1] 238.9305 AIC(mod3) ## [1] 235.3695 confint(modh) ## 2.5 % 97.5 % ## count_(Intercept) 3.01668827 4.815159689 ## count_grain -0.01159973 -0.004840422 ## zero_(Intercept) 0.68811013 13.994679574 ## zero_grain -0.03500187 0.004381010 modzi &lt;- zeroinfl(data=d,richness~grain,dist=&quot;negbin&quot;) summary(modzi) ## ## Call: ## zeroinfl(formula = richness ~ grain, data = d, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.5740 -0.8408 -0.1085 0.6161 2.0427 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.896314 0.446411 8.728 &lt; 2e-16 *** ## grain -0.008132 0.001659 -4.902 9.49e-07 *** ## Log(theta) 1.514259 0.514694 2.942 0.00326 ** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.37854 11.19100 -0.481 0.631 ## grain 0.00447 0.03878 0.115 0.908 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 4.5461 ## Number of iterations in BFGS optimization: 35 ## Log-likelihood: -114.6 on 5 Df AIC(modh) ## [1] 238.9305 AIC(modzi) ## [1] 239.1852 AIC(mod3) ## [1] 235.3695 17.6 Models with binomial errors The most commonly used GL is probably logistic regression. In this particular model the response can only take values of zero or one. Thus it is clear from the outset that errors cannot be normal. Let’s set up a simple simulated data set to show how this works. Imagine we are interested in mortality of pine trees following a ground fire. We might assume that the population of tree diameters are log normally distributed with a mean of twenty. set.seed(1) diam&lt;-sort(rlnorm(500,mean=log(20),sd=0.5)) summary(diam) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.445 14.660 19.636 23.018 28.079 134.407 hist(diam,col=&quot;grey&quot;,breaks=10) Let’s simulate some response data based on an extremely simple underlying pattern for tree mortality. We might assume that trees with diameters of over 40 cm have bark that has reached a thickness that prevents the tree being killed by the fire. We might also assume a simple linear relationship between diameter and mortality up to this threshold and build a simple rule based vector of the probability that a tree survives the fire as a function of its diameter. p&lt;-diam/50 p[p&gt;1]&lt;-1 plot(diam,p,ylab=&quot;Survival probability&quot;,xlab=&quot;Diameter&quot;,type=&quot;l&quot;,lwd=3) Although we have a very simple underlying deterministic model, we will not see this directly when we collect data. Any individual tree will be either alive or dead. Thus our response will be zeros and ones. This is the problem that logistic regression deals with very neatly without the need to calculate proportions explicitly. f&lt;-function(x)rbinom(1,1,x) response&lt;-as.vector(sapply(p,f)) head(response) ## [1] 0 0 0 1 0 0 d&lt;-data.frame(diam,response) plot(diam,response) lines(diam,p,lwd=3) The task for the statistical model is to take this input and turn it back into a response model. Generalised linear models do this using a link function. In R it is very easy to specify the model. We simply write a model using the same syntax as for a linear model (one with gaussian errors) but we state the family of models we wish to use as binomial. mod1&lt;-glm(response~diam,family=&quot;binomial&quot;) summary(mod1) ## ## Call: ## glm(formula = response ~ diam, family = &quot;binomial&quot;) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8202 -0.8891 -0.6053 1.0175 2.0428 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.60771 0.28033 -9.302 &lt;2e-16 *** ## diam 0.10869 0.01217 8.929 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 688.91 on 499 degrees of freedom ## Residual deviance: 565.51 on 498 degrees of freedom ## AIC: 569.51 ## ## Number of Fisher Scoring iterations: 5 We can see that R does find a model that matches the underlying pattern very well by using the model for prediction. Again we visualise the model in order to understand it. This is always preferable to trying to understand a model from a table of numbers. Visualisation is particularly important for models with parameters expressed on a logit scale as this is not intuitive. g0 &lt;- ggplot(d,aes(x=diam,y=response)) g1&lt;-g0+geom_point()+stat_smooth(method=&quot;glm&quot;,method.args=list(family=&quot;binomial&quot;)) g1 If we wanted to check whether there was a response shape that differed from that assumed by the general linear model we could try a general additive model with a smoother. library(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-24. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. g1&lt;-g0+geom_point()+stat_smooth(method=&quot;gam&quot;,method.args=list(family=&quot;binomial&quot;),formula=y~s(x)) g1 The curve is very similar. Note that as the smoother uses a form of “local” regression the confidence intervals expand in areas where there is little data. In some cases the response would take a different form. This could happen if there were some optimum point at which some response occurred, for example the occurence of a species along an altitudinal gradient or shoreline. In this case the gam model would fit the data better than the linear model. We will look at how this can be tested formally later. A quick test is to calculate the AIC. If this is much lower for the gam it indicates that the gam may be a better fit. glm_mod&lt;-glm(data=d, response~diam, family=binomial) gam_mod&lt;-gam(data=d, response~s(diam), family=binomial) AIC(glm_mod) ## [1] 569.5078 AIC(gam_mod) ## [1] 566.4594 In this case it is very slightly lower, but not enough to suggest the use of a gam. 17.7 The logit link function The logit link function used in binomial glms makes the slope of the line quite difficult to understand. In most cases this doesn’t matter much, as you can concentrate on the sign and signficance of the parameter and show the line as a figure. However when analysing differences in response as a function of levels of a factor you do need to understand the logit link. To illustrate let’s take a very simple example. Ten leaves are classified as being taken from shade or sun and classified for presence of rust. library(purrr) set.seed(1) light&lt;-rep(c(&quot;shade&quot;,&quot;sun&quot;),each=10) presence&lt;-1*c(rbernoulli(10,p=0.5),rbernoulli(10,p=0.1)) d&lt;-data.frame(light,presence) We can get a table of the results easily. table(d) ## presence ## light 0 1 ## shade 4 6 ## sun 9 1 So 6 of the leaves in the shade had rust present and 4 did not. The odds of rust are therefore 6 to 4. Odds are used in the logit transform rather than simple proportions because odds can take values between 0 and infinity, while proportions are bounded to lie between zero and one. Taking the logarithm of the odds leads to an additive model. There are two factor levels, shade and sun. The default reference when a model is fitted will be the factor that is first in alphabetical order, i.e. shade. So after fitting a model the intercept will be the log of the odds in the shade. The effect of light will be the log odds in the sun minus the log odds in the shade. odds_shade&lt;-6/4 odds_sun&lt;-1/9 log(odds_shade) ## [1] 0.4054651 log(odds_sun)-log(odds_shade) ## [1] -2.60269 We can see that this coincides with the model output. mod&lt;-glm(data=d,presence~light,family=&quot;binomial&quot;) summary(mod) ## ## Call: ## glm(formula = presence ~ light, family = &quot;binomial&quot;, data = d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.354 -0.459 -0.459 1.011 2.146 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.4055 0.6455 0.628 0.5299 ## lightsun -2.6027 1.2360 -2.106 0.0352 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 25.898 on 19 degrees of freedom ## Residual deviance: 19.962 on 18 degrees of freedom ## AIC: 23.962 ## ## Number of Fisher Scoring iterations: 4 If the coeficients are exponentiated then the first coeficient represents the baseline odds and the second coeficient represesnts this value divided by the odds for the “treatment”. As binomial models are often used in epidemiology this explains why we could hear statements such as “eating processed meat increases the odds of contracting bowel cancer by a factor of 2”. This is a literal interpretation of the exponentiated coeficient. exp(coef(mod)) ## (Intercept) lightsun ## 1.50000000 0.07407408 odds_shade ## [1] 1.5 odds_sun/odds_shade ## [1] 0.07407407 To convert the odds into proportions divide the odds by 1 plus the odds. odds_shade/(1+odds_shade) ## [1] 0.6 odds_sun/(1+odds_sun) ## [1] 0.1 So this gives the proportions as estimated by the model. exp(coef(mod)[1])/(1+exp(coef(mod)[1])) ## (Intercept) ## 0.6 exp(coef(mod)[1] + coef(mod)[2])/(1+exp(coef(mod)[1] + coef(mod)[2])) ## (Intercept) ## 0.1 17.8 Exercises GLMS can also be used when the explanatory variable is a factor. Here is a very simple data set that consists of counts of ragworm in two types of substrate, classified simply into mud and sand. Analyse the data using both a general linear model and a generalised linear model. Comment on the differences between the two aproaches. d&lt;-read.csv(&quot;/home/aqm/course/data/HedisteCounts.csv&quot;) Binomial (prensence/absence) model In some cases the actual numbers of organisms counted can be a poor choice of response variable. If organisms are highly aggregated then presence vs absence is a better choice. Reanalyse the ragworm data, this time using presence as the response. d$pres&lt;-1*(d$Count&gt;0) ## This sets up a variable consisting of ones and zeros Leafminers and leaf exposure to light The number of leaf miners were counted on 200 leaves exposed to different levels of ambient light, measured as a percentage of full exposure. Analyse these data using an appropriate GLM. d&lt;-read.csv(&quot;/home/aqm/course/data/leafminers.csv&quot;) plot(d) library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout g0&lt;-ggplot(d,aes(x=light,y=nminers)) glm1&lt;-g0+geom_point()+stat_smooth(method=&quot;glm&quot;,method.args=list( family=&quot;poisson&quot;), se=TRUE) +ggtitle(&quot;Poisson regression with log link function&quot;) ggplotly(glm1) mod&lt;-glm(data=d,nminers~light,family=&quot;poisson&quot;) summary(mod) ## ## Call: ## glm(formula = nminers ~ light, family = &quot;poisson&quot;, data = d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0888 -1.7275 -1.1676 0.0864 5.9691 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.36721 1.09865 -4.885 1.03e-06 *** ## light 0.06610 0.01203 5.496 3.88e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 751.31 on 199 degrees of freedom ## Residual deviance: 720.43 on 198 degrees of freedom ## AIC: 1025.3 ## ## Number of Fisher Scoring iterations: 6 log(2)/coef(mod)[2] ## light ## 10.48647 "],
["modelling-with-multiple-variables.html", "Chapter 18 Modelling with multiple variables 18.1 Introduction 18.2 Example data 18.3 Muliple regression 18.4 Collinearity 18.5 Model selection 18.6 Generalised Additive Models 18.7 Tree models 18.8 Which technique to use? 18.9 References", " Chapter 18 Modelling with multiple variables 18.1 Introduction When looking at simple regression models we have used a single numerical variable to explain and/or predict the variability in a second variable. Analysis of co-variance, uses one numerical variable and a factor. What if we have two or more numerical variables that could explain and/or predict some variable that we are interested in. Can we build models from them? From a mathematical perspective using more variables in the model is straightforward. However as we have seen, the complex nature of ecological data has to be considered carefully. Multiple regression implies all the same assumptions as regression. Simple intepretation of multiple regression models also relies on an aditional assumption. That is that the explanatory variables do not display a high degree of multiple co-linearity. In other words they should not be correlated with each other. This is rarely the case in ecology. Multiple co-linearity does not prevent the use of such models, but it does raise some very tricky issues. 18.2 Example data The effect of fragmentation of habitat as a result of human activities is a common theme in ecology. We will look at an example presented by Zuur et al (2010). Forest bird abundances expressed as an index (details of how this was measured are not given) were observed in 56 forest patches in south-eastern Victoria, Australia. The aim of the study was to relate the index of forest bird abundance to six habitat variables; size of the forest patch, distance to the nearest patch, distance to the nearest larger patch, mean altitude of the patch, year of isolation by clearing, and an index of stock grazing history (1 = light, 5 = intensive). Zuur’s analysis is given in the appendix of the book. In our analysis the grazing index will be treated as a numerical variable on an ordinal scale. Zuur treats it as a factor. This does not alter the main conclusions of the analysis, and helps to clarify and illustrate some addional issues. d&lt;-read.csv(&quot;https://tinyurl.com/aqm-data/loyn.csv&quot;) 18.3 Muliple regression The three steps in building a regression model with many explanatory variables are. Look at the distributions of the explanatory and response variables with particular attention to influential outliers. Look for collinearity of the explanatory variables. Investigate the relationship between the response variable and the explanatory variables 18.3.1 Analysis of the distribution of variability The first step in any analysis involving regression is to investigate the distribution of each variable and look for potential issues with influential outliers. Rembember that the assumption of normality in regression aplies to the residuals, not the explanatory variables themselves. Equally spaced observations (i.e. forming a flat, uniform distribution) would be ideal. We more or less have this for grazing, although the range is slightly limited and the measurements will have error due to the subjective judgment of grazing intensity. A symetrical,more or less normal distribution would also be suitable. However highly skewed distributions cause serious problems. Let’s first look at the histograms. par(mfcol=c(2,3)) hist(d$ABUND) hist(d$AREA) hist(d$DIST) hist(d$LDIST) hist(d$YR.ISOL) hist(d$ALT) There are issues with both the measures of distance and area. However in this case the outliers are not the result of errors. A few forest patches are much larger than the rest. This type of pattern is very common. We do not usually want to remove these sort of outliers, as they are part of the phenomenon that we are studying. However if a single observation falls in the tail for several of the variables at once it will have very high leverage and potentially exert a great deal of influence over the result. Another way of looking for patterns in the outliers is to form Cleveland dotplots. These are very simple diagnostic tools. The value of the observation is simply plotted against the order of the data. This can help to show points that fall far from the rest of the values for several variables. par(mfrow = c(3, 2), mar = c(3, 3, 3, 1)) dotchart(d$ABUND, main = &quot;ABUND&quot;) dotchart(d$AREA, main = &quot;AREA&quot;) dotchart(d$DIST, main = &quot;DIST&quot;) dotchart(d$LDIST, main = &quot;LDIST&quot;) dotchart(d$YR.ISOL, main = &quot;YR.ISOL&quot;) dotchart(d$ALT, main = &quot;ALT&quot;) We can see that there are two observations with high AREA values, one observation with a high DIST value, and a couple of observations with high LDIST values. These are all different forest patches. If the same patch had much larger values of all variables, then it probably should be dropped from the analysis, as it could exert too much influence on the results. At the very least the analysis should be conducted twice, once with the oulier included and then with the outler removed. 18.3.2 Transformation The alternative to dropping outliers is to apply a transformation. This is aimed at “pulling in”&quot; the tail of the distribution in order to give the variables better statistical properties for modelling. We will try a log transformation and look at the results. par(mfcol=c(3,2)) d$LogDist&lt;-log10(d$DIST) d$LogArea&lt;-log10(d$AREA) d$LogLDist&lt;-log10(d$LDIST) hist(d$LogDist) hist(d$LogArea) hist(d$LogLDist) dotchart(d$LogDist,main=&quot;Log Dist&quot;) dotchart(d$LogArea,main=&quot;Log Area&quot;) dotchart(d$LogLDist,main=&quot;Log LDist&quot;) The transformations seem to have effectively neutralised the outliers, so we can proceed to the next step using all the data. 18.4 Collinearity To assess collinearity, we will use three tools: Pairwise scatterplots, correlation coefficients, and variance inflation factors (VIF). The first two can be combined in one graph with some R code that is modified from the pairs help file. The modified function can be loaded as a script from the course site. library(aqm) We first select the variables that we are interested in. You may want to look at the data frame again with str to check the order. We can quickly subset the data to only include the 2nd and the 6th to 11th columns using the command below. d1&lt;-d[,c(2,6:11)] str(d1) ## &#39;data.frame&#39;: 56 obs. of 7 variables: ## $ ABUND : num 5.3 2 1.5 17.1 13.8 14.1 3.8 2.2 3.3 3 ... ## $ YR.ISOL : int 1968 1920 1900 1966 1918 1965 1955 1920 1965 1900 ... ## $ GRAZE : int 2 5 5 3 5 3 5 5 4 5 ... ## $ ALT : int 160 60 140 160 140 130 90 60 130 130 ... ## $ LogDist : num 1.59 2.37 2.02 1.82 2.39 ... ## $ LogArea : num -1 -0.301 -0.301 0 0 ... ## $ LogLDist: num 1.59 2.37 2.49 1.82 2.39 ... Now the function we have loaded produces a scatterplot of each variable against each of the rest, and shows the correlation coefficient in a font that is proportional to its size. Xpairs(d1) So there is a strong correlation between abundance, grazing and the logarithm of area. There is a weaker correlation with the year of isolation. These are the relationships we are interested in. However there are also correlations between the logarithm of the distance to the nearest patch and the logarithm of the distance to the nearest large patch. This is not suprising if the nearest patch is also a large patch. We will only need to worry about this if either of the terms are included in a model. Grazing is also correlated with year of isolation and log area. The problem with these correlations is that they potentially confound the interpretation. The degree of confounding depends on the strengthof the correlation. If, for example, all the fragments that are heavily grazed were also small it would be very hard to clearly attribute low abundance to grazing or to area. You can look at the significance of the correlations using the function cor.prob that was also included in the script loaded above. This places the correlation coeficient below the diagonal in the matrix and its significance above. In this particular data set correlation coeficients of above 0.3 (log Area with Altitude) are significant. round(cor.prob(d1),2) ## ABUND YR.ISOL GRAZE ALT LogDist LogArea LogLDist ## ABUND 1.00 0.00 0.00 0.00 0.35 0.00 0.39 ## YR.ISOL 0.50 1.00 0.00 0.08 0.89 0.04 0.24 ## GRAZE -0.68 -0.64 1.00 0.00 0.29 0.00 0.80 ## ALT 0.39 0.23 -0.41 1.00 0.10 0.04 0.04 ## LogDist 0.13 -0.02 -0.14 -0.22 1.00 0.02 0.00 ## LogArea 0.74 0.28 -0.56 0.28 0.30 1.00 0.00 ## LogLDist 0.12 -0.16 -0.03 -0.27 0.60 0.38 1.00 We will look at the variance inflation factor after fitting some models. 18.5 Model selection One of the most interesting uses of multiple regression is to establish how many variables might be involved in determining the response. Every time we use regression with observational data such as these we need to remember that correlation is not causation. We cannot unequivocally attribute a cause to the effect. However we can analyse the strength of the association and interpret this carefully. In order to achieve this we need to be aware of some of the pitfalls that arise as a result of multiple colinearity. Let’s first look at the grazing and area effects. We can fit an additive model simply by typing the names of the terms. We can then test their significance using anova. lm.mod1&lt;-lm(ABUND~GRAZE+LogArea,data=d1) anova(lm.mod1) ## Analysis of Variance Table ## ## Response: ABUND ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GRAZE 1 2952.3 2952.35 71.094 2.301e-11 *** ## LogArea 1 1184.6 1184.64 28.527 1.977e-06 *** ## Residuals 53 2200.9 41.53 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Grazing and LogArea are quite closely correlated. So we have a problem. If we type the model formula in a different order we get different p-values for the terms! lm.mod2&lt;-lm(ABUND~LogArea+GRAZE,data=d1) anova(lm.mod2) ## Analysis of Variance Table ## ## Response: ABUND ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## LogArea 1 3471.0 3471.0 83.583 1.757e-12 *** ## GRAZE 1 666.0 666.0 16.038 0.0001946 *** ## Residuals 53 2200.9 41.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both terms are still significant, but the result is quite different. Why is this? Let’s break down the analysis in steps. First let’s look at the relationship with grazing alone. plot(ABUND~GRAZE,data=d1) lm.mod.g&lt;-lm(ABUND~GRAZE,data=d1) anova(lm.mod.g) ## Analysis of Variance Table ## ## Response: ABUND ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GRAZE 1 2952.3 2952.3 47.09 6.897e-09 *** ## Residuals 54 3385.6 62.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 abline(lm.mod.g) Now, imagine that we first looked at the strongest relationship that was apparent from the pairs plot. This is the relationship with log area. We could fit a simple regression model. plot(ABUND~LogArea,data=d1) lm.mod.a&lt;-lm(ABUND~LogArea,data=d1) anova(lm.mod.a) ## Analysis of Variance Table ## ## Response: ABUND ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## LogArea 1 3471.0 3471.0 65.377 7.178e-11 *** ## Residuals 54 2866.9 53.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 abline(lm.mod.a) The residuals from the regression is the variability that is not explained by area. We could then take this unexplained variability and see if any of this could still be explained by grazing. This would involve fitting a second model. plot(residuals(lm.mod.a)~GRAZE,data=d1) lm.mod.g2&lt;-lm(residuals(lm.mod.a)~GRAZE,data=d1) abline(lm.mod.g2) anova(lm.mod.g2) ## Analysis of Variance Table ## ## Response: residuals(lm.mod.a) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GRAZE 1 457.82 457.82 10.262 0.002279 ** ## Residuals 54 2409.12 44.61 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significance of grazing is greatly reduced after we have taken into account the effect of area. The first variable “soaks up”&quot; a lot of the variablity that is also correlated with grazing. So less of the variability in the residuals can be explained by grazing. So, the order in which we analyse the variables is important. Although the sum of squares and the p-values are slightly different when we fit both terms together using the model formula for multiple regression, the general effect is the same. This does not occur if the variables are completely uncorrelated (orthogonal). Hence there is a need to look at the data very carefully whenever you build a multiple regression model. 18.5.1 Dropping terms In the simple case of two explanatory variables we can get around the problem by dropping each of the terms in turn, refitting the model and looking at the difference. The results for mod1 (grazing first) and mod2 (log area first) are now identical. drop1(lm.mod1,test=&quot;F&quot;) ## Single term deletions ## ## Model: ## ABUND ~ GRAZE + LogArea ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 2200.9 211.59 ## GRAZE 1 666.0 2866.9 224.40 16.038 0.0001946 *** ## LogArea 1 1184.6 3385.6 233.71 28.527 1.977e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 drop1(lm.mod2,test=&quot;F&quot;) ## Single term deletions ## ## Model: ## ABUND ~ LogArea + GRAZE ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 2200.9 211.59 ## LogArea 1 1184.6 3385.6 233.71 28.527 1.977e-06 *** ## GRAZE 1 666.0 2866.9 224.40 16.038 0.0001946 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 18.5.2 Stepwise model selection The question underlying the analysis is to find out which of the set of explanatory variables are most closely associated with bird abundance. In one sense we already have an answer from the pairs plot. Some of the variables are clearly associated with abundance when used in a model on their own. But we want to explain as much of the variability as possible. How many terms are useful? One way of addressing this is to fit a model with all the terms and then drop each in turn to check for significance. lm.mod.full&lt;-lm(ABUND~.,data=d1) drop1(lm.mod.full,test=&quot;F&quot;) ## Single term deletions ## ## Model: ## ABUND ~ YR.ISOL + GRAZE + ALT + LogDist + LogArea + LogLDist ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 1996.8 214.14 ## YR.ISOL 1 108.83 2105.7 215.11 2.6705 0.10864 ## GRAZE 1 131.07 2127.9 215.70 3.2163 0.07908 . ## ALT 1 27.02 2023.9 212.90 0.6630 0.41945 ## LogDist 1 4.68 2001.5 212.27 0.1149 0.73609 ## LogArea 1 1059.75 3056.6 235.98 26.0049 5.494e-06 *** ## LogLDist 1 3.80 2000.7 212.25 0.0933 0.76130 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 However, again there is a problem with this if there is any collinearity. This initial analysis suggests that we can drop any one of the variables from the model with the exception of log Area without significantly reducing the amount of variance explained. However the problem is that if we dropped more variables from the model some of these variables would become significant again as they picked up variability that was explained by the lost variables. 18.5.3 The variance inflation factor The variables that are more closely correlated with all the rest are those that are likely to have the least significance when dropped from the full model. We can rank the variables according to their colinearity with the rest by calculating the variance inflation factor. library(car) sort(vif(lm.mod.full)) ## ALT LogDist YR.ISOL LogArea LogLDist GRAZE ## 1.467937 1.654553 1.804769 1.911514 2.009749 2.524814 The variance inflation factor is quite a simple measure to understand. If we remove abundance from the data frame we are left only with the explanatory variables. expl&lt;-d1[,-1] If we fit a model using all these variables in order to explain the variability in any one of the other explanatory variables we can get a value for R2. If the variable is closely correlated with all the rest this will be large. If we subtract this from one we get the ammount of variability not explained. The vif is simply the reciprocal of this. For example for grazing. vif.mod&lt;-lm(GRAZE~.,data=expl) Rsq&lt;-summary(vif.mod)$r.squared Rsq ## [1] 0.6039312 vif&lt;-1/(1-Rsq) vif ## [1] 2.524814 High values of vif are problematical, but there is no concensus of what is a high value. Zuur (2007) states that some statisticians suggest that values higher than 5 or 10 are too high (Montgomery and Peck 1992). In this case none of the vif values are that large, but the problem of lack of explanatory power in the presence of other variables is still apparent. Therefore a good approach is to rank the vif values as we have done here and think about the problem in context. Variables that do not explain much of the variability in the response variable are not going to be important whatever their vif. However a variable with a high vif value could explain a lot of the variability when used on its own in a model, but very little in combination with others. Grazing seems to be this sort of variable. The point here is that from an ecological perspective we would suspect that high grazing values should have an effect on bird density by altering habitat. So we would not want to drop the term from the model as a result of artefacts arising as a result of collinearity. An alternative to using p-values for model selection is the use of AIC. AIC is quite conservative, in other words it tends to allow models to retain more parameters than some other methods for model selection. We can use AIC for backward model selection using the step function in R. Backwards model selection using AIC is based on the same principle as the drop1 function, but terms are retained if they reduce AIC by more than 2 points. The process is repeated until dropping any further terms does not reduce AIC by more than 2 points. lm.mod.step&lt;-step(lm.mod.full,test=&quot;F&quot;) ## Start: AIC=214.14 ## ABUND ~ YR.ISOL + GRAZE + ALT + LogDist + LogArea + LogLDist ## ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## - LogLDist 1 3.80 2000.7 212.25 0.0933 0.76130 ## - LogDist 1 4.68 2001.5 212.27 0.1149 0.73609 ## - ALT 1 27.02 2023.9 212.90 0.6630 0.41945 ## &lt;none&gt; 1996.8 214.14 ## - YR.ISOL 1 108.83 2105.7 215.11 2.6705 0.10864 ## - GRAZE 1 131.07 2127.9 215.70 3.2163 0.07908 . ## - LogArea 1 1059.75 3056.6 235.98 26.0049 5.494e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Step: AIC=212.25 ## ABUND ~ YR.ISOL + GRAZE + ALT + LogDist + LogArea ## ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## - LogDist 1 12.64 2013.3 210.60 0.3159 0.57661 ## - ALT 1 35.12 2035.8 211.22 0.8778 0.35331 ## &lt;none&gt; 2000.7 212.25 ## - YR.ISOL 1 121.64 2122.3 213.55 3.0399 0.08739 . ## - GRAZE 1 132.44 2133.1 213.84 3.3098 0.07486 . ## - LogArea 1 1193.04 3193.7 236.44 29.8161 1.489e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Step: AIC=210.6 ## ABUND ~ YR.ISOL + GRAZE + ALT + LogArea ## ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## - ALT 1 57.84 2071.1 210.19 1.4653 0.23167 ## &lt;none&gt; 2013.3 210.60 ## - GRAZE 1 123.48 2136.8 211.94 3.1280 0.08294 . ## - YR.ISOL 1 134.89 2148.2 212.23 3.4169 0.07033 . ## - LogArea 1 1227.11 3240.4 235.25 31.0846 9.412e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Step: AIC=210.19 ## ABUND ~ YR.ISOL + GRAZE + LogArea ## ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 2071.1 210.19 ## - YR.ISOL 1 129.81 2200.9 211.59 3.2590 0.07682 . ## - GRAZE 1 188.45 2259.6 213.06 4.7315 0.03418 * ## - LogArea 1 1262.97 3334.1 234.85 31.7094 7.316e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm.mod.step) ## ## Call: ## lm(formula = ABUND ~ YR.ISOL + GRAZE + LogArea, data = d1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.5159 -3.8136 0.2027 3.1271 14.5542 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -134.26065 86.39085 -1.554 0.1262 ## YR.ISOL 0.07835 0.04340 1.805 0.0768 . ## GRAZE -1.90216 0.87447 -2.175 0.0342 * ## LogArea 7.16617 1.27260 5.631 7.32e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.311 on 52 degrees of freedom ## Multiple R-squared: 0.6732, Adjusted R-squared: 0.6544 ## F-statistic: 35.71 on 3 and 52 DF, p-value: 1.135e-12 Notice how grazing became significant once more once altitude was dropped. This is probably because fragments that are more heavily grazed are in the valleys. We do have a useful model. But we need to be careful in the intepretation of the intercept. Because we used year of isolation in the model the intercept refers to the value at year zero, altitude zero, grazing zero and log area zero. This is not really helpful. However the slopes are interpretable. They represent the expected change in bird density for each unit change in the variables, when holding all other variables constant. We should report these values along with their confidence intervals, or standard errors. confint(lm.mod.step) ## 2.5 % 97.5 % ## (Intercept) -3.076166e+02 39.0952733 ## YR.ISOL -8.739958e-03 0.1654462 ## GRAZE -3.656915e+00 -0.1473963 ## LogArea 4.612500e+00 9.7198311 Notice that although the stepwise procedure based on AIC retained year of isolation in the model, the confidence interval includes zero and we cannot be sure if the effect is positive or negative at the 95% level. This is another way of looking at statistical significance. This term is not significant at the usual 0.05 cutoff. You might consider dropping it from the model. More on this later. 18.5.4 Diagnostics Once the model has been decided on the usual diagnostics should be conducted. par(mfcol=c(2,2)) plot(lm.mod.step) The plots suggest that the main assumptions are not seriously violated. However you should be aware that spatial data such as these could lack independence due to autocorrelation. In order to analyse this we would need the coordinates, which are not provided in this data set. 18.6 Generalised Additive Models In a previous class we looked at how flexible models can be fitted todata in order to capture more complex responses that are not well modelled by regression. General additive models can be used in a very similar way to multiple regression. They will show up any non linear response when plotted. 18.6.1 Fitting a gam with multiple variables library(mgcv) gam.mod1&lt;-gam(ABUND~s(GRAZE,k=4)+s(LogArea)+s(YR.ISOL),data=d1) summary(gam.mod1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(GRAZE, k = 4) + s(LogArea) + s(YR.ISOL) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.5143 0.7369 26.48 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(GRAZE) 2.682 2.909 6.078 0.00141 ** ## s(LogArea) 2.760 3.479 11.485 3.12e-06 *** ## s(YR.ISOL) 2.900 3.542 0.842 0.55123 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.736 Deviance explained = 77.6% ## GCV = 36.499 Scale est. = 30.41 n = 56 Note that in the case of grazing it was necessary to set the number of knots to four. This is because the default in mgcv is to begin with 10 and reduce the complexity through crossvalidation. We can’t have 10 knots for grazing as there are only 5 values. par(mfcol=c(2,2)) plot(gam.mod1) 18.6.2 Quick model selection for Gams The cross validation algorithm used when fitting gams does not allow stepwise term deletion to be carried out. One quick method of model selection is to add “select=T”&quot; when fitting a model with all the terms. This allows the smoother to reduce to having no knots, which effectively eliminates a term from the model. gam.mod.sel&lt;-gam(ABUND~s(YR.ISOL)+s(GRAZE,k=4)+s(ALT)+s(LogDist)+s(LogArea)+s(LogLDist),select=T,data=d1) anova(gam.mod.sel) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(YR.ISOL) + s(GRAZE, k = 4) + s(ALT) + s(LogDist) + ## s(LogArea) + s(LogLDist) ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(YR.ISOL) 1.104e-11 9.000e+00 0.000 0.710 ## s(GRAZE) 2.181e+00 3.000e+00 8.207 1.36e-05 ## s(ALT) 6.048e-01 3.000e+00 0.492 0.118 ## s(LogDist) 4.183e-11 9.000e+00 0.000 1.000 ## s(LogArea) 2.247e+00 8.000e+00 5.729 2.63e-09 ## s(LogLDist) 8.506e-12 9.000e+00 0.000 0.962 18.6.3 Akaike weighting, relative strength of evidence approach. As mentioned previously, in recent years information criteria (AIC) based approaches have become increasingly used for model selection by ecologists. This has been largely due to a string of influential papers by Burnham and Anderson. AIC is used in the automated stepwise procedure that we have seen already. However the popularity of the Burnham and Anderson model comparison approach arises as a result of the underlying philosophy. Burnham and Anderson do not like stepwise procedures, which are carried out without thought. They insist that simply letting the computer decide on a best model is a poor strategy. The alternative is to suggest a small subset of candidate models that make sense within the context of the study. AIC is then used to evaluate the strength of the evidence provided by the data for each model. AIC stands for “Akaike’s Information Criteria”. To simplify a long story AIC is based on the concept of penalised likelihood. The (-2log) likelihood is a measure of the fit of the model, as is R2, and the penalty paid is 2 points for each parameter used in the model. All other things being equal models with lower AIC scores are better than those with high AIC scores, and a difference of 2 points in the (-2log) likelihood is worth paying for one parameter. To use AIC in model selection using the protocol laid out by Burnham and Anderson there are a number of things to keep in mind. All the models in the set of candidates must use exactly the same set of observations and therefore be based on the same sample size n. All the models must use exactly the same response variable. For example do not compare one model using abundance with another using log(abundance) Models must use the same methods to calculate likelihoods. This is a technical issue, but in cases where normally distributed errors are asumed this can be taken as true. 18.6.4 The method. Choose a subset of models which can be justified as good candidates for the best model on both statistical and biological grounds. Calculate AIC for all models. Burnham and Anderson suggest using an adjusted value called AICc if AIC is small. In fact this could be used for all analyses. Identify the model with the smallest AIC. Denote its AIC as \\(AIC_{min}\\). This is the best model. We could stop at this point but there is more information to extract from the models. Calculate the AIC differences, for each model.\\(\\Delta_{i}=AIC_{i}-AIC_{min}\\) Compute the relative likelihood for each model. \\(RL_{i}=exp(-0.5\\Delta_{i})\\) Compute Akaike weights for each model. These are the normalized relative likelihoods. \\(w_{i}=\\frac{RL_{i}}{\\sum RL}\\). The Akaike weights can be interpreted as probabilities that the given model is the best model.If we were to go back and obtain more data from the population and refit the same models again, then the Akaike weight gives the probability that a given model would be judged the best model on repeated sampling. The package MuMIn in R will do steps 2 to 6 for us. library(MuMIn) gam.mod1&lt;-gam(ABUND~s(GRAZE,k=4)+s(LogArea),data=d1) gam.mod2&lt;-gam(ABUND~s(GRAZE,k=4)+s(LogArea)+s(YR.ISOL),data=d1) gam.mod3&lt;-gam(ABUND~s(GRAZE,k=4)+s(LogArea)+s(ALT),data=d1) gam.mod4&lt;-gam(ABUND~s(YR.ISOL)+s(GRAZE,k=4)+s(ALT)+s(LogDist)+s(LogArea)+s(LogLDist),data=d1) model.sel(gam.mod1,gam.mod2,gam.mod3,gam.mod4) ## Model selection table ## (Int) s(GRA,4) s(LgA) s(YR.ISO) s(ALT) s(LgD) s(LLD) df logLik AICc ## gam.mod1 19.51 + + 6 -173.231 361.9 ## gam.mod3 19.51 + + + 8 -171.390 362.3 ## gam.mod2 19.51 + + + 10 -169.964 365.9 ## gam.mod4 19.51 + + + + + + 13 -167.720 372.9 ## delta weight ## gam.mod1 0.00 0.510 ## gam.mod3 0.40 0.417 ## gam.mod2 3.95 0.071 ## gam.mod4 10.98 0.002 ## Models ranked by AICc(x) So, if we take this information theoretic approach the gam model which includes only grazing and log area has a 51% probabliity of being the best within the set of models we proposed, while the model with an additional term for altitude has a 42% probability. We can also compare GAM’s with linear models, as the likelihoods are calculated in the same way. model.sel(gam.mod1,lm.mod.step) ## Model selection table ## (Int) s(GRA,4) s(LgA) GRA LgA YR.ISO class df logLik ## gam.mod1 19.51 + + gam 6 -173.231 ## lm.mod.step -134.30 -1.902 7.166 0.07835 lm 5 -180.555 ## AICc delta weight ## gam.mod1 361.9 0.0 0.995 ## lm.mod.step 372.3 10.4 0.005 ## Models ranked by AICc(x) The GAM wins easily. This is as a result of the GAM finding the correct form for the grazing response rather than assuming a straight line. 18.7 Tree models One of the problems with multiple regression and generalised additive models is that possible interactions between variables are not included. Zuur et al suggest that grazing could be included as a categorical factor, which allows some interactive effects to be looked at by taking a similar approach to the one we used for analysis of covariance. However interactions between two numerical variables cannot be easily captured. One easy way around this is to use tree models. The idea behind a tree model is to find a series of binary splitting rules that divide the response data into the most homogeneous subsets. The easiest way to follow this is with an example. 18.7.1 Fitting and plotting a tree model library(rpart) tree.mod&lt;-rpart(ABUND~.,data=d1) par(xpd=T) plot(tree.mod) text(tree.mod,use.n=T,digits=3) This has a direct interpretation. We read the tree as a set of rules. If a rule is true we take the left hand branch, if it is false the right hand branch. The first split is based on the variable that can explain most of the deviance when used in this way. This is grazing. So if grazing is &gt;=4.5 i.e. takes values of 5 on the ordinal scale we do not need to consider any other variables. The mean abundance levels for these sites is 6.3 based on 13 observations. If grazing pressure is low, some more explanatory variables are important. Large fragments with an area over log10(1.145)= 14 hectares have a mean abundance of 30.1. The final split for smaller fragments is less easy to explain. Fragments isolated after 1964 have a slightly lower mean abundance than those isolated earlier. However R starts off using an arbitrary degree of complexity. Maybe this tree has overfit to the data? We can test this by seeing if the tree can be pruned. 18.7.2 Pruning the tree The theory behind tree models is presented in Zuur and Crawley. The most difficult element to understand is how to select an optimum tree based on the complexity parameter. The method has some similarity to the use of AIC for model selection. It is based on penalised likelihood (measured as the deviance). We want a tree that fits the data as well as possible, but uses as few parameters (splits) as possible. The default stopping rule that produced the first tree we looked at may have gone too far. We can get an R squared value (variability, or deviance explained) for each split along with the relative error calculated directly or as a result of cross validation. Relative error is 1-R squared. Cross validation produces confidence intervals for this error. We do not need to go into any more gory details regarding how cross validation is run, apart from mentioning that the cross validation error will always be higher than the apparent error as it is based on simulations using part of the data to fit a model and the rest to validate it. par(mfcol=c(2,2)) rsq.rpart(tree.mod) ## ## Regression tree: ## rpart(formula = ABUND ~ ., data = d1) ## ## Variables actually used in tree construction: ## [1] GRAZE LogArea YR.ISOL ## ## Root node error: 6337.9/56 = 113.18 ## ## n= 56 ## ## CP nsplit rel error xerror xstd ## 1 0.466991 0 1.00000 1.05667 0.135782 ## 2 0.232025 1 0.53301 0.77947 0.126016 ## 3 0.044967 2 0.30098 0.40851 0.081904 ## 4 0.010000 3 0.25602 0.42353 0.084239 plotcp(tree.mod) It should be apparent from the figures that adding a third split does not result in much gain in variance explained (R squared) or much reduction in relative error. So we can prune the tree using a complexity parameter of 0.1 (see the third figure above). We can also get a nicer figure using the partykit library. tree.mod&lt;-prune(tree.mod,cp=0.1) library(partykit) plot(as.party(tree.mod), digits=3) We can also look at confidence intervals for the node values. These should not overlap if the splits are significant. library(gplots) plotmeans(d$ABUND~round(predict(tree.mod),1),connect=F) grid() So, the analysis leads to similar conclusions regarding the importance of the variables as multiple regresion. Date of isolation does not seem to be worth keeping in the model. While conservative fitting procedures may retain this term, the effect is not strong enough to be considered important. It is also ambiguous and hard to interpret in a meaningful way. A point to be aware of is that the deviance is calculated using the sum of squares. Outliers and points with high leverage can still influence the results of the analysis. Homogeneity of variance is not assumed, as variance explained is calculated for each split separately. However it is a good idea to carry out some basic diagnistics in order to identify problematic observations that are not well predicted by the model and to check that the residuals are at least approximately normally distributed. par(mfcol=c(2,2)) plot(residuals(tree.mod)~predict(tree.mod)) boxplot(residuals(tree.mod)~round(predict(tree.mod),1)) hist(residuals(tree.mod)) qqnorm(residuals(tree.mod)) qqline(residuals(tree.mod)) The R squared of the tree model is 1-0.3= 0.7. This is comparable to the value we got using GAM and it is higher than the value for conventional multiple regression. The GAM and tree models show clearly that the effect of grazing is only important at the highest level of the index. The tree model also points out a possible interaction. At high levels of grazing there seems to be no further effect of area on abundance. This may be because high levels of grazing have altered the forest habitat so much that it is no longer suitable for many types of forest bird. Tree models are particularly useful for quickly establishing which variables are most important in large data sets. Sometimes only a single split is found. If the same variable is used to split the data repeatedly the tree is effectively carrying out a regression on that variable, but with a series of steps instead of a smooth line. 18.8 Which technique to use? The three techniques led to more or less the same conclusion regarding the relative importance of the explanatory variables. The only slight point of contention was whether altitude or year since isolation could have some effect. The GAM model and the tree model point out that the effect of grazing is only important at the highest level. This might also have been identified if we had used grazing as a factor in a linear model, which we did not try here. You may want to read Zuur et als analysis of these data. Each of the modelling techniques have their own strengths and weaknesses. Multiple regression is a classic methodology that is understood by most researchers. It is simple to report the results in a conventional style (see this paper by the researcher who collected the data we have just analysed for an example). Many relationships are approximated by a straight line, so regression coefficients form a useful summary of effects. Using linear regression forces you to investigate the nature of the data thoroughly in order to ensure that the assumptions of the technique are not severely violated. With care, data sets with a high degree of multiple colinearity can be analysed, providing the consequences of colinearity are recognised when selecting the model. However the technique can lead to curvilinear responses being missed, along with interactions. Generalised additive models have become very popular in the ecological literature over the last few years. They allow complex responses to be identified. The relative importance of each explanatory variable can be assessed using GAMS just as in multiple regression. Linear regression can fail to spot curvilinear responses which can sometimes lead to misspecified models that do not capture an important element in the data. However GAM models are difficult to communicate to others. They can only really be plotted. So unless the data and the fitted model are provided they cannot be used for predicting new cases after the report has been written up. Tree models are simple to interpret, make relatively few assumptions and can show up interactions. Providing trees are not too complex the results can be easily communicated as a set of verbal rules. However the assumption that responses take the form of clear break points is often very misleading. Although tree models find the best break points, they are often still arbitrary and do not necessarily represent any real process. There is no particular reason to prefer a single method over the others. In fact, until you have looked carefully at your data you are unlikely to know which technique will produce the most insight. It is important to end up with a defensible model which most closely meets the assumptions. This class has shown that with practice and a little guidance it can be possible to combine all three techniques in order to find out as much as possible about the data. This is a sensible approach to take, even though it requires a bit more work. A write up of the analysis would sumarise the main findings from the technique that produced the most defensible model in the main body of the report, and include appendices showing the key results from alternative approaches. Ideally the data and R code used to fit the models would also be supplied so that a reader could check the validity of the conclusions. Do be aware that none of these methods (with the possible exception of tree models) will work well in the case of data with large numbers of zeros, or with large and heterogeneous variability. 18.9 References Anderson, D. R., Burnham, K. P. (2001). Kullback-Leibler information as a basis for strong inference in ecological studies. Wildlife Research, 28(2), 111-120. Burnham, K. P, Anderson, D. R., Link, W. A., Johnson, D. H. (2001). Suggestions for presenting the results of data analyses. Journal of Wildlife Management, 65(3), 373-378. Thompson, W. L., Anderson, D. R., Burnham, K. P. (2000). Null hypothesis testing: problems, prevalence, and an alternative. The Journal of Wildlife Management, 912-923. "],
["analysis-of-multi-species-data.html", "Chapter 19 Analysis of multi-species data 19.1 Working with the sites by species matrix 19.2 Resampling individuals", " Chapter 19 Analysis of multi-species data One of the commonest tasks that quantitative ecologists face is the analysis of site by species matrices. Such data are routinely produced as a result of many types of ecological survey. Because multiple species are involved such data is always challenging to analyse. There are a lot of questions that can be addressed with multispecies data. Many of these involve analysis of species diversity in some respect. Species diversity is classically regarded as consisting of three components. \\(gamma=alpha+beta\\) Alpha diversity is measured at the scale of some sampling unit (i.e. plot or quadrat). Gamma diversity is the overall diversity over the whole area that is being measured. Beta diversity is the difference between the two. Or is it? The issues involved in analysing diversity are extremely complex and could form the basis of a whole taught unit. For the moment let’s accept this simplified version of the issue for the pragmatic purpose of finding some methods that will allow us to work with a sites by species matrix. 19.1 Working with the sites by species matrix One approach to working with the species by sites matrix would be to produce a measure that could be used as a response variable in some other model. In this class we will look at some simple measures of species diversity. The next class will apply some more complex methods in order to look at species composition. However you first need to become familiar with the format of the matrix in order to work with the data comfortably. 19.1.1 BCI data Let’s look at some realistically complex data from a classic study of tropical diversity. The Smithsonian institute has a permanent sample plot of 50 hectares of tropical forest located on the Island of Barro Colorado in Panama. The data set consists of a grid of completely censused plot of 100m x 100m within which all trees over 5cm in diameter were counted and identified to species. library(rgdal) x&lt;- rep(seq(625754, 626654, by=100),each=5) y&lt;- rep(seq(1011569, 1011969, by=100),len=50) coords&lt;-data.frame(x,y) dd&lt;-data.frame(coords,id=1:100) coordinates(dd)&lt;-~x+y proj4string(dd)&lt;-CRS(&quot;+init=epsg:32617&quot;) dd&lt;-spTransform(dd, CRS(&quot;+init=epsg:4326&quot;)) library(mapview) m&lt;-mapview(dd) library(leaflet.extras) m@map %&gt;% addFullscreenControl() library(vegan) library(reshape) data(BCI) You can look at this matrix using str(BCI) ## &#39;data.frame&#39;: 50 obs. of 225 variables: ## $ Abarema.macradenia : int 0 0 0 0 0 0 0 0 0 1 ... ## $ Vachellia.melanoceras : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Acalypha.diversifolia : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Acalypha.macrostachya : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Adelia.triloba : int 0 0 0 3 1 0 0 0 5 0 ... ## $ Aegiphila.panamensis : int 0 0 0 0 1 0 1 0 0 1 ... ## $ Alchornea.costaricensis : int 2 1 2 18 3 2 0 2 2 2 ... ## $ Alchornea.latifolia : int 0 0 0 0 0 1 0 0 0 0 ... ## $ Alibertia.edulis : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Allophylus.psilospermus : int 0 0 0 0 1 0 0 0 0 0 ... ## $ Alseis.blackiana : int 25 26 18 23 16 14 18 14 16 14 ... ## $ Amaioua.corymbosa : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Anacardium.excelsum : int 0 0 0 0 0 0 0 1 0 0 ... ## $ Andira.inermis : int 0 0 0 0 1 1 0 0 1 0 ... ## $ Annona.spraguei : int 1 0 1 0 0 0 0 1 1 0 ... ## $ Apeiba.glabra : int 13 12 6 3 4 10 5 4 5 5 ... ## $ Apeiba.tibourbou : int 2 0 1 1 0 0 0 1 0 0 ... ## $ Aspidosperma.desmanthum : int 0 0 0 1 1 1 0 0 0 1 ... ## $ Astrocaryum.standleyanum : int 0 2 1 5 6 2 2 0 2 1 ... ## $ Astronium.graveolens : int 6 0 1 3 0 1 2 2 0 0 ... ## $ Attalea.butyracea : int 0 1 0 0 0 1 1 0 0 0 ... ## $ Banara.guianensis : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Beilschmiedia.pendula : int 4 5 7 5 8 6 5 9 11 14 ... ## $ Brosimum.alicastrum : int 5 2 4 3 2 2 6 4 3 6 ... ## $ Brosimum.guianense : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Calophyllum.longifolium : int 0 2 0 2 1 2 2 2 2 0 ... ## $ Casearia.aculeata : int 0 0 0 0 0 0 0 1 0 0 ... ## $ Casearia.arborea : int 1 1 3 2 4 1 2 3 9 7 ... ## $ Casearia.commersoniana : int 0 0 1 0 1 0 0 0 1 0 ... ## $ Casearia.guianensis : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Casearia.sylvestris : int 2 1 0 0 0 3 1 0 1 1 ... ## $ Cassipourea.guianensis : int 2 0 1 1 3 4 4 0 2 1 ... ## $ Cavanillesia.platanifolia : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Cecropia.insignis : int 12 5 7 17 21 4 0 7 2 16 ... ## $ Cecropia.obtusifolia : int 0 0 0 0 1 0 0 2 0 2 ... ## $ Cedrela.odorata : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Ceiba.pentandra : int 0 1 1 0 1 0 0 1 0 1 ... ## $ Celtis.schippii : int 0 0 0 2 2 0 1 0 0 0 ... ## $ Cespedesia.spathulata : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Chamguava.schippii : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Chimarrhis.parviflora : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Maclura.tinctoria : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Chrysochlamys.eclipes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Chrysophyllum.argenteum : int 4 1 2 2 6 2 3 2 4 2 ... ## $ Chrysophyllum.cainito : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Coccoloba.coronata : int 0 0 0 1 2 0 0 1 2 1 ... ## $ Coccoloba.manzinellensis : int 0 0 0 0 0 0 0 2 0 0 ... ## $ Colubrina.glandulosa : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Cordia.alliodora : int 2 3 3 7 1 1 2 0 0 2 ... ## $ Cordia.bicolor : int 12 14 35 23 13 7 5 10 7 13 ... ## $ Cordia.lasiocalyx : int 8 6 6 11 7 6 6 3 0 4 ... ## $ Coussarea.curvigemma : int 0 0 0 1 0 2 1 0 1 1 ... ## $ Croton.billbergianus : int 2 2 0 11 6 0 0 4 2 0 ... ## $ Cupania.cinerea : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Cupania.latifolia : int 0 0 0 1 0 0 0 0 0 0 ... ## $ Cupania.rufescens : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Cupania.seemannii : int 2 2 1 0 3 0 1 2 2 0 ... ## $ Dendropanax.arboreus : int 0 3 6 0 5 2 1 6 1 3 ... ## $ Desmopsis.panamensis : int 0 0 4 0 0 0 0 0 0 1 ... ## $ Diospyros.artanthifolia : int 1 1 1 1 0 0 0 0 0 1 ... ## $ Dipteryx.oleifera : int 1 1 3 0 0 0 0 2 1 2 ... ## $ Drypetes.standleyi : int 2 1 2 0 0 0 0 0 0 0 ... ## $ Elaeis.oleifera : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Enterolobium.schomburgkii : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Erythrina.costaricensis : int 0 0 0 0 0 3 0 0 1 0 ... ## $ Erythroxylum.macrophyllum : int 0 1 0 0 0 0 0 1 1 1 ... ## $ Eugenia.florida : int 0 1 0 7 2 0 0 1 1 3 ... ## $ Eugenia.galalonensis : int 0 0 0 0 0 0 0 1 0 0 ... ## $ Eugenia.nesiotica : int 0 0 1 0 0 0 5 4 3 0 ... ## $ Eugenia.oerstediana : int 3 2 5 1 5 2 2 3 3 3 ... ## $ Faramea.occidentalis : int 14 36 39 39 22 16 38 41 33 42 ... ## $ Ficus.colubrinae : int 0 1 0 0 0 0 0 0 0 0 ... ## $ Ficus.costaricana : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Ficus.insipida : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Ficus.maxima : int 1 0 0 0 0 0 0 0 0 0 ... ## $ Ficus.obtusifolia : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Ficus.popenoei : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Ficus.tonduzii : int 0 0 1 2 1 0 0 0 0 0 ... ## $ Ficus.trigonata : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Ficus.yoponensis : int 1 0 0 0 0 1 1 0 0 0 ... ## $ Garcinia.intermedia : int 0 1 1 3 2 1 2 2 1 0 ... ## $ Garcinia.madruno : int 4 0 0 0 1 0 0 0 0 1 ... ## $ Genipa.americana : int 0 0 1 0 0 0 1 0 1 1 ... ## $ Guapira.myrtiflora : int 3 1 0 1 1 7 3 1 1 1 ... ## $ Guarea.fuzzy : int 1 1 0 1 3 0 0 2 0 3 ... ## $ Guarea.grandifolia : int 0 0 0 0 0 0 0 1 0 0 ... ## $ Guarea.guidonia : int 2 6 2 5 3 4 4 0 1 5 ... ## $ Guatteria.dumetorum : int 6 16 6 3 9 7 8 6 2 2 ... ## $ Guazuma.ulmifolia : int 0 0 0 1 0 0 0 0 0 0 ... ## $ Guettarda.foliacea : int 1 5 1 2 1 0 0 4 1 3 ... ## $ Gustavia.superba : int 10 5 0 1 3 1 8 4 4 4 ... ## $ Hampea.appendiculata : int 0 0 1 0 0 0 0 0 2 1 ... ## $ Hasseltia.floribunda : int 5 9 4 11 9 2 7 6 3 4 ... ## $ Heisteria.acuminata : int 0 0 0 0 1 1 0 0 0 0 ... ## $ Heisteria.concinna : int 4 5 4 6 4 8 2 5 1 5 ... ## $ Hirtella.americana : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Hirtella.triandra : int 21 14 5 4 6 6 7 14 8 7 ... ## $ Hura.crepitans : int 0 0 0 0 0 2 1 1 0 0 ... ## $ Hieronyma.alchorneoides : int 0 2 0 0 0 0 0 0 1 0 ... ## [list output truncated] ## - attr(*, &quot;original.names&quot;)= chr &quot;Abarema.macradenium&quot; &quot;Acacia.melanoceras&quot; &quot;Acalypha.diversifolia&quot; &quot;Acalypha.macrostachya&quot; ... dim(BCI) ## [1] 50 225 The researchers therefore found 225 species in the 50 plots. The area is quite homogeneous, but there is a very high diversity of tree species. There are over 3000 species of tree in Panama. Whether 225 is considered as a measure of gamma diversity (for the 50 plots) or alpha diversity (for 50 ha plots within an area with a higher gamma diversity clearly depends on the scale at which diversity is being observed and studies. There are complex issues here. 19.1.2 Reshaping the site by species matrix Note that there are many zeros in the site by species matrix. As we have seen before, this is not strictly a “raw” data format, as it can be derived from a more compact table format. Here is how to interchange the two formats using the reshape package. # Make a data frame with Site ID as one of the columns bci&lt;-data.frame(Site=1:50,BCI) #Melt the data frame using the Site ID as the identifier. bci&lt;-melt(bci,id=&quot;Site&quot;) # Remove zeros bci&lt;-bci[bci$value&gt;0,] To reshape this long format back into a sites by species matrix form, with a column for the site name you can run this line of code. bci2&lt;-data.frame(cast(bci,Site~variable,fill=0)) Check the format. You may often have to remove some of the columns in order to work with a matrix that only consists of species abundence. In this case bcimat&lt;-as.matrix(bci2[,-1]) Let’s look at how many species are found in each plot. We can produce a vector of species richness using the specnumber function. S&lt;-specnumber(BCI) summary(S) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 77.00 86.00 91.00 90.78 94.00 109.00 hist(S,col=&quot;grey&quot;) So, species number could perhaps be used as a response variable if we had some explanatory variables for each sub plot. We will come back to this issue later. 19.1.3 Working with apply Many of the methods for working with quadrat type data work directly on the broad, site by species matrix. Although it is usually possible to use data frame objects in R for matrix based operations there is a difference between a data frame and a true matrix. A matrix can only hold numbers, while a data frame can hold any type of values. The “apply” command will work on any matrix, but would produce odd results on some data frames. When you ask R to apply a function to a matrix it is like writing a function at the end of a set of columns or rows on an Excel spreadsheet and then dragging them across. This is only going to work if all the columns or rows contain the same sort of values. If we want to apply a function (say sum) to rows of a matrix (say BCI) we use a 1 to refer to rows and write apply(BCI,1,sum). If we want to apply the function to the columns we write apply(BCI,2,sum) So, for example to find the abundances of the species. We can sum over all the subplots to produce a vector of abundances. abun&lt;-apply(BCI,2,sum) A few abundant species contribute a relatively large number of individuals to the total. head(sort(abun,dec=T)) ## Faramea.occidentalis Trichilia.tuberculata Alseis.blackiana ## 1717 1681 983 ## Oenocarpus.mapora Poulsenia.armata Quararibea.asterolepis ## 788 755 724 19.1.4 Working with the long format data frame It can be more convenient to use dplyr to produce tables. To do this we need to work with the reshaped long data. When the matrix was put into the “molten” database form by the reshape operation species name became the variable and the count of individual trees the value. So this code will produce a table. library(dplyr) library(DT) bci %&gt;% group_by(variable) %&gt;% summarise(occur=n(),abun=sum(value)) -&gt; abunds datatable(abunds) We can calculate the relative abundances of these top ten species as percentages using R round(100*head(sort(abun,dec=T))/sum(abun),2) ## Faramea.occidentalis Trichilia.tuberculata Alseis.blackiana ## 8.00 7.83 4.58 ## Oenocarpus.mapora Poulsenia.armata Quararibea.asterolepis ## 3.67 3.52 3.37 So even though there are 225 species recorded, 20% of the observations are from only three species. summary(abun) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 7.00 25.00 95.36 82.00 1717.00 The median abundance is 25. 50% of the species have 25 or fewer individuals. This is a very typical pattern in diversity data, whether in the tropics or elsewhere. Most individuals belong to a handful of common species. However, most species have few individuals. To put it in a delberately perverse way common species turn out to be rare and rare species are common! This has generated a huge literature based on attempts to describe this pattern mathematically and find underlying reasons for it. If we plot a histogram of abundances we can see the pattern. hist(abun,col=&quot;grey&quot;) Taking logarithms hist(log10(abun),col=&quot;grey&quot;) This may explain why there is a difference between the number of species found in each subplot and the total number of species. It is simply chance. As there are so many rare species you would not expect eachsubplot to contain all the species. We can look at the pattern ofaccumulation of species that we obtain if we aggregate plots randomly into larger units using the specaccum function. AcCurve&lt;-specaccum(BCI,&quot;random&quot;) plot(AcCurve,ci.type=&quot;poly&quot;, col=&quot;red&quot;, lwd=2, ci.lty=0, ci.col=&quot;grey&quot;) boxplot(AcCurve, col=&quot;blue&quot;, pch=&quot;+&quot;,add=T) specpool(BCI) ## Species chao chao.se jack1 jack1.se jack2 boot boot.se n ## All 225 236.3732 6.54361 245.58 5.650522 247.8722 235.6862 3.468888 50 So species richness accumulates in a non linear manner. This has been modelled as a hyperbolic function in order to estimate the asymptotic(maximum) species richness. Fitting a Lomolino curve actually produces an estimate of the maximum number of species. ## Fit Lomolino model to the exact accumulation AcCurve&lt;-specaccum(BCI,&quot;exact&quot;) mod1 &lt;- fitspecaccum(AcCurve, &quot;lomolino&quot;) coef(mod1) ## Asym xmid slope ## 258.440682 2.442061 1.858694 plot(AcCurve) ## Add Lomolino model using argument &#39;add&#39; plot(mod1, add = TRUE, col=2, lwd=2) There are a range of “non parametric” estimators of species richness that claim to do the same thing. pool&lt;-apply(BCI,2,sum) estimateR(pool) ## S.obs S.chao1 se.chao1 S.ACE se.ACE ## 225.000000 237.214286 7.434824 238.217659 7.118588 However if we look at the accumulation on a log scale we may be less convinced that it is flattening off at all. plot(AcCurve, ci.type=&quot;poly&quot;, col=&quot;red&quot;, lwd=2, ci.lty=0, ci.col=&quot;grey&quot;,log=&quot;x&quot;) The practical implications are that when planning field work you may quickly observe at least half the species that you are going to find in any given area. However rare species will continue to add to the list, and there may be no real limit to this process. Because the accumulation is more or less linear on a logarithmic scale there are clearly diminishing returns. If, say, six more species are added by doubling the number of sites, four times the number of sites will be needed to add six more, eight more for the next six and so on. In this case you should notice that you expect to find around half the total number of species if you draw two sites at random from the data set and count the numbers of species. This may be useful when designing future studies. The dune meadows data set found in the vegan package uses quadrat cover classes, so the numbers do not represent counts. The default settings will be OK for this sort of data. data(dune) AcCurve&lt;-specaccum(dune) plot(AcCurve) 19.2 Resampling individuals Some of the methods for resampling species accumulation curves (eg. Colemans, and rarefaction) are based on individuals. We can try an alternative form of resampling at the level of each plot. resamp2 &lt;- function(X, rep=1,plot=FALSE) { require(vegan) Samp &lt;- function(size, Indivs) { length(table(sample(Indivs, size, replace=F))) } N&lt;-X[X&gt;0] TotSp &lt;- length(N) TotInds&lt;-sum(N)#### Accumulation curve up to the maximum number of #individuals Sp &lt;- 1:TotSp Inds &lt;- rep(1:TotSp, N) Size &lt;- rep(floor(TotInds*0.25):TotInds, rep) ###Note its(TotInds*0.25):TotInds sp.count &lt;- sapply(Size, Samp, Inds) if(plot)plot(Size,sp.count,xlab=&quot;Number of individuals&quot;,ylab=&quot;Number of species&quot;,log=&quot;x&quot;) Sp.Ac1&lt;-lm(sp.count~log(Size)) Sp.Ac2&lt;-lm(sp.count~log2(Size)) fa&lt;-fisher.alpha(N) res&lt;-list(Logslope=Sp.Ac1$coefficients[2],Log2slope=Sp.Ac2$coefficients[2],Alpha=fa) return(res) } resamp2(BCI[1,],plot=TRUE) ## $Logslope ## log(Size) ## 30.85559 ## ## $Log2slope ## log2(Size) ## 21.38747 ## ## $Alpha ## [1] 35.67297 ans&lt;-apply(BCI,1,resamp2) res&lt;-do.call(&quot;rbind&quot;,ans) datatable(res) %&gt;% formatRound(columns=c(1:3), digits=1) Once more, this pattern has some useful practical implications and some deeper theoretical ones. By looking at the slope after log transforming to base 2 we obtain an estimate of the number of new species which would be added each time we double the number of individuals counted. 19.2.1 Simpson’s and Shannon’s indices Ok, we have seen that species abundance distributions tend to be very highly skewed, with a few common species and many more rare species. The counts of individuals in any one sampling unit will reflect this. Intuitively there is a difference between a sampling unit that contains 10 individuals of species A, 10 of species B and 10 of species C when it is compared to a sampling unit that has 28 of species A and only one individual of each of the other species. There is a very long tradition in Ecology of measuring diversity using indices that combine measures of both species richness and equitability. There are two widely used indices of diversity, and around 30 that are less well known. 19.2.1.1 Simpson’s index The simplest diversity index to understand is Simpson’s index. Let’s take the example of an equitable community with ten individuals of five species. The proportional abundance for each species is 0.2. eq&lt;-c(10,10,10,10,10) prop&lt;-eq/sum(eq) prop ## [1] 0.2 0.2 0.2 0.2 0.2 Now imagine we draw an individual at random from the community. It could be of any one of the five species with an equal probability of 0.2. Let’s say that it is species A. If we replace the individual and draw another, what is the probability that it will also be of species A? The answer must also be 0.2. So the probability of drawing two individuals of species A is 0.2 x 0.2 = 0.2² = 0.04. The same applies to all the other species. So the overall probability of drawing two individuals of the same species is given by the sum of p². We can call this D. If we subtract D from 1 we have the probability of drawing two individuals at random that are of different species. Alternatively we can find the reciprocal of D which represents the number of equally abundant species that would provide the probability obtained. \\(D=\\sum p^{2}\\) \\(simp=1-D\\) \\(invsimp=\\frac{1}{D}\\) sprop&lt;-prop^2 sprop ## [1] 0.04 0.04 0.04 0.04 0.04 D&lt;-sum(sprop) 1-D ## [1] 0.8 D ## [1] 0.2 Now what do we find for a much less equitable community? uneq&lt;-c(100,10,2,2,1) prop&lt;-uneq/sum(uneq) prop ## [1] 0.869565217 0.086956522 0.017391304 0.017391304 0.008695652 In this case we are much more likely to draw an individual of the common species first. We are then very likely to draw another. So the probability of getting two individuals of the same species is much higher. sprop&lt;-prop^2 sprop ## [1] 7.561437e-01 7.561437e-03 3.024575e-04 3.024575e-04 7.561437e-05 D&lt;-sum(sprop) 1-D ## [1] 0.2356144 1/D ## [1] 1.30824 The probability of obtaining two individuals of the same species is higher, as we are likely to draw two individuals of the very common species. Thus the ``effective’’ number of species as measured by the inverse of Simpson’s index is lower. Simpson’s index is influenced by both the number of species and the equitability of the distribution. We can obtain a measure of pure equitability that takes a value between zero and 1 by dividing the inverse of simpsons’s by the number of species. Simp&lt;-diversity(BCI,&quot;simp&quot;) Simp ## 1 2 3 4 5 6 7 8 ## 0.9746293 0.9683393 0.9646078 0.9716117 0.9678267 0.9627557 0.9672014 0.9671998 ## 9 10 11 12 13 14 15 16 ## 0.9534257 0.9663808 0.9658398 0.9550599 0.9692075 0.9718626 0.9709057 0.9686598 ## 17 18 19 20 21 22 23 24 ## 0.9545126 0.9676685 0.9655820 0.9748589 0.9686058 0.9548316 0.9723529 0.9694268 ## 25 26 27 28 29 30 31 32 ## 0.9726152 0.9709567 0.9669962 0.9499296 0.9481041 0.9602659 0.9635807 0.9565267 ## 33 34 35 36 37 38 39 40 ## 0.9586946 0.9607876 0.7983976 0.9648567 0.9565015 0.9365144 0.9360204 0.9137131 ## 41 42 43 44 45 46 47 48 ## 0.9731442 0.9731849 0.9569632 0.9578733 0.9528853 0.9646728 0.9672083 0.9676412 ## 49 50 ## 0.9609552 0.9679784 InvSimp&lt;-diversity(BCI,&quot;invsimp&quot;) InvSimp ## 1 2 3 4 5 6 7 8 ## 39.415554 31.584877 28.254778 35.225771 31.081658 26.849731 30.489077 30.487609 ## 9 10 11 12 13 14 15 16 ## 21.471056 29.744868 29.273803 22.251827 32.475442 35.539830 34.371014 31.907937 ## 17 18 19 20 21 22 23 24 ## 21.984098 30.929617 29.054548 39.775448 31.853042 22.139382 36.170213 32.708387 ## 25 26 27 28 29 30 31 32 ## 36.516636 34.431303 30.299530 19.971863 19.269343 25.167317 27.457940 23.002620 ## 33 34 35 36 37 38 39 40 ## 24.209883 25.502106 4.960258 28.454909 22.989309 15.751596 15.629977 11.589250 ## 41 42 43 44 45 46 47 48 ## 37.235945 37.292428 23.235938 23.737903 21.224806 28.306797 30.495526 30.903463 ## 49 50 ## 25.611603 31.228916 Es&lt;-InvSimp/specnumber(BCI) Es ## 1 2 3 4 5 6 7 ## 0.42382316 0.37601044 0.31394198 0.37474225 0.30773918 0.31587919 0.37181801 ## 8 9 10 11 12 13 14 ## 0.34645010 0.23856729 0.31643477 0.33648049 0.26490271 0.34919830 0.36265132 ## 15 16 17 18 19 20 21 ## 0.36958080 0.34309609 0.23638815 0.34752379 0.26655549 0.39775448 0.32174790 ## 22 23 24 25 26 27 28 ## 0.24328991 0.36535568 0.34429881 0.34777748 0.37836597 0.30605585 0.23496309 ## 29 30 31 32 33 34 35 ## 0.22406212 0.25945688 0.35659662 0.26139341 0.28151027 0.27719680 0.05976214 ## 36 37 38 39 40 41 42 ## 0.30929249 0.26124214 0.19209264 0.18607116 0.14486563 0.36505828 0.42864860 ## 43 44 45 46 47 48 49 ## 0.27018532 0.29306053 0.26203464 0.32914881 0.29897574 0.33959850 0.28144618 ## 50 ## 0.33579479 19.2.1.2 Shannon’s index Shannon’s index (sometimes called the Shannon-Wienner index) is a commonly used diversity index that is rather similar in practice to Simpson’s index. Unfortunately very few users of the index have an intuitive idea of it’s meaning, as it is based on some rather obscure information theory. \\(H=-\\sum p.log(p)\\) The values that H can take tend to vary between 0.8 and 4. Values above 2.5 are high, but there is no simple rule. Fortunately Shannon’s index can easily be converted into an index of equitability (or evenness) in a rather similar way to Simpson’s by dividing it by the maximum value that it could take, which in this case is log(S). The index downweights rare species rather more than does Simpson’s index. The two indices basically measure the same thing. Simpson’s is much simpler to understand and can be used by default. Shannon’s is used by tradition as it was thought to be the better measure for many years. Shannon’s is therefore always worth calculating and reporting for comparative purposes. Simpson’s and Shannon’s indices are always highly correlated. An empirical correlation between species richness per se and evenness also tends to occur, but this is not automatic. H&lt;-diversity(BCI,&quot;shannon&quot;) Eh&lt;-H/log(specnumber(BCI)) div&lt;-data.frame(S,Simp,InvSimp,H,Es,Eh) source(&quot;https://tinyurl.com/aqm-data/QEScript&quot;) Xpairs(div) The main reason that quantitative ecologists calculte diversity indices in most cases is to relate them to environmental variables. The two components of diversity are species richness and equitability (evenness). Either or both of these can take be modelled as functions of environmental variables. A significant reduction in evenness can, in appropriate circumstances, be taken as a symptom of environmental degradation. Low evenness scores can be the result of a community being dominated by a few species that are tolerant to, or favoured by, anthropogenic pollution or disturbance. However scores must be placed in context. The size of the site (quadrat or soil core) can have a great influence on diversity indices and this must always be taken into account. 19.2.2 Exercises 19.2.2.1 Mexican trees The first exercise involves data taken from three forest types in Mexico. The abundances are counts of individual trees. What differences are there in species diversity between the forest types? #mexveg&lt;- read.csv(&quot;http://tinyurl.com/QEcol2013/mexveg.csv&quot;) #mexmat&lt;-mexveg[,-c(1,2)] "],
["analysing-patterns-in-species-composition.html", "Chapter 20 Analysing patterns in species composition 20.1 Differences between sites 20.2 Mantel tests 20.3 Correlation with environmental variables 20.4 Ordination 20.5 Non-metric multi dimensional scaling NMDS 20.6 Single gradient NMDS 20.7 Multiple axis NMDS 20.8 Clustering 20.9 Clam test 20.10 Simper 20.11 Clustering by species 20.12 Anosim 20.13 Adonis 20.14 Canonical Correspondence Analysis", " Chapter 20 Analysing patterns in species composition In many ecological settings multivariate analysis is concerned with looking at gradients of species composition or finding methods to classify sites based on their species composition. If you think about the methods that you have seen so far you may “reinvent” one of the earliest methods of analysing species composition. You could think about finding the correlation (or rank correlation) between the occurrence of one species and another. If you do this for all possible pairs then you will obtain a matrix like this data(BCI) cmat&lt;-cor(BCI,method=&quot;spearman&quot;) This is data “overload” for a data set such as the BCI plots. Although you could possibly tease out some relationships from this for a few species that you know well, it is difficult to spot any pattern. A data reduction technique that can be applied to any correlation matrix is principal components analysis. This could be used if the number of species were small in relation to the number of sites. The technique is often useful when dealing with multi-colinearity of environmental variables. However principal components analysis will not work in a meaningful sense with so many variables. Let’s try PCA using only the most common species. selected&lt;-names(sort(apply(BCI,2,sum),dec=T)[1:5]) selected ## [1] &quot;Faramea.occidentalis&quot; &quot;Trichilia.tuberculata&quot; &quot;Alseis.blackiana&quot; ## [4] &quot;Oenocarpus.mapora&quot; &quot;Poulsenia.armata&quot; BCIcommon&lt;-BCI[,names(BCI)%in%selected] cor(BCIcommon) ## Alseis.blackiana Faramea.occidentalis Oenocarpus.mapora ## Alseis.blackiana 1.0000000 0.30314087 -0.25984312 ## Faramea.occidentalis 0.3031409 1.00000000 -0.09437266 ## Oenocarpus.mapora -0.2598431 -0.09437266 1.00000000 ## Poulsenia.armata -0.3643551 -0.64995053 -0.25869865 ## Trichilia.tuberculata 0.3747695 0.45599388 -0.28713347 ## Poulsenia.armata Trichilia.tuberculata ## Alseis.blackiana -0.3643551 0.3747695 ## Faramea.occidentalis -0.6499505 0.4559939 ## Oenocarpus.mapora -0.2586987 -0.2871335 ## Poulsenia.armata 1.0000000 -0.3482301 ## Trichilia.tuberculata -0.3482301 1.0000000 Xpairs(BCIcommon) pca&lt;-princomp(BCIcommon,cor=F,scores=T) plot(pca) summary(pca) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Standard deviation 23.380136 13.9622989 12.3222552 9.47657946 5.31946450 ## Proportion of Variance 0.540407 0.1927262 0.1501092 0.08878306 0.02797453 ## Cumulative Proportion 0.540407 0.7331332 0.8832424 0.97202547 1.00000000 Let’s take this output a step at a time. The scree plot shows how much of the variability is captured by each principal component. If we want to reduce the number of variables effectively we would like to see a steep scree plot. This would show a high degree of muktiple colinearity, which would in this sort of case be useful (the abundance of one species is a good predictor of the abundance of others). The summary shows how much of the “explainable” variability is captured by each axes. When we have as many axes as variables the cumulative variance explained will be 1. This does NOT mean that all the variability in the data is captured by five axes, simply that all the explainable variability has been. If you have variables that are measured on different scales you should either transform them onto a common scale before running PCA or use the version of PCA that takes the correlation matrix. In this case I used cor=FALSE. PCA provides a set of loadings for each variable. If cor=FALSE the loadings correspond to a simple linear equation of the form \\(score1=loading1*var1+loading2*var2 .. etc\\) loadings(pca) ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Alseis.blackiana 0.393 0.514 0.734 0.112 0.173 ## Faramea.occidentalis 0.571 -0.358 -0.210 0.674 0.218 ## Oenocarpus.mapora -0.301 -0.384 0.868 ## Poulsenia.armata -0.529 0.496 -0.236 0.510 0.398 ## Trichilia.tuberculata 0.488 0.520 -0.595 -0.356 0.102 ## ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## SS loadings 1.0 1.0 1.0 1.0 1.0 ## Proportion Var 0.2 0.2 0.2 0.2 0.2 ## Cumulative Var 0.2 0.4 0.6 0.8 1.0 normalise&lt;-function(x){ (max(x)-x)/(max(x)-min(x)) } normalise(scores(pca)[,1]) ## 1 2 3 4 5 6 7 8 ## 0.7945789 0.5852645 0.6533543 0.5378946 0.8021029 0.7168187 0.5633315 0.5450559 ## 9 10 11 12 13 14 15 16 ## 0.4030080 0.5486166 0.6991813 0.4694122 0.5160359 0.6038255 0.6228736 0.7280851 ## 17 18 19 20 21 22 23 24 ## 0.4224111 0.6011511 0.4608798 0.6949454 0.9023451 0.3527045 0.6629439 0.5952049 ## 25 26 27 28 29 30 31 32 ## 0.5544068 0.7672981 0.5302189 0.3453368 0.3596322 0.3364153 0.6831697 0.2855110 ## 33 34 35 36 37 38 39 40 ## 0.3862426 0.3524696 0.2225161 0.6620093 0.3698350 0.1141841 0.1723414 0.0000000 ## 41 42 43 44 45 46 47 48 ## 0.7207904 0.8027668 0.9235511 0.9514009 1.0000000 0.6266537 0.6623081 0.8261893 ## 49 50 ## 0.9160899 0.9879240 The data provided in the vegan package does not include any information on measured local environmental variability (micro topography, soil, moisture etc). However the position of the plots are provided, as they were placed on a rectangular grid. If we design a plot to show one of the PCA axes we might pick up a spatial pattern (based only on the five commonest species) x&lt;- rep(seq(625754, 626654, by=100),each=5) y&lt;- rep(seq(1011569, 1011969, by=100),len=50) coords&lt;-data.frame(x,y) plot(coords,pch=21,bg=2,cex=3*normalise(scores(pca)[,1])) str(pca) library(rgl) spheres3d(pca$scores[,1:3],radius=5,col=&quot;red&quot;) rgl.bbox() 20.1 Differences between sites Another way of condensing the sites by species matrix into a simpler form is based on an investigation of total compositional difference. This can be conducted on completelya priori grounds. A simple example is when species have been classified into taxonomic or functional groups. The proportional abundance in each group can then form a response variable. An example may be pioneer species and species of mature forest, or soft bodied invertebrates compared to hard bodied invertebrates. However there is often a need to look at compositional differences that are derived from the data themselves. The starting point for many analyses involves calculating differences (or similarities) between sites. As we have seen, PCA does this, but it assumes linearity and is based on the same assumptions as simple linear regresion, which may not be appropriate in many cases. It is not a suitable method for data sets representing long ecological gradients with high species turnover. The issue of how best to measure differences between sites is a potentially difficult one. It all depends upon what you mean be a difference. Let’s first take the simplest case in which only presence or absence of species are considered. Imagine just two sites. Site one has species A, B, C and D present. But site 2 has species A,B, E, F and G. library(limma) Venn1&lt;-function(vec1,vec2,names=c(&quot;Site 1&quot;,&quot;Site 2&quot;)){ d1&lt;-data.frame(id=sitea,abun1=1) d2&lt;-data.frame(id=siteb,abun2=1) d&lt;-merge(d1,d2,all=T) d[is.na(d)]&lt;-0 g &lt;- cbind(d$abun1, d$abun2) vennDiagram(vennCounts(g),names=names) t(g) } Venn2&lt;-function(g){ vennDiagram(vennCounts(t(g))) } #Example sitea&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;) siteb&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;e&quot;,&quot;f&quot;,&quot;g&quot;) Venn1(sitea,siteb) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1 1 1 1 0 0 0 ## [2,] 1 1 0 0 1 1 1 One way of measuring the difference (or similarity) between the two sites would therfore be to look at the ratio of the number of shared species (2) against the total number of records made of species presences (4+5). This is known as Sorenson’s index. \\(Sorenson=\\frac{2c}{A+B}\\) Where c= number of shared species, A = number of presence records for site A and B =number of presence records for site B. So the difference is 2*2/4+5 This can be extended to count data by simply weighting the values by the number of individuals. The index is then commonly known as the Bray Curtis index. There are some important issues to be aware of when thinking about differences. What happens if site 1 has species A and B, while site 2 has species A,B,C,D, E and F? sitea&lt;-c(&quot;a&quot;,&quot;b&quot;) siteb&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;) Venn1(sitea,siteb) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 1 0 0 0 0 ## [2,] 1 1 1 1 1 1 In one respect the sites could be considered to be identical, as the species in site 1 are just a nested sub set of those in site 2. In another sense they are quite different because species richness is clearly not the same. There are a range of other difference indices available in addition to Sorenson’s. These weight such situations in different ways, and can result in quite different outcomes. You should be aware of this issue if you go on to use these methods in a dissertation. You may need to look in more depth at other indices, depending on the questions you wish to adress. For our purposes we will use Sorenson’s (Bray-Curtis) dissimilarity as an acceptable measure of differences between sites. Assuming that we have decided on a way of measuring the difference between two sites another question then arises. Why would we want to do this at all? If we have fifty sites there are 1225 paired comparisons between them. How can we possibly interpret the results? A common reason for calculating differences between sites is to look for patterns of change in species composition. This can be achieved in many different ways. The BCI data appeared to come from a rather homogeneous area of tropical forest, with high species diversity. So we might expect that any differences between sites may be just the result of sampling issues. The plots simply are not large enough to include all the species. Can we look at this? 20.2 Mantel tests One way to look for a pattern in species composition is to relate differences between sites in species composition to other differences between sites. The simplest measure of a difference between two sites is simply the distance between them. The so called first law of geography (that has been attributed to Tobler) is Everything is related to everything else, but near things are more related than distant things. This is generally true, but can we test this? Well first let’s extract the Bray Curtis difference between sites in the BCI data. compdist&lt;-vegdist(BCI) geodist&lt;-dist(coords,&quot;euclidean&quot;) plot(compdist~geodist) There seems to be a relationship, although there is a lot of noise. However we can’t simply use correlation or regression analysis to test the significance of the difference because a difference matrix has a lot of duplicated information. Instead we use a permutation based procedure called a Mantel test. mantel(compdist,geodist) ## ## Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## mantel(xdis = compdist, ydis = geodist) ## ## Mantel statistic r: 0.4078 ## Significance: 0.001 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.0710 0.0884 0.1074 0.1231 ## Permutation: free ## Number of permutations: 999 There is a significant, and quite high, correlation between differences in composition and geographical distance. 20.3 Correlation with environmental variables The previous example did not use any environmental variables. It only allowed us to conclude that the sites showed some spatial dependence. Mapping the position of sites with similar composition may help to show the pattern. We will come on to that later. How could we use Mantel tests to look at the relationship with environmental variability? One issue is that if we want to use Euclidean distances the environmental variables should be on the same scale. The function bioenv cleverly finds the best subset of environmental variables, so that the Euclidean distances of scaled environmental variables have the maximum (rank) correlation with community dissimilarities. The example below uses the RIKZ data discussed by Zuur. You should look carefully at the data and Zuur’s example analysis. RIKZsp&lt;-read.csv(&quot;/home/aqm/data/Examples/RIKZsp.csv&quot;) RIKZenv&lt;-read.csv(&quot;/home/aqm/data/Examples/RIKZenv.csv&quot;) bioenv(RIKZsp ~., RIKZenv) ## ## Call: ## bioenv(formula = RIKZsp ~ ., data = RIKZenv) ## ## Subset of environmental variables with best correlation to community data. ## ## Correlations: spearman ## Dissimilarities: bray ## Metric: euclidean ## ## Best model has 2 parameters (max. 4 allowed): ## height exposure ## with correlation 0.5887359 This technique may be particularly useful for screening variables for further work. 20.4 Ordination One of the goals when analysing a sites by species matrix is to attempt to place sites in some sort of logical order based on their composition. The expectation is that the order of sites will reflect one or more environmental gradients. Therefore the techniques are used in direct and indirect gradient analysis. Indirect gradient analysis attempts to ordinate the sites first, and then explain the results in terms of environmental gradients. Traditionally the techniques used for this include correspondence analysis and detrended correspondence analysis. These are forms of eigenanalyses that are conducted on the sites by species matrix. One of the issues with these technqies is the so called arch effect. This is an artefect that is removed by detrending. As detrending is a rather ``ad hoc’’ procedure some influential authors and editors (particularly Jari Oksanen) recommend multi dimensional scaling instead. So we will look at this first. 20.5 Non-metric multi dimensional scaling NMDS Difference matrices are hard to understand because of the large number of dimensions. Multidimensional scaling tries to find one or more axes for the data that produce a configuration of the points in such a way that the distances between them are as close as possible to the actual distances in the distance matrix. For non-metric MDS, only the rank order of entries in the data matrix (not the actual dissimilarities) is assumed to contain the key information. So in this case, the distances of the final configuration should as far as possible be in the same rank order as the original data. 20.6 Single gradient NMDS A straightforward way of using NMDS is to ask the algorithm to line up the species along one single axis. Some information is lost by doing this, and the technique may introduce some artefacts. However it is then easy to use the variable in other models (such as regression, or GAM’s) as a simple measure of change in species composition. library(mgcv) nmds&lt;-metaMDS(RIKZsp,k=1) ## Square root transformation ## Wisconsin double standardization ## Run 0 stress 0.2843762 ## Run 1 stress 0.2837468 ## ... New best solution ## ... Procrustes: rmse 0.04975478 max resid 0.1904147 ## Run 2 stress 0.2641231 ## ... New best solution ## ... Procrustes: rmse 0.1095776 max resid 0.3320365 ## Run 3 stress 0.2736419 ## Run 4 stress 0.2848651 ## Run 5 stress 0.2585569 ## ... New best solution ## ... Procrustes: rmse 0.04905002 max resid 0.228782 ## Run 6 stress 0.2848723 ## Run 7 stress 0.2629673 ## Run 8 stress 0.2549915 ## ... New best solution ## ... Procrustes: rmse 0.02175033 max resid 0.1185243 ## Run 9 stress 0.2628508 ## Run 10 stress 0.2629189 ## Run 11 stress 0.2765355 ## Run 12 stress 0.2891089 ## Run 13 stress 0.2578624 ## Run 14 stress 0.2795548 ## Run 15 stress 0.2719987 ## Run 16 stress 0.2877859 ## Run 17 stress 0.2850045 ## Run 18 stress 0.2831328 ## Run 19 stress 0.2819743 ## Run 20 stress 0.2840513 ## *** No convergence -- monoMDS stopping criteria: ## 20: stress ratio &gt; sratmax grad&lt;-scores(nmds,&quot;sites&quot;) mod&lt;-gam(grad~s(height,k=4)+s(exposure,k=3),data=RIKZenv) anova(mod) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## grad ~ s(height, k = 4) + s(exposure, k = 3) ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(height) 2.27 2.65 8.829 0.000216 ## s(exposure) 1.00 1.00 22.129 3.56e-05 par(mfcol=c(1,2)) plot(mod) The stress of an NMDS is a measure of how close to the true rank order the scaled values are. The lower the stress the better. There is no clear rule governing how much stress is tolerable, but Kruskall advised that ordinations with stress values of over 0.2 are likely to be unreliable. This ordination just falls just above this level. It can sometimes be useful to look at a Shephard diagram for the ordination. This shows graphically how the stress arises by plotting the distance between the points in ordination space against observed dissimilarity. stressplot(nmds) 20.7 Multiple axis NMDS The more axes that are used in an NMDS the lower the overall stress. One of the reasons to use several axes is to look at the configuration of the points with respect to environmental variables and attempt to explain the resulting patterns. Understanding the results may require some further thought and work, as it is harder to see patterns in two or more dimensions. nmds&lt;-metaMDS(RIKZsp,k=2) ## Square root transformation ## Wisconsin double standardization ## Run 0 stress 0.169786 ## Run 1 stress 0.1710791 ## Run 2 stress 0.2002069 ## Run 3 stress 0.173308 ## Run 4 stress 0.1700876 ## ... Procrustes: rmse 0.008924239 max resid 0.02894342 ## Run 5 stress 0.173412 ## Run 6 stress 0.1749756 ## Run 7 stress 0.1697888 ## ... Procrustes: rmse 0.001114526 max resid 0.002818072 ## ... Similar to previous best ## Run 8 stress 0.1700916 ## ... Procrustes: rmse 0.008992612 max resid 0.02866552 ## Run 9 stress 0.1700971 ## ... Procrustes: rmse 0.01028124 max resid 0.04722189 ## Run 10 stress 0.169786 ## ... New best solution ## ... Procrustes: rmse 0.0002504292 max resid 0.0006553166 ## ... Similar to previous best ## Run 11 stress 0.1697862 ## ... Procrustes: rmse 0.0001865756 max resid 0.0004629947 ## ... Similar to previous best ## Run 12 stress 0.173412 ## Run 13 stress 0.1733055 ## Run 14 stress 0.1998082 ## Run 15 stress 0.1697889 ## ... Procrustes: rmse 0.001335313 max resid 0.003557728 ## ... Similar to previous best ## Run 16 stress 0.1697862 ## ... Procrustes: rmse 0.0002993492 max resid 0.0008210701 ## ... Similar to previous best ## Run 17 stress 0.173305 ## Run 18 stress 0.1950252 ## Run 19 stress 0.1697869 ## ... Procrustes: rmse 0.0004946049 max resid 0.001266879 ## ... Similar to previous best ## Run 20 stress 0.1940953 ## *** Solution reached stressplot(nmds) One way to look at the pattern of dissimilarites in two dimensional space that is produced by NMDS is to plot contours representing the smoothed (using gam) mean values of environmental variables. For example we can plot the NMDS results with contours for height above sea level (blue) and exposure value (red). ordisurf(nmds,RIKZenv$height,col=&quot;blue&quot;) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x1, x2, k = 10, bs = &quot;tp&quot;, fx = FALSE) ## ## Estimated degrees of freedom: ## 5.81 total = 6.81 ## ## REML score: 36.39946 ordisurf(nmds,RIKZenv$exposure,col=&quot;red&quot;,add=T) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x1, x2, k = 10, bs = &quot;tp&quot;, fx = FALSE) ## ## Estimated degrees of freedom: ## 5.08 total = 6.08 ## ## REML score: 42.63227 A more traditional way of looking at this is to assume linearity and show the strength and direction of the linear relationship as vectors on the ordination plot. This may hide the shape of the relationship, but emphasises the size of the linear correlation. fit &lt;- envfit(nmds~height+exposure, data=RIKZenv, perm = 999) plot(nmds) plot(fit) 20.8 Clustering Another way of working with the distance matrix is to attempt to group the data. Looking at the BCI data again. vdist&lt;-vegdist(BCI) clus&lt;-hclust(vdist) plot(clus) There seems to be two main divisions. This may be useful in tracing the spatial pattern that we observed. We can cut the tree to find a grouping variable. grp&lt;-cutree(clus,2) plot(coords,pch=21,bg=grp,cex=2) The results may be related to a slight ridge that appears along the middle of the plot. Or it may indicate disturbance of some kind. alt text 20.9 Clam test The clam function may help in finding which species are specialists within the groups of sites. clam&lt;-clamtest(BCI, grp==1, alpha=0.005) ##clam plot(clam) In this case the division is not very convincing. However notice that the group contains comparatively high abundance of Socratea exorrhiza, the so called ``walking palm’’. This may be indicative of recent disturbance. Thus the technique may have located a past forest opening due perhaps to logging. Poulsenia armata is another pioneer species found in this group. 20.10 Simper Another way of looking at which species are responsible for the grouping is to run a Simper analysis. This provides measures of the contribution of individual species to the overall Bray-Curtis dissimilarity between two sites. If the species are placed in order the analysis helps to find species that discriminate between the groups. simp&lt;-simper(BCI, grp) ## summary(simp,ordered=T) head(summary(simp,ordered=T)[[1]],15) ## average sd ratio ava avb ## Faramea.occidentalis 0.028345590 0.017562085 1.6140218 25.419355 48.894737 ## Gustavia.superba 0.022816427 0.052073612 0.4381572 5.483871 24.947368 ## Poulsenia.armata 0.022236536 0.017492076 1.2712348 21.967742 3.894737 ## Trichilia.tuberculata 0.022154916 0.019967568 1.1095450 27.838710 43.052632 ## Alseis.blackiana 0.018448477 0.019468124 0.9476248 14.032258 28.842105 ## Hirtella.triandra 0.013510798 0.011082365 1.2191259 16.903226 8.263158 ## Socratea.exorrhiza 0.011101236 0.013918844 0.7975688 9.677419 2.421053 ## Oenocarpus.mapora 0.010109309 0.007309825 1.3829755 16.870968 13.947368 ## Quararibea.asterolepis 0.009835408 0.007011586 1.4027364 13.483871 16.105263 ## Prioria.copaifera 0.009740702 0.008321527 1.1705427 5.677419 8.894737 ## Drypetes.standleyi 0.008327951 0.009723961 0.8564361 7.096774 3.421053 ## Guarea.guidonia 0.007609609 0.006879644 1.1061049 6.903226 8.526316 ## Virola.sebifera 0.007518728 0.005622919 1.3371575 13.322581 10.736842 ## Cordia.bicolor 0.007385838 0.007516220 0.9826532 8.419355 3.368421 ## Beilschmiedia.pendula 0.006962698 0.008382180 0.8306548 6.741935 4.473684 ## cumsum ## Faramea.occidentalis 0.05956732 ## Gustavia.superba 0.10751528 ## Poulsenia.armata 0.15424462 ## Trichilia.tuberculata 0.20080244 ## Alseis.blackiana 0.23957130 ## Hirtella.triandra 0.26796380 ## Socratea.exorrhiza 0.29129268 ## Oenocarpus.mapora 0.31253705 ## Quararibea.asterolepis 0.33320584 ## Prioria.copaifera 0.35367560 ## Drypetes.standleyi 0.37117651 ## Guarea.guidonia 0.38716785 ## Virola.sebifera 0.40296821 ## Cordia.bicolor 0.41848930 ## Beilschmiedia.pendula 0.43312118 20.11 Clustering by species Species can also be clustered by transposing the matrix. This should cluster species that co-occur in the same sites together. In the case of the large number of species found in the BCI data interpretation is a major task! It is only worth looking at the commonest species. BCI2&lt;-BCI[,apply(BCI,2,sum)&gt;100] spdist&lt;-vegdist(t(BCI2)) clus&lt;-hclust(spdist) plot(clus) 20.12 Anosim Although we may be able to find ways to group sites, we should use a statistical test of some kind in order to establish whether the differences between groups are significant. If we can divide the data by groups (factors), such as sediment type, we can run an equivalent of a multiple analysis of variance (MANOVA) using the distance matrix as input. This technique is known as Anosim. It is particularly widely used by marine ecologists as a result of early implementation in the software Primer. Anosim tests whether compositional dissimilarities between groups is greater than those within the groups. Plotting the results in R produces a notched boxplot that can be interpreted in the usual way (if the notches do not overlap then the median dissmilarity is significantly different). We could use Anosim to test whether there really is a significant difference between the two groups of sites we obtained by cutting the cluster analysis of the BCI data. aa&lt;-anosim(BCI,as.factor(grp)) aa ## ## Call: ## anosim(x = BCI, grouping = as.factor(grp)) ## Dissimilarity: bray ## ## ANOSIM statistic R: 0.2698 ## Significance: 0.001 ## ## Permutation: free ## Number of permutations: 999 plot(aa) 20.13 Adonis Using Adonis is rather like fitting a multiple linear models to all the species in the matrix. It is also based on permutation tests and partitions the sums of squares of a multivariate data set in an analogous to MANOVA (multivariate analysis of variance). The inputs are linear predictors with the response being a matrix of more than two columns. Adonis tends to be very (too) powerful. In other words it will show significant relationships where other methods fail. This is because it only really needs one of the multiple species in the data set to have a strong relationship for the method to produce significance. Whether you can intepret the results easily is another matter. mod&lt;-adonis(RIKZsp~.,data=RIKZenv) mod ## ## Call: ## adonis(formula = RIKZsp ~ ., data = RIKZenv) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F) ## salinity 1 0.8701 0.87013 2.8108 0.06384 0.001 *** ## height 1 1.2030 1.20299 3.8861 0.08826 0.001 *** ## grain 1 0.6368 0.63676 2.0570 0.04672 0.009 ** ## exposure 1 0.7044 0.70441 2.2755 0.05168 0.005 ** ## Residuals 33 10.2156 0.30956 0.74950 ## Total 37 13.6299 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Asking for coefficents will produce a long list of the coefficents for each species (not shown). #coefficents(mod) library(rgdal) ## Loading required package: sp ## rgdal: version: 1.4-3, (SVN revision 828) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 2.1.2, released 2016/10/24 ## Path to GDAL shared files: /usr/share/gdal/2.1 ## GDAL binary built with GEOS: TRUE ## Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493] ## Path to PROJ.4 shared files: (autodetected) ## Linking to sp version: 1.3-1 dd&lt;-data.frame(coords,grp) coordinates(dd)&lt;-~x+y 20.14 Canonical Correspondence Analysis Canonical correspondence analysis is a rather complex technique that combines ordination with regression in one algorithm. The technique ordinates sites and species along axes that are constrained to be linear combinations of environmental variables. For this reason the proximity of sites on a bi-plot may not reflect similarity in species composition as well as NMDS. A screeplot shows the eigenvalues for each axis. The total ``inertia’’ o the ordination is the sum of all the eigenvalues, not just those shown on the screeplot. In order to cano&lt;-cca(RIKZsp~.,data=RIKZenv) screeplot(cano) The technique can be used instead of Adonis in order to look at which species-environment relationships are most important. One way to do this is to treat the model rather like a multiple regression. In R the AIC is calculated for the model. Thus backwards stepwise methods can be used to drop variables from the model. Burnham and Anderson type approaches could also be used for model selection, although there is no package in R that automates this. cano&lt;-step(cano) ## Start: AIC=251.79 ## RIKZsp ~ salinity + height + grain + exposure ## ## Df AIC ## - grain 1 250.94 ## - salinity 1 250.96 ## &lt;none&gt; 251.79 ## - exposure 1 252.38 ## - height 1 252.83 ## ## Step: AIC=250.94 ## RIKZsp ~ salinity + height + exposure ## ## Df AIC ## - salinity 1 250.63 ## &lt;none&gt; 250.94 ## - exposure 1 251.68 ## - height 1 251.91 ## ## Step: AIC=250.64 ## RIKZsp ~ height + exposure ## ## Df AIC ## &lt;none&gt; 250.63 ## - height 1 251.53 ## - exposure 1 252.72 cano ## Call: cca(formula = RIKZsp ~ height + exposure, data = RIKZenv) ## ## Inertia Proportion Rank ## Total 8.126 1.000 ## Constrained 1.422 0.175 2 ## Unconstrained 6.704 0.825 32 ## Inertia is scaled Chi-square ## ## Eigenvalues for constrained axes: ## CCA1 CCA2 ## 0.9006 0.5217 ## ## Eigenvalues for unconstrained axes: ## CA1 CA2 CA3 CA4 CA5 CA6 CA7 CA8 ## 0.7666 0.7621 0.6101 0.5545 0.5121 0.4519 0.4474 0.4267 ## (Showing 8 of 32 unconstrained eigenvalues) After deciding on the model we can get a measure of the variance explained by looking at the proportion of the inertia that is attributed to the constrained axis. In this case it is 17.5%. Notice that because the axes are constrained to be linear combinations of environmental variables, they will tend to show clear orthogonality. In the case of the RIKZ data we have already seen that the relationship with height above sea level is not truly linear, thus the CCA has introduced some distortion. ordisurf(cano,RIKZenv$height,col=&quot;blue&quot;) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x1, x2, k = 10, bs = &quot;tp&quot;, fx = FALSE) ## ## Estimated degrees of freedom: ## 1.98 total = 2.98 ## ## REML score: 29.72099 ordisurf(cano,RIKZenv$exposure,col=&quot;red&quot;,add=T) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x1, x2, k = 10, bs = &quot;tp&quot;, fx = FALSE) ## ## Estimated degrees of freedom: ## 1.99 total = 2.99 ## ## REML score: 27.47425 The ordiplot function can be used to produce a dynamic biplot which allows labelling of species or sites. This avoids clutter. fig &lt;- ordiplot(cano) identify(fig, &quot;spec&quot;) “” "],
["simulating-and-analysing-data-from-questionnaires.html", "Chapter 21 Simulating and analysing data from questionnaires 21.1 Introduction 21.2 Simulating a single vector of Likert data 21.3 Numerical vectors to Likert vectors 21.4 Simulating responses that differ between subject specific variables 21.5 Statistical tests", " Chapter 21 Simulating and analysing data from questionnaires 21.1 Introduction The Likert scale is very commonly chosen for measuring the degree of agreement with statements in questionnaires. We can use the example of Likert scale data to look at a range of technical aspects regarding the way data is handled in R. The data collected from questionnaires is best handled using the approach used in relational data bases. At least three linked tables are usually needed. Likert data can be analysed using a range of multivariate analyses. In this example we will look at latent factor analysis which is closely related to principal components analysis. 21.1.1 Packages used ## Load the packages library(dplyr) library(aqm) library(ggplot2) library(tidyr) library(MASS) library(psych) 21.2 Simulating a single vector of Likert data To simulate responses from the Likert scale with known expected proportions we can write a function that takes the number of simulated responses required and a vector of proportion responding in each class. The following chunk builds a function that will by default return 1000 responses, which in this case will tend to be positive. rand_likert&lt;-function(n=1000, p=c(1,2,3,5,10)) { ## Form a vector of responses lscale&lt;-c(&quot;Strongly disagree&quot;,&quot;Disagree&quot;,&quot;Neutral&quot;,&quot;Agree&quot;,&quot;Strongly agree&quot;) ## Sample 1000 times with replacement and unequal probability lik&lt;-sample(lscale,n,replace=TRUE,prob=p) ## Turn the response into an ordered factor lik&lt;-factor(lik,lscale,ordered=TRUE) lik } Notice that the function works in the following way .. The “lscale” object is a vector of categorical responses. The second command samples responses from this vector with a probability given by the numbers in a second vector. If this is not provided then all the values have an equal probability of being chosen. So we can simulate responses that tend to be positive by providing a vector with high values on the right hand side (as in the default for the function) The result of sampling is a character vector. We can form the character vector into an ordered factor by providing a list of values in order. The lscale vector is such a list, so the function returned an ordered factor. The function has been included in the aqm package on the server. # If working offf the server the aqm package can be installed by running # devtools::install_github(&quot;dgolicher/aqm&quot;) 21.2.1 Character vectors and factors The example provides another opportunity to explain the difference between character vectors and factors. The function returns an ordered factor q1&lt;-rand_likert(100) str(q1) ## Ord.factor w/ 5 levels &quot;Strongly disagree&quot;&lt;..: 5 5 4 2 5 2 2 4 4 5 ... The vector actually consists of numerical values between 1 and 5 with the factor levels as labels. If we coerce the factor to a character vector then the vector will just contains text. q1&lt;-as.character(q1) str(q1) ## chr [1:100] &quot;Strongly agree&quot; &quot;Strongly agree&quot; &quot;Agree&quot; &quot;Disagree&quot; ... It is important to be aware that if data held in the form of text is read into R from a data file using read.csv then R will silently coerce the character vector to a factor. This behaviour differs from that of the read_csv function in the readr package which keeps the data as a character vector unless other instructions are provided. Notice that when we coerce a character vector for Likert scale text into a factor it will not automatically be in the order we would like. The default order will simply be the alphabetical order. q1&lt;-as.factor(q1) str(q1) ## Factor w/ 5 levels &quot;Agree&quot;,&quot;Disagree&quot;,..: 4 4 1 2 4 2 2 1 1 4 ... levels(q1) ## [1] &quot;Agree&quot; &quot;Disagree&quot; &quot;Neutral&quot; ## [4] &quot;Strongly agree&quot; &quot;Strongly disagree&quot; This is not what we want for analysis, as the factor is not in the right order. To produce an ordered factor we need to look at the levels of the factor which are produced by default and then place them in a more logical order. This step requires a little care, as there is the possibility for errors if text has not been used consistently when scoring the results. q1&lt;-factor(q1,levels(q1)[c(5,2,3,1,4)],ordered=TRUE) str(q1) ## Ord.factor w/ 5 levels &quot;Strongly disagree&quot;&lt;..: 5 5 4 2 5 2 2 4 4 5 ... 21.3 Numerical vectors to Likert vectors When collecting the real life data and capturing it in a spreadsheet the Likert scale may often be scored as numbers on a scale of 1 to 5. When the data is read into R the numbers will appear as integers. This can be changed num_to_likert&lt;-function(x){ lscale&lt;-c(&quot;Strongly disagree&quot;,&quot;Disagree&quot;,&quot;Neutral&quot;,&quot;Agree&quot;,&quot;Strongly agree&quot;) lik&lt;-factor(x,1:5,ordered=TRUE) levels(lik)&lt;-lscale lik } x&lt;-sample(1:5, 10,replace=TRUE) x ## [1] 4 2 2 5 4 2 1 3 5 3 num_to_likert(x) ## [1] Agree Disagree Disagree Strongly agree ## [5] Agree Disagree Strongly disagree Neutral ## [9] Strongly agree Neutral ## 5 Levels: Strongly disagree &lt; Disagree &lt; Neutral &lt; ... &lt; Strongly agree The function can be applied to all the columns of a data frame read into R by using the mutate_all function in dplyr. Notice that the inverse operation is very easy. Just coerce the vector to numeric. as.numeric(x) ## [1] 4 2 2 5 4 2 1 3 5 3 21.3.1 Forming a data frame Let’s now simulate the responses of one thousand people to four questions. Notice that the first question takes the default vector of probabilities. The second question provokes more negative responses. The third question is more neutral and the final question has a “marmite” type response. q1&lt;-rand_likert() q2&lt;-rand_likert(p=c(4,3,1,1,1)) q3&lt;-rand_likert(p=c(1,2,5,2,1)) q4&lt;-rand_likert(p=c(5,1,1,1,4)) q5&lt;- beta_likert(n=1000) ## Using the beta_likert explained later in the text d&lt;-data.frame(id=1:1000,q1,q2,q3,q4,q5) Looking at the raw data.. dt(d) This produces a wide data frame. This is likely to represent the way data were entered in a spreadsheet or other tabular data capturing software. It is not wrong in any way to represent data in this form. Some analyses do work on data in a wide format. One example would be to use base R to tabulate each response. table(d$q1) ## ## Strongly disagree Disagree Neutral Agree ## 52 99 133 221 ## Strongly agree ## 495 Or to use qqplot to produce a barplot for a single question ggplot(d,aes(x=q2)) + geom_bar() +coord_flip() However this can get very tedious and repetitive as we need to repeat the code for each question. The four columns holding the responses to questions can be thought of as forming a matrix. A matrix is a rectangular data structure with two dimensions. The key characteristic of a data matrix is that all the values in the matrix are on the same scale. So a likert based matrix can only contain values from 1 to 5 or the associated text. A true data frame would consist of only three columns. The id of the individual providing the response, a column holding the code for the question asked and a third column containing the response. Providing the data that has either been simulated or read into R from a spreadsheet has a unique id as the first column then the data can be pivoted into a long format with the following code. df&lt;-pivot_longer(d, cols=-id,names_to=&quot;question&quot;,values_to = &quot;response&quot;) dt(df) Now we can tabulate all the data with just one string of commands. df %&gt;% group_by(question,response) %&gt;% summarise(n=n()) %&gt;% mutate(percent = round(100*n / sum(n),0)) -&gt; tb dt(tb) 21.3.2 Adding question text In order to collect real data the survey will be based on questions which may frequently consist of longish sentences. They may also be grouped by theme. The easiest way to handle questionnaire data of this type is to use a relational data structure in which data corresponding to details on the question content are held in an additional table that can be linked to the responses and data on the individuals who provided the responses held in another table. The tables can then be linked together. For example, the text of the questions used in the NSS survey of student satisfaction is included in the aqm package. When capturing your own survey data you should provide a similar table of data on each question. Notice that this avoids having to use long strings of text as headers in the table of Likert data that you import into R. Just use simple question indices as column headers if the data are in wide format in the spreadsheet, or a column with question id codes if the data are in long format. data(nss_questions) dt(nss_questions) 21.3.3 Joining the two tables Tables can be joined together very easily in R by using either the merge function in base R or the dplyr family of join functions. The latter approach is now the preferred one, although the results are exactly the same. In order to merge two tables in a simple manner you must ensure that one AND ONLY ONE of the columns has the same name in each table and that the column contains data of the same form in both tables. ALWAYS CHECK VERY CAREFULLY BEFORE MERGING If you inadvertently try to join tables which have several columns of the same name you would be telling R to form an object with the number of dimensions of the number of rows to the power of the number of shared column names. This could exceed the memory capacity and crash your R session, and is never what you actually want. In this case the questions column holds characters representing the question number which are identical in each table. So the join works on both the raw data and on the derived table. library(dplyr) df&lt;-inner_join(df,nss_questions) tb&lt;-inner_join(tb,nss_questions) 21.3.4 Plotting the results There are many different possible figures that could be used to display Likert data. The worst would be to use pie charts. Pie Charts are generally considered to be very poor figures. They can be produced in R but there is no default option in ggplots. A much better figure is a labelled barchart with the percentages placed on the top of the bars. This looks better when flipped horizontally. Notice that wherever possible you should aim to help the reader of a descriptive data analysis such as this to extract the raw data. ggplot(data=tb,aes(x=response,y=percent,label=percent)) + facet_wrap(&#39;short_text&#39;) + geom_bar(stat=&quot;identity&quot;) + geom_label() + xlab(&quot;Response on Likert scale&quot;) +ylab(&quot;Percent of responses (n=500)&quot;) + coord_flip() #theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Remove coord flip and comment if you want vertical bars with labels at the bottom. Another very good, more compact option is to use a stacked bar chart, again flipped horizontally to produce a shaded ribbon for each response. ggplot(tb) + aes(x = short_text, y = percent, fill = response, label=percent) + geom_bar(stat=&quot;identity&quot;, width = 0.5)+ coord_flip() + xlab(&quot;Short description of question text&quot;) +ylab(&quot;Percent of responses (n=500)&quot;) + geom_text(aes(label = percent), position = position_stack(vjust = 0.5)) + scale_fill_manual( values = colorRampPalette( RColorBrewer::brewer.pal(n = 5, name = &quot;RdBu&quot;))(6), guide = guide_legend(reverse = TRUE)) 21.3.5 Grouping data One way of getting around the well known poor statistical properties of raw Likert scale data is to pool questions. If three or more questions are asked on different aspects of the same topic the mean of the numerical values of the Likert scale can be taken as a measure of agreement. These responses will tend to follow an approximately Gaussian (Normal) distribution, as the Gaussian distribution arises as a result of addition of various factors acting independently. In this case the results are not actually independent but the general principle will still apply. However using this approach may involve making some arbitrary decisions regarding the grouping of questions. As an illustration of how to apply this idea we can group the responses to individual questions on the Likert scale according to the groups in the NSS question table. str(df) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5000 obs. of 6 variables: ## $ id : int 1 1 1 1 1 2 2 2 2 2 ... ## $ question : chr &quot;q1&quot; &quot;q2&quot; &quot;q3&quot; &quot;q4&quot; ... ## $ response : Ord.factor w/ 5 levels &quot;Strongly disagree&quot;&lt;..: 1 1 3 5 2 3 2 3 1 3 ... ## $ text : chr &quot;Staff are good at explaining things&quot; &quot;Staff have made the subject interesting&quot; &quot;The course is intellectually stimulating&quot; &quot;My course has challenged me to achieve my best work&quot; ... ## $ short_text: chr &quot;Well explained&quot; &quot;Interesting&quot; &quot;Stimulating&quot; &quot;Challenging&quot; ... ## $ group : chr &quot;teaching&quot; &quot;teaching&quot; &quot;teaching&quot; &quot;teaching&quot; ... df %&gt;% group_by(id,group) %&gt;% summarise (score=mean(as.numeric(response))) -&gt; mean_score head(mean_score) ## # A tibble: 6 x 3 ## # Groups: id [3] ## id group score ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 learning 2 ## 2 1 teaching 2.5 ## 3 2 learning 3 ## 4 2 teaching 2.25 ## 5 3 learning 3 ## 6 3 teaching 3.25 The mean scores for a group of questions will tend to form a normal distribution. So conventional statistical techniques can be applied to such data. This is a very good way of getting around some of the issues with the ordinal likert scale. mean_score %&gt;% filter(group==&quot;teaching&quot;) %&gt;% ggplot(aes(x=score)) + geom_histogram() 21.4 Simulating responses that differ between subject specific variables The questions asked about Likert scale data often consist of looking at differences in response which are related to other variables collected on subjects. The data manipulation shown above is based on a simulated questionnaire consisting only of Likert responses. The data collected on the individual survey respondents will often be quite mixed. It may consist of factors, either ordered or unordered, numerical values, character vectors or yes/no binary responses. A real life survey may also contain free text. This can be analysed in R using a range of methods shown in a later chapter. In order to organise your data neatly you need to aggregate all the Likert scale responses into one data table and all non Likert data into one (or possibly more than one) additional table which can be linked to the Likert responses through a common subect id. This will usually be anonymous. Only a person in possession of a table containing the actual name of the respondent can de-anonimise the data, and this can be stored separately from the rest of the data. Let’s simulate two simple characteristics of subjects in a survey. Notice that the table representing characteristics of the subjects can be linked to their responses through the anonymous id column. This is common to both tables. id&lt;-1:1000 gender&lt;-rep(c(&quot;Male&quot;,&quot;Female&quot;),each=500) age&lt;-sample(18:50,1000,replace=TRUE) subjects&lt;-data.frame(id,gender,age) 21.4.1 Using the beta distribution to simulate Likert data The beta distribution is a continuous distribution which is bounded to lie between 0 and 1. Two parameters are used to define the form of the distribution, the shape and scale. This makes the distribution very flexible in form. It can easily represent “marmite” type patterns by setting certain combinations of shape and scale. However the parametrisation of a beta distribution is rather non intuitive. We can make the parametrisation more intuitive by reparametrising the beta in terms of mean and “standard deviation” x&lt;-seq(0.01,0.99, length=100) plot(x,aqm::dmbeta(x,mean=0.5,sd=0.3), type=&quot;l&quot;, main=&quot;Beta distribution with mean of 0.5 and sd of 0.3&quot;) If the sd is set to a high value the beta takes the form of a marmite type response. plot(x,aqm::dmbeta(x,mean=0.5,sd=1), type=&quot;l&quot;, main=&quot;Beta distribution with mean of 0.5 and sd of 3&quot;) The aqm package has a variation of rand_likert function which uses the beta distribution (bounded between 0 and 1) to represent the probability vector. This produces simulated responses with a known overall mean and “standard deviation”. Do note that the standard deviation is not that of a normal deviation. The beta distribution really has a shape and scale parameter and this is simply a convenient approximation to make using the function easier. table(beta_likert(mean=0.1, sd=0.5)) ## Produces negative responses ## ## Strongly disagree Disagree Neutral Agree ## 68 22 9 1 ## Strongly agree ## 0 Simulated ‘marmite’ response. table(beta_likert(mean=0.5,sd=1)) ## Marmite responses ## ## Strongly disagree Disagree Neutral Agree ## 18 17 21 22 ## Strongly agree ## 22 Simulating responses which follow a pattern linked to the characteristics of the subjects is rather more complex than simulating data that follows a normal distribution. We need to set up some underlying preference values around which there is some random noise. This is a latent beta distribution from which discrete answers are produced. gender_effect&lt;-rep(c(0.4,0.6),each=500) ## The female respondents tend to be more positive age_effect&lt;-age/100 ### Responses have a monotonic increase in positivity over the age range ## Simulate four questions q1&lt;-beta_likert_vec(age_effect,sd=0.6) q2&lt;-beta_likert_vec(age_effect,sd=0.6) q3&lt;-beta_likert_vec(gender_effect,sd=0.5) q4&lt;-beta_likert_vec(gender_effect,sd=0.5) responses&lt;-data.frame(id=1:1000,q1,q2,q3,q4) Ok. We now have simulated data in which the responses to questions 1 and 2 are strongly associated with a continuous age effect and questions 3 and 4 are associated with gender. In a real data set there would probably be additive and interactive effects. These can also be simulated, but we’ll keep it simple for the moment in order to clearly illustrate the analysis. 21.4.2 Joining the tables The same approach can be taken to the data as previously. If the responses are first pivoted into a long data frame the subjects data can be joined as columns using the id as a common index. df&lt;-pivot_longer(responses, cols=-id,names_to=&quot;question&quot;,values_to = &quot;response&quot;) df&lt;-inner_join(subjects,df) df %&gt;% group_by(gender,question,response) %&gt;% summarise(n=n()) %&gt;% mutate(percent = round(100*n / sum(n),0)) -&gt; tb 21.4.3 ggplot(tb) + aes(x = question, y = percent, fill = response, label=percent) + geom_bar(stat=&quot;identity&quot;, width = 0.5)+ coord_flip() + geom_text(aes(label = percent), position = position_stack(vjust = 0.5)) + scale_fill_manual( values = colorRampPalette( RColorBrewer::brewer.pal(n = 5, name = &quot;RdBu&quot;))(6), guide = guide_legend(reverse = TRUE)) + facet_wrap(~gender) 21.5 Statistical tests Although many people do use simple statistical tests to look at associations between factors such as gender and responses on the Likert scale there are some issues when raw Likert data is used. As the scale is ordinal and bounded tests based on assumptions of normality of residuals are not valid. Lack of formal validity should not rule out using a statistical procedure entirely, as in real life all the assumptions of a statistical procedure are rarely met in full. Running a simple t-test is tolerable and should not be criticised too harshly. However it is not really the correct way to analyse the data. df %&gt;% filter(question ==&quot;q4&quot;) -&gt; q4 t.test(as.numeric(q4$response)~ q4$gender) ## ## Welch Two Sample t-test ## ## data: as.numeric(q4$response) by q4$gender ## t = 15.234, df = 997.61, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.9304271 1.2055729 ## sample estimates: ## mean in group Female mean in group Male ## 3.510 2.442 The Kruskall-Wallis test is also sometimes advised for Likert scale data. As it is based on ranks it is technically valid. There is no strong mathematical objection to using it. The null being tested is that the location parameters of the distribution of the scores are the same in each group. This is sometimes assumed to be a test that the median’s are not significantly different, but this is not quite the correct interpretation. This also shows no significant difference. However it is extremely hard to interpret and doesn’t lead to any clear measure of effect size. So not the correct procedure either. kruskal.test(as.numeric(q4$response)~ q4$gender) ## ## Kruskal-Wallis rank sum test ## ## data: as.numeric(q4$response) by q4$gender ## Kruskal-Wallis chi-squared = 187.47, df = 1, p-value &lt; 2.2e-16 21.5.1 Analysing as a binary responses One common way to deal with Likert scale data that at least partially adresses the potential “marmite” issue is to simplify the data into binary classes and look at the number of responses falling into each class. This is the approach which is taken when analysing NSS scores in order to form University league tables. The measure typically used is the proportion of students who are satisfied i.e. giving a score above 3 on the Likert scale. We can use ones and zeros to represent satisfaction. df %&gt;% mutate(satisfied=1*(as.numeric(response)&gt;3)) -&gt;df Once we have converted the scale into zeros and ones then the total satisfaction for a question is simply the sum of these ones and zeros. The binom.test function in R produces 95% confidence intervals on the proportion of successes as one of its outputs. So we can use that to produce a table holding the proportion satisfied, converted to a percentage together with the appropriate upper and lower 95% confidence intervals. binom_ci&lt;-function(p,n){ b&lt;-binom.test(p,n) round(b$conf.int*100,1) } df %&gt;% group_by(gender,question) %&gt;% summarise(n=n(),satisfied=sum(satisfied), lwr=binom_ci(satisfied,n)[1],percent=round(100*satisfied/n,1),upr=binom_ci(satisfied,n)[2]) -&gt; bin_table dt(bin_table) The results can now be turned into a figure which is conditioned on gender using a facet wrap. g0&lt;-ggplot(bin_table,aes(x=gender)) + geom_point(aes(y=percent),colour=&quot;red&quot;) + facet_wrap(~question, scales=&quot;free&quot;) g1&lt;-g0+geom_errorbar(aes(ymin=lwr,ymax=upr)) g1 The figure allows us to quickly screen the questions for significant results. When the confidence intervals do not overlap then a statistical test using logistic regression (generalised linear model of the binomial family) will produce a p-value below 0.05. df %&gt;% filter(question==&#39;q3&#39;) -&gt; q3 mod&lt;-glm(data=q3,satisfied~gender,family=&#39;binomial&#39;) summary(mod) ## ## Call: ## glm(formula = satisfied ~ gender, family = &quot;binomial&quot;, data = q3) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.1876 -0.6605 -0.6605 1.1672 1.8054 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.02400 0.08945 0.268 0.788 ## genderMale -1.43549 0.14385 -9.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1296.1 on 999 degrees of freedom ## Residual deviance: 1187.9 on 998 degrees of freedom ## AIC: 1191.9 ## ## Number of Fisher Scoring iterations: 4 21.5.2 Visualising response to a continuous variable Logistic regression can also be used to analyse the response to a continuous variable. In order to visualise the response we can add a smoother to a ggplot using glm as the model fitting method. This produces lines with confidence intervals. g1&lt;-ggplot(df,aes(x=age,y=satisfied)) g1&lt;-g1+stat_smooth(method=&quot;glm&quot;,method.args=list(family=&quot;binomial&quot;),formula=y~x) +facet_wrap(~question) g1 If we suspected that the response may in fact be more complex we could experiment by fitting gams instead of glms. library(mgcv) g1&lt;-ggplot(df,aes(x=age,y=satisfied)) g1&lt;-g1+stat_smooth(method=&quot;gam&quot;,method.args=list(family=&quot;binomial&quot;),formula=y~s(x,k=3)) +facet_wrap(~question) g1 The results in this case are very similar, as we would expect. Statistical tests can be run for each question by fitting a logistic regression. df %&gt;% filter(question==&#39;q1&#39;) -&gt; q1 mod&lt;-glm(data=q1,satisfied~age,family=&#39;binomial&#39;) summary(mod) ## ## Call: ## glm(formula = satisfied ~ age, family = &quot;binomial&quot;, data = q1) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.8959 -0.7357 -0.5978 -0.4715 2.1417 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.020406 0.338962 -8.911 &lt; 2e-16 *** ## age 0.046294 0.008909 5.196 2.04e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1011.80 on 999 degrees of freedom ## Residual deviance: 983.28 on 998 degrees of freedom ## AIC: 987.28 ## ## Number of Fisher Scoring iterations: 4 So the analysis has correctly discovered the bivariate relationships that were used to simulate the data. The analysis can then be extended to analyse the response with respect to both explanatory variables together. df %&gt;% filter(question==&#39;q1&#39;) -&gt; q1 mod&lt;-glm(data=q1,satisfied~age + gender,family=&#39;binomial&#39;) summary(mod) ## ## Call: ## glm(formula = satisfied ~ age + gender, family = &quot;binomial&quot;, ## data = q1) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9182 -0.7295 -0.5929 -0.4678 2.1678 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.080106 0.348058 -8.849 &lt; 2e-16 *** ## age 0.046148 0.008918 5.174 2.29e-07 *** ## genderMale 0.126883 0.159437 0.796 0.426 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1011.80 on 999 degrees of freedom ## Residual deviance: 982.65 on 997 degrees of freedom ## AIC: 988.65 ## ## Number of Fisher Scoring iterations: 4 21.5.3 Screening all questions f&lt;-function(y,x1,x2)summary(glm(y~x1+x2,family=&#39;binomial&#39;))$ coefficients[,4] df %&gt;% group_by(question) %&gt;% summarise(p_val_age=round(f(satisfied,age,gender)[2],3), p_val_gender=round(f(satisfied,age,gender)[3],3)) %&gt;% dt() 21.5.4 Latent factor analysis Factor analysis is a potentially very useful method for analysing questionnaire data from a psychometric perspective. The purpose of factor analysis is to estimate a model that explains the observed covariance structure in the data collected in terms of a set of unobserved factors and weightings. The aim of factor analysis is parsimony of description. In other words we want to look at the relationships between many observed variables in terms of a more limited set of components or latent factors. There is a clear explanation of the mathematics involved here https://www.youtube.com/watch?v=WV_jcaDBZ2I&amp;list=PLwJRxp3blEvaOTZfSKXysxRmi6gXJf5gP&amp;index=2&amp;t=0s The best package for running factor analysis on http://www.personality-project.org/r/book/ The site contains a lot of material on psychometrics including an interesting personality test. https://sapa-project.org/ Let’s make up some data to illustrate the idea behind latent factor analysis in a straightforward, hopefully easily understood way. Imagine we have set up a questionnaire that aims to look at people’s personalities in terms of just two main traits. Perhaps we are interested in musicians and other stage performers. Musicians are creative people, but despite being in public view they are not all necessarily extroverted. The two traits might be completely unrelated. The logic behind latent factor analysis is that we have an idea about the possible existence of some underlying factors, but until we have obtained data from the questionnaire we don’t really know the extent to which each question is really associated (correlated) with these underlying factors. When we are making up simulated data we do already know the answer of course. Let’s assume we are going to survey 300 people with a range of hidden scores on two axes. The axes in our simulated data are completely orthogonal, in other words we are assuming that there is no correlation between extroversion and creativity. In practice this may not be true and a detailed analysis can actually tease apart some subtle patterns of correlations between latent factors. However the more complex the relationships the more data that will be required set. Tens of thousands of observations may be needed to look at complex hierarchical structures involving five or more latent factors. For two latent factors which are truly orthogonal our survey may be sufficiently large, providing that the actual questions have been skilfully designed and well targeted towards a statistical analysis of the hypotheses we have in mind. id&lt;-1:300 extroversion&lt;-rmbeta(300,0.5,0.5) creativity&lt;-rmbeta(300,0.5,0.5) plot(extroversion,creativity) Notice that for illustrative purposes there is no effect of additional covariates such as gender, age etc. In a real study we certainly would be interested in the covariates. In fact one on of the key strengths of factor analysis is that like PCA (which it is based on) it is form of data reduction. The scores on the reduced axes can then be related to other factors. So we might in fact be interested in finding out if extroversion is related to gender. If we were simulating data with differences we would set them up at this stage. 21.5.5 Setting up the data frame We can use the beta_likert_vec function in aqm to set up Likert responses which are correlated with the hidden factors. q1&lt;-beta_likert_vec(extroversion) q2&lt;-beta_likert_vec(extroversion) q3&lt;-beta_likert_vec(extroversion) q4&lt;-beta_likert_vec(extroversion) q5&lt;-beta_likert_vec(extroversion) q6&lt;-beta_likert_vec(creativity) q7&lt;-beta_likert_vec(creativity) q8&lt;-beta_likert_vec(creativity) q9&lt;-beta_likert_vec((creativity+extroversion)/2) q10&lt;-beta_likert_vec((creativity+extroversion)/2) d&lt;-data.frame(q1,q2,q3,q4,q5,q6,q7,q8,q9,q10) 21.5.6 Data manipulation The function simulates data as an ordered factor. To change the responses into numbers on a scale of 1 to 5 use coercion. Note that the dplyr mutate_all is very handy for this, but it is only appropriate if all the columns hold the same type of data (i.e. it might go wrong if an id column was included) d_num&lt;-mutate_all(d,as.numeric) Another approach is to convert the Likert scale responses to binary scores. f&lt;-function(x)1*(x&gt;3) d_bin&lt;-mutate_all(d_num,f) 21.5.7 Correlation matrix In order to look at relationships between the numerical values we could use rank correlation. Using Pearson’s would not be correct. However this is really only an exploratory analysis and in many situations it would produce similar results. rank_cor &lt;- cor(d_num,method=&quot;spearman&quot;) cor.plot(rank_cor, numbers=T, upper=FALSE, main = &quot;Rank Correlation&quot;, show.legend = FALSE) When running factor analysis on Likert scale data we will usually use either polychoric or tetrachoric correlation. Polychoric correlation estimates the correlation between two theorised normally distributed continuous latent variables, from two observed ordinal variables. Tetrachoric correlation uses just about the same underlying mathematics but is the name given to polychoric correlation both observed variables are dichotomous. If you already know some stats you can think of polychoric correlation as being rather like runing a chisq test on a contingency table made up by crossing observations. For example crossing question1 and question 2 produces the following table table(d$q1,d$q2) ## ## Strongly disagree Disagree Neutral Agree Strongly agree ## Strongly disagree 26 8 12 10 1 ## Disagree 12 16 20 6 2 ## Neutral 10 11 8 15 12 ## Agree 2 8 21 12 15 ## Strongly agree 3 6 10 25 29 A mathematical issue can arise if some of the cells are empty. R will warn you of this and make a correction when the procedure is applied. 21.5.8 Polychoric scree plots There are many different procedures for selecting the number of factors to extract from the data. The most commonly used approach is the scree plot. This is named after the screes on mountain sides. The idea is that the random variability forms a pile of scree at the foot of the slope. A steep slope implies a small number of factors. fa.parallel(d_num, fm=&quot;minres&quot;, fa=&quot;fa&quot;, cor=&#39;poly&#39;, main = &quot;Polychoric scree plot based on ordinal responses&quot;) ## Warning in matpLower(x, nvar, gminx, gmaxx, gminy, gmaxy): 4 cells were adjusted ## for 0 values using the correction for continuity. Examine your data carefully. ## Parallel analysis suggests that the number of factors = 2 and the number of components = NA fa.parallel(d_bin, fm=&quot;minres&quot;, fa=&quot;fa&quot;, cor=&#39;tet&#39;, main = &quot;Tetrachoric scree plot based on binary responses&quot;) ## Parallel analysis suggests that the number of factors = 3 and the number of components = NA mod1&lt;-fa(d_num, nfactor=2, cor=&quot;poly&quot;, fm=&quot;mle&quot;, rotate = &quot;oblimin&quot;) ## Warning in matpLower(x, nvar, gminx, gmaxx, gminy, gmaxy): 4 cells were adjusted ## for 0 values using the correction for continuity. Examine your data carefully. mod2&lt;-fa(d_bin, nfactor=2, cor=&quot;tet&quot;, fm=&quot;mle&quot;, rotate = &quot;oblimin&quot;) mod1$loadings ## ## Loadings: ## ML1 ML2 ## q1 0.738 ## q2 0.731 ## q3 0.796 ## q4 0.664 ## q5 0.654 ## q6 0.734 ## q7 0.747 ## q8 0.800 ## q9 0.460 0.426 ## q10 0.431 0.474 ## ## ML1 ML2 ## SS loadings 2.989 2.154 ## Proportion Var 0.299 0.215 ## Cumulative Var 0.299 0.514 fa.diagram(mod1,simple=FALSE) fa.diagram(mod2,simple=FALSE) hist(mod1$scores[,1]) 21.5.9 Hierarchical factors and item response theory Analysis of real life data can be a much more complex task. Some data sets may appear to have only one latent factor when in reality this is a compound of elements that can be broken down further. An example of this sort of hierachical data is included in the psych package. The ability data set is based on tests of ability on four lower order factors (verbal reasoning, letter series, matrix reasoning, and spatial rotations). These are all highly correlated with overall analytical ability. So a scree plot seems to suggest that there is only one factor. data(ability) fa.parallel(ability, fm=&quot;minres&quot;, fa=&quot;fa&quot;, cor=&#39;tet&#39;, main = &quot;Tetrachoric scree plot based on binary responses&quot;) ## Parallel analysis suggests that the number of factors = 6 and the number of components = NA mod_ability&lt;-fa(d_bin, nfactor=4, cor=&quot;tet&quot;, fm=&quot;mle&quot;, rotate = &quot;oblimin&quot;) fa.diagram(mod_ability,simple=FALSE) "],
["simple-text-processing-with-sentiment-analysis.html", "Chapter 22 Simple text processing with sentiment analysis 22.1 Introduction 22.2 Reading in the data 22.3 Making a data frame consisting of just words 22.4 Count the frequenciy of each word 22.5 Word cloud 22.6 Find the sentiments associated with the words 22.7 Plotting the frequencies of the sentiments 22.8 Words associated with each sentiment 22.9 Used in a questionaire context 22.10 Example: One line per tweet 22.11 Extracting the words 22.12 Sentiment scores 22.13 Is there a relationship between the scores and the number of times the tweet is favourited? 22.14 Using Udpipe 22.15 Nouns 22.16 Verbs 22.17 Adjectives", " Chapter 22 Simple text processing with sentiment analysis 22.1 Introduction This is a very simple example of the sort of work flow involved with text processing. As an example I have used the text from this page obtained by googling nature and bereavement. http://journeyofhearts.org/healing/nature.html The page has been cut and pasted into a text file. library(wordcloud) library(dplyr) library(tidyr) library(scales) library(stringr) library (readr) library(tidytext) options(scipen=999) knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE) 22.2 Reading in the data The data from a simple text file can be read in using read_lines. Blank lines are then filtered out and the factor coerced into a character vector. d&lt;-data.frame(text=read_lines(&quot;nature_healing.txt&quot;)) d %&gt;% filter(text != &quot;&quot;) %&gt;% mutate(text=as.character(text))-&gt;d DT::datatable(d) 22.3 Making a data frame consisting of just words The tidytext package has functions to extract the words (tokens) and to remove stop words. library(SnowballC) data(stop_words) d %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) -&gt; words 22.4 Count the frequenciy of each word ### Count the frequencies words %&gt;% group_by(word) %&gt;% count() %&gt;% arrange(-n) -&gt;word_count ## Show as table DT::datatable(word_count) 22.5 Word cloud Making a word cloud from the table of frequencies is easy using the word-cloud package. wordcloud(word_count$word, word_count$n, random.order=FALSE, max.words = 100, colors=brewer.pal(8, &quot;Dark2&quot;), use.r.layout=TRUE) 22.6 Find the sentiments associated with the words This is the part that may not quite work as well as you might hope. There are various lexicons in the R package tidytext. These lexicons are simple tables with words and associated emotions, or scores. Some words can have several emotions associated with them. # Get Lexicon nrc &lt;- sentiments %&gt;% filter(lexicon == &quot;nrc&quot;) %&gt;% dplyr::select(word, sentiment) ## Join to words words %&gt;% inner_join(nrc, by = &quot;word&quot;) -&gt; word_sentiment DT::datatable(word_sentiment) 22.7 Plotting the frequencies of the sentiments library(ggplot2) word_sentiment %&gt;% group_by(sentiment) %&gt;% summarise(n=n()) %&gt;% arrange(n) %&gt;% mutate(sentiment = reorder(sentiment, n)) -&gt;ws ggplot(data=ws,aes(x=sentiment,y=n,label=n)) + geom_bar(stat=&quot;identity&quot;) + geom_label() + coord_flip() 22.8 Words associated with each sentiment word_sentiment %&gt;% group_by(sentiment,word) %&gt;% count() %&gt;% filter(n&gt;3) %&gt;% arrange(-n) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x=word,y=n)) + geom_bar(stat=&quot;identity&quot;,fill=&quot;red&quot;) + facet_wrap(~ sentiment, scales = &quot;free&quot;, ncol = 5) + coord_flip() 22.9 Used in a questionaire context Shorter pieces of text can be scored for usage of positive and negative terms in order to be used as a response variable in an statistical analysis. However be aware of the potential weaknesses of the lexicon used, and the potential for mis-scoring, especially when the terms are negated or taken out of context. 22.10 Example: One line per tweet The word tokens are extracted from a data frame along with the covariates in the rows that contain text. So if shorter pieces of text are scored numerically the relationship between the scores and other covariates can be looked at. For example, here is the text of some of Donald Trump’s recent tweets along with the number of times his followers have favourited them. Each tweet can be scored as being positive or negative. d&lt;-read_csv(&quot;recent_tweets.csv&quot;) d$text&lt;-as.character(d$text) d %&gt;% select(id,text,favoriteCount,created, hour) -&gt;d DT::datatable(d) 22.11 Extracting the words data(stop_words) d$text2&lt;-d$text d %&gt;% select(hour, text2, id, favoriteCount,text) %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) -&gt; words 22.12 Sentiment scores This time using the affin lexicon. This produces scores between -4 and +4 for positive and negative afin&lt;-get_sentiments(&quot;afinn&quot;) words %&gt;% inner_join(afin, by = &quot;word&quot;) -&gt; word_score 22.13 Is there a relationship between the scores and the number of times the tweet is favourited? word_score %&gt;% group_by(id,text2) %&gt;% summarise(n=n(),score=mean(score),favourited=mean(favoriteCount)) %&gt;% ggplot(aes(x=score,y=favourited, label =text2)) + geom_point() + geom_smooth() -&gt;g1 library(plotly) ggplotly(g1) The answer seems to be no. Donald Trump’s twitter followers don’t care much whether he expresses positive or negative sentiments. By plotting out the results using plotly at least some of the text can be seen by hovering on the tweet. Trump scored the most favourite counts when he tweeted “Merry Christmas”, which scored quite highly on the positivity index! 22.14 Using Udpipe The Udpipe package has some powerful features for breaking down text into parts of speach. There are some ideas in this tutorial. https://towardsdatascience.com/easy-text-analysis-on-abc-news-headlines-b434e6e3b5b8 library(udpipe) #model &lt;- udpipe_download_model(language = &quot;english&quot;) udmodel_english &lt;- udpipe_load_model(file = &#39;english-ewt-ud-2.3-181115.udpipe&#39;) dd&lt;-udpipe_annotate(udmodel_english, words$word) dd&lt;-data.frame(dd) 22.15 Nouns dd %&gt;% group_by(upos,token) %&gt;% filter(upos==&quot;NOUN&quot;) %&gt;% summarise(n=n()) %&gt;% arrange(-n) -&gt; nouns wordcloud(nouns$token, nouns$n, random.order=FALSE, max.words = 100, colors=brewer.pal(8, &quot;Dark2&quot;), use.r.layout=TRUE) 22.16 Verbs dd %&gt;% group_by(upos,token) %&gt;% filter(upos==&quot;VERB&quot;) %&gt;% summarise(n=n()) %&gt;% arrange(-n) %&gt;% filter(token!=&quot;republicans&quot;)-&gt; verbs wordcloud(verbs$token, verbs$n, random.order=FALSE, max.words = 100, colors=brewer.pal(8, &quot;Dark2&quot;), use.r.layout=TRUE) 22.17 Adjectives dd %&gt;% group_by(upos,token) %&gt;% filter(upos==&quot;ADJ&quot;) %&gt;% summarise(n=n()) %&gt;% arrange(-n) %&gt;% filter(token!=&quot;southern&quot;)-&gt; verbs wordcloud(verbs$token, verbs$n, random.order=FALSE, max.words = 100, colors=brewer.pal(8, &quot;Dark2&quot;), use.r.layout=TRUE) "],
["time-series-analysis-climate-data.html", "Chapter 23 Time series analysis: Climate data 23.1 Load the data 23.2 Plot the time series for the hadcrut4 data set 23.3 Example to explain serial autocorrelation 23.4 Accounting for serial autocorrelation in the real climate data 23.5 Analysing the significance of the slope 23.6 Seasonal data 23.7 Seasonal decomposition by loess smoothing", " Chapter 23 Time series analysis: Climate data Modelling time series presents a range of challenges. One of the most basic is deciding on the questions that we want to answer. A large proportion of the literature on time series modelling is devoted to the topic of prediction. For example models of financial time series are used in the stock market for short term prediction (extrapolation) in order to estimate possible prices of shares within hours or days of the current moment. The predictive precision of such models clearly decays with time. Financial models are less concerned with understanding the mechanistic component of the model and more concerned with the stochastic element, as this determines risk and uncertainty. It is common for ecologists, climatologists and paleoecologists to ask whether there has been any consistent upward or downwards trend in a time series and aim to link this with underlying mechanisms. For example climate change is commonly reported in terms of change in temperature per decade, which implies a linear trend. However although a this simple model is useful as a generalisation, it would not be expected to fit real data very closely. Real time series are more complex. An overall trend towards higher temperatures does not imply that there may not be periods of temporary cooling. Furthermore any analysis has to face the additional challenges of periodicity and serial auto correlation. Some periodic effects, such as seasonality, are well defined and can be relatively cleanly extracted from a time series in order to be analysed. Other periodic effects, such as the influence of El Niño on climatic variables are more subtle and uncertain. In order to look at the analysis of time series we will begin by using an excellent example of good practice provide by Gavin Simpson. Gavin Simpson is a key contributor to the vegan package in R and a sophisticated and practical minded data analyst. The original material is available in the form of weblogs from here http://www.fromthebottomoftheheap.net/2011/06/11/global-warming-since-1995-now-significant/ http://www.fromthebottomoftheheap.net/2011/06/12/additive-modelling-and-the-hadcrut3v-global-mean-temperature-series/ http://www.fromthebottomoftheheap.net/2014/05/09/modelling-seasonal-data-with-gam/ Gavin Simpson provides code to download and reformat data found online. As this is rather tricky to run and may be vulnerable to changes in URLs (as in fact occurred) I have run all the code and added the data into the qecol package. 23.1 Load the data To load the data install (or re-install) the qecol package from github using devtools. # library(devtools) #install_github(&quot;dgolicher/qecol&quot;) Packages used in the analysis. rm(list=ls()) library(qecol) library(mgcv) library(nlme) library(ggplot2) library(reshape) 23.2 Plot the time series for the hadcrut4 data set The hadcrut4 data consist of annual global temperature anomalies compared to the mean global temperature in the years from 1961 to 1990. The data are supplied by the University of East Anglia global climate modelling group. data(hadcrut4) ylab &lt;- expression(Temperature~Anomaly~(1961-1990)~degree*C) g0&lt;-ggplot(hadcrut4,aes(x=Year,y=Annual)) g1&lt;-g0+geom_point()+geom_line()+ylab(ylab) g1 There seems to be a very clear upward trend. Let’s try fitting a simple linear model. g1+geom_smooth(method=&quot;lm&quot;) mod1&lt;-lm(Annual~Year,data=hadcrut4) summary(mod1) ## ## Call: ## lm(formula = Annual ~ Year, data = hadcrut4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.34150 -0.11025 -0.00876 0.11406 0.40404 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.3342281 0.5028501 -18.56 &lt;2e-16 *** ## Year 0.0047738 0.0002602 18.35 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1592 on 163 degrees of freedom ## Multiple R-squared: 0.6737, Adjusted R-squared: 0.6717 ## F-statistic: 336.6 on 1 and 163 DF, p-value: &lt; 2.2e-16 The fit of the linear model is clearly very poor. It does not capture the changes in the trend over the long time period that the data represent. A smoother relationship is needed. We can try a gam. g1+geom_smooth(method=&quot;gam&quot;,formula=y~s(x)) mod &lt;- gam(Annual ~ s(Year, k = 20), data = hadcrut4) This shows up periods in which the temperature has been stable, and also some periods in which the temperature has declined in addition to the more general upward trend. However there may be a problem with serial temporal auto-correlation. Serial auto correlation occurs when anomalies can be predicted from a previous year’s values. In this case the model may over fit and be too “wavy” as one of the key assumptions of a gam model is that the residuals are independently drawn from an identical distribution (iids). When this is not the case the model fitting algorithm will try too hard to find a curve that passes close to every data point. If the model formula allows for some serial auto correlation the underlying curve may be simpler in form. 23.3 Example to explain serial autocorrelation Gavin Simpson provides an example in which he simulates data with a known structure. The underlying relationship is a parabola. However data on either side of the line fall in groups with correlated residuals. set.seed(321) n &lt;- 100 time &lt;- 1:n xt &lt;- time/n Y &lt;- (1280 * xt^4) * (1- xt)^4 y &lt;- as.numeric(Y + arima.sim(list(ar = 0.3713), n = n)) plot(y~time) lines(Y~time,col=&quot;red&quot;) If we adjust a gam model with a few knots we can look at the auto correlation in the residuals. mod&lt;-gam(y~s(time,k=5)) plot(mod,residuals=TRUE,pch=21,bg=2,cex=0.5) plot(residuals(mod),type=&quot;b&quot;) abline(h=0,lwd=2,col=2) Notice how the residuals cluster on each side of the line. If the previous residual is positive it is more likely that the next one will also be positive. This indicates lack of independence. They are “serially auto-correlated”. To understand more clearly how auto correlation works we will look at the lagged time series of the residuals. The first order lag is the time immediately preceding the present. If we plot the residuals against the lagged residuals and find that there is a linear relationship this indicates a first order auto correlation. pres_residuals&lt;-residuals(mod) lag_residuals&lt;-pres_residuals[-length(pres_residuals)] pres_residuals&lt;-pres_residuals[-1] df&lt;-data.frame(pres_residuals,lag_residuals) head(df) ## pres_residuals lag_residuals ## 1 -1.0555271 -0.2219854 ## 2 -0.4305369 -1.0555271 ## 3 0.9734785 -0.4305369 ## 4 0.2337716 0.9734785 ## 5 2.2494336 0.2337716 ## 6 -0.5634219 2.2494336 g0&lt;-ggplot(df,aes(x=pres_residuals,y=lag_residuals)) g0+geom_point()+geom_smooth(method=&quot;lm&quot;) We can calculate to correlation coefficient for this. cor(pres_residuals,lag_residuals) ## [1] 0.3419599 We could continue to look at the relationship two time steps apart and more. However there is an easier way. R will calculate these for us and plot the correlation coefficients for us. acf(residuals(mod)) The problem with just looking at the auto correlation function is that the higher order auto-correlations are not independent of lower order effects. In order to control for this we can use the partial auto correlation function. This “partials out” the effects of lower order correlations leaving only the independent correlations. pacf(residuals(mod)) This suggests that an auto-regressive model form is needed as the acf is highly significant at lag one. The pcf suggests that higher order auto correlation is not occurring (which is in fact correct as it was not responsible for the simulated data.) So fit three models to the data. m1 &lt;- smooth.spline(xt, y) m2 &lt;- gam(y ~ s(xt, k = 20)) m3 &lt;- gamm(y ~ s(xt, k = 20), correlation = corAR1(form = ~ time)) The estimated degrees of freedom (complexity of the underlying relationship) varies between the models. edf2 &lt;- summary(m2)$edf edf3 &lt;- summary(m3$gam)$edf plot(y ~ xt, xlab = expression(x[t]), ylab = expression(y[t])) lines(Y ~ xt, lty = &quot;dashed&quot;, lwd = 1) lines(fitted(m1) ~ xt, lty = &quot;solid&quot;, col = &quot;darkolivegreen&quot;, lwd = 2) lines(fitted(m2) ~ xt, lty = &quot;solid&quot;, col = &quot;red&quot;, lwd = 2) lines(fitted(m3$lme) ~ xt, lty = &quot;solid&quot;, col = &quot;midnightblue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Truth&quot;, paste(&quot;Cubic spline (edf = &quot;, round(m1$df, 2), &quot;)&quot;, sep = &quot;&quot;), paste(&quot;AM (edf = &quot;, round(edf2, 2), &quot;)&quot;, sep = &quot;&quot;), paste(&quot;AM + AR(1) (edf = &quot;, round(edf3, 2), &quot;)&quot;, sep = &quot;&quot;)), col = c(&quot;black&quot;, &quot;darkgreen&quot;, &quot;red&quot;, &quot;midnightblue&quot;), lty = c(&quot;dashed&quot;, rep(&quot;solid&quot;, 3)), lwd = c(1, rep(2, 3)), bty = &quot;n&quot;, cex = 0.8) 23.4 Accounting for serial autocorrelation in the real climate data We need to fit a model in which the residuals may be serially correlated. \\(y = \\beta_0 + f_{\\mathrm{trend}}(x) + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2\\mathbf{\\Lambda})\\) The inclusion of the correlation matrix in the model formula results in a change in the weighting of the residuals in order to account for the correlation structure. This acts to counter the tendency of the underlying smooth function to over fit. The first step is to adjust a model with a limited number of knots that may represent a possible pattern. Gavin Simpson uses more knots in his initial model than I personally think should be used at this stage. test_mod &lt;- gam(Annual ~ s(Year, k = 10), data = hadcrut4) plot(test_mod, residuals = TRUE, pch = 19, cex = 0.75) Now look at the acf and pacf functions. acf(residuals(test_mod)) pacf(residuals(test_mod)) What you should be looking at is first how far back the auto correlation goes in terms of lags. However if we use the acf alone this could be misleading as the dependence on previous lags is not taken into account. The partial auto correlation function is needed. In this case there is significant auto correlation to lag one, and a possibly significant, but very weak, further effects at lag2. All others appear to be too weak to be important. So we can try building a first and second order auto correlation into the model. m1 &lt;- gamm(Annual ~ s(Year, k = 30), data = hadcrut4) m2 &lt;- gamm(Annual ~ s(Year, k = 30), data = hadcrut4,correlation = corARMA(form = ~ Year, p = 1)) ## First order m3 &lt;- gamm(Annual ~ s(Year, k = 30), data = hadcrut4,correlation = corARMA(form = ~ Year, p = 2)) ## Second order anova(m1$lme, m2$lme, m3$lme) ## Model df AIC BIC logLik Test L.Ratio p-value ## m1$lme 1 4 -261.4864 -249.0626 134.7432 ## m2$lme 2 5 -278.2396 -262.7099 144.1198 1 vs 2 18.753292 &lt;.0001 ## m3$lme 3 6 -278.8305 -260.1948 145.4152 2 vs 3 2.590839 0.1075 The model with a first order auto-regressive term is significantly better than the model with no serial auto correlation. plot(m2$gam, residuals = TRUE, pch = 19, cex = 0.75) Gavin Simpson provides a very useful function to look at model diagnostics for a fitted GAMM. with(hadcrut4, tsDiagGamm(m2, timevar = Year, observed = Annual)) There seem to be no major problems. plot(Annual ~ Year, data = hadcrut4, type = &quot;b&quot;, ylab = ylab) pdat &lt;- with(hadcrut4, data.frame(Year = seq(min(Year), max(Year), length = 200))) p1 &lt;- predict(m1$gam, newdata = pdat) p2 &lt;- predict(m2$gam, newdata = pdat) lines(p1 ~ Year, data = pdat, col = &quot;red&quot;) lines(p2 ~ Year, data = pdat, col = &quot;blue&quot;) legend(&quot;topleft&quot;, legend = c(&quot;Uncorrelated Errors&quot;,&quot;AR(1) Errors&quot;), bty = &quot;n&quot;, col = c(&quot;red&quot;,&quot;blue&quot;), lty = 1) The differences between the gam with and without auto correlation are actually quite minor so it would in fact have been OK to have ignored the serial auto correlation and relied on mgcv to select the number of knots for the GAM alone. However it would be bad practice to do this by default, as you may be asked to justify the assumption that there serial auto correlation in the residuals can be ignored. 23.5 Analysing the significance of the slope Gavin Simpson provides a very useful function for looking at local trends in a fitted model. In other words we may be interested in trying to detect periods in which significant upwards or downwards trends were taking place. We do that by looking at the slope of the fitted model (the first derivative of the values). Gavin Simpson’s code achieves this though numerical, rather than analytically (there is no form for this) differentiation. m2.d &lt;- Deriv(m2$gam, n = 200) plot(m2.d, sizer = TRUE, alpha = 0.01) I have changed the original function so that warming periods show up in red and cooling periods in blue, which is more logical. The grey bands are confidence intervals. So the function shows up periods in which there has been significant warming. This can be placed on the original time series. plot(Annual ~ Year, data = hadcrut4, type = &quot;b&quot;, ylab = ylab) lines(p2 ~ Year, data = pdat) CI &lt;- confint(m2.d, alpha = 0.01) S &lt;- signifD(p2, m2.d$Year$deriv, CI$Year$upper, CI$Year$lower, eval = 0) lines(S$incr ~ Year, data = pdat, lwd = 3, col = &quot;red&quot;) lines(S$decr ~ Year, data = pdat, lwd = 3, col = &quot;blue&quot;) 23.6 Seasonal data The previous data set used mean annual temperatures. However raw climate data will clearly have a seasonal component. This is a predictable cyclical element that is distinct from any other auto correlation in the data. The “cet” data set contains a long time series of data representing the mean monthly temperatures in Central England. GAM’s can be used to break down the time series into two components. The 12 monthly seasonal component and the remaining trend. \\(y = \\beta_0 + f_{\\mathrm{seasonal}}(x_1) + f_{\\mathrm{trend}}(x_2) + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2\\mathbf{\\Lambda})\\) The spline used for the monthly trend is a cyclical spline. This makes sure that the start and end point match. data(cet) cet&lt;-cet[order(cet$Date),] m &lt;- gamm(Temperature ~ s(Month_n, bs = &quot;cc&quot;, k = 12) + s(Year), data = cet) layout(matrix(1:2, ncol = 2)) plot(m$gam, scale = 0) Details of how to account for serial auto correlation are included in Gavin Simpson’s post. http://www.fromthebottomoftheheap.net/2014/05/09/modelling-seasonal-data-with-gam/ I won’t go through all this myself. However it is worth pointing out that serial auto correlation can only be detected in long time series. In this case the time series is longer than the previous one. There is a scaling issue. Once auto correlation is accounted for the result is much smoother in this time series than the last. Whether this is desirable or not is debatable and there are no clear answers. 23.7 Seasonal decomposition by loess smoothing Seasonal decomposition using loess is automated in an R function. This can be a very convenient way of looking at monthly time series. We will work with the cet data after 1900. We need to form a time series object for this analysis. Look at the help pages on time series objects to understand this step. data(cet) cet&lt;-cet[order(cet$Date),] cet&lt;-cet[format(cet$Date, &quot;%Y&quot;)&gt;1900,] min(cet$Date) ## [1] &quot;1901-01-15&quot; cet_ts&lt;-ts(cet$Temperature,start=1900,freq=12) The stl functiondecomposes the time series into a seasonal, trend and irregular components. This is very convenient and easy to use. Loess smoothing produces similar looking results to splines. The downside is that loess does not have such a clear theoretical basis, so it is difficult to base the complexity of the curves on statistical criteria. There are two adjustable parameters that can change the results. The seasonal window (s.window) and the trend window (t.window). If the value of the t.window settings is 1 the algorithmn will extract each year’s seasonal pattern independently and not smoth accross several years. If the setting is increased the seasonal pattern is averaged over several years. If the value is very large a single seasonal pattern is used over all years. cet_stl&lt;-stl(cet_ts,s.window=1,t.window=300) plot(cet_stl) cet_stl&lt;-stl(cet_ts,s.window=500,t.window=300) plot(cet_stl) cet_stl&lt;-stl(cet_ts,s.window=5,t.window=50) plot(cet_stl) cet_stl&lt;-stl(cet_ts,s.window=5,t.window=500) plot(cet_stl) It is not easy to decide on optimum settings for these parameters. They depend on the questions that are being asked. If you are interested in variability in seasonal patterns then the s.window parameter should be set to a low value. If you are interested in the annual trends over time then it may be set to a higher value. If the t.window setting is very large the trend will become linear. The elements of this decomposition can be extracted from the fitted object and used seperately. plot(cet_stl$time.series[,2]) This should give some ideas regarding how time series can be analysed. As mentioned in the introduction, one of the most important elements in time series analysis is to ensure that the question being adressed is clearly stated and that uncertainties are made explicit. There is no single correct way to analyse these sort of data. "],
["references-2.html", "References", " References The papers referred to here are all available on the server http://r.bournemouth.ac.uk:82/AQM/AQM_2018/articles/ "]
]
